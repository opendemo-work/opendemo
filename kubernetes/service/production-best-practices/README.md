# ğŸ­ Kubernetes Serviceç”Ÿäº§ç¯å¢ƒæœ€ä½³å®è·µ

> ä¼ä¸šçº§Kubernetes Serviceéƒ¨ç½²ã€è¿ç»´å’Œä¼˜åŒ–çš„å®Œæ•´æŒ‡å—ï¼Œæ¶µç›–é«˜å¯ç”¨ã€æ€§èƒ½è°ƒä¼˜ã€ç›‘æ§å‘Šè­¦ã€ç¾å¤‡æ¢å¤ç­‰å…³é”®ç”Ÿäº§è¦ç´ 

## ğŸ“‹ æ¡ˆä¾‹æ¦‚è¿°

æœ¬æ¡ˆä¾‹æ±‡é›†äº†Kubernetes Serviceåœ¨ç”Ÿäº§ç¯å¢ƒä¸­çš„æœ€ä½³å®è·µï¼Œå¸®åŠ©æ‚¨æ„å»ºç¨³å®šã€é«˜æ•ˆã€å¯æ‰©å±•çš„å¾®æœåŠ¡æ¶æ„ã€‚

### ğŸ”§ æ ¸å¿ƒæŠ€èƒ½ç‚¹

- **é«˜å¯ç”¨æ¶æ„**: å¤šåŒºåŸŸéƒ¨ç½²ã€æ•…éšœè½¬ç§»
- **æ€§èƒ½ä¼˜åŒ–**: è´Ÿè½½å‡è¡¡è°ƒä¼˜ã€è¿æ¥æ± ç®¡ç†
- **ç›‘æ§å‘Šè­¦**: å…¨æ–¹ä½æŒ‡æ ‡æ”¶é›†ã€æ™ºèƒ½å‘Šè­¦
- **ç¾å¤‡æ¢å¤**: å¤šé›†ç¾¤å¤‡ä»½ã€å¿«é€Ÿæ¢å¤
- **æˆæœ¬ä¼˜åŒ–**: èµ„æºé…é¢ç®¡ç†ã€æˆæœ¬æ§åˆ¶
- **åˆè§„è¿ç»´**: å®¡è®¡æ—¥å¿—ã€å˜æ›´ç®¡ç†

### ğŸ¯ é€‚ç”¨äººç¾¤

- ä¼ä¸šæ¶æ„å¸ˆ
- SREå·¥ç¨‹å¸ˆ
- è¿ç»´å›¢é˜Ÿè´Ÿè´£äºº
- æŠ€æœ¯ç®¡ç†è€…

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç”Ÿäº§ç¯å¢ƒå‡†å¤‡

```bash
# åˆ›å»ºç”Ÿäº§å‘½åç©ºé—´
kubectl create namespace production-services

# åº”ç”¨ç”Ÿäº§ç¯å¢ƒæ ‡ç­¾
kubectl label namespace production-services environment=production tier=critical

# é…ç½®èµ„æºé…é¢
kubectl apply -f production-resource-quota.yaml -n production-services
```

### 2. éƒ¨ç½²ç”Ÿäº§çº§åº”ç”¨

```bash
# éƒ¨ç½²é«˜å¯ç”¨åº”ç”¨
kubectl apply -f production-deployment.yaml -n production-services

# éªŒè¯éƒ¨ç½²çŠ¶æ€
kubectl get pods -n production-services -o wide
kubectl get services -n production-services
```

---

## ğŸ“š è¯¦ç»†æ•™ç¨‹

### 1. é«˜å¯ç”¨Serviceæ¶æ„

æ„å»ºå…·å¤‡æ•…éšœè‡ªæ„ˆèƒ½åŠ›çš„Serviceæ¶æ„ã€‚

#### å¤šå‰¯æœ¬éƒ¨ç½²é…ç½®

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: production-app
  namespace: production-services
  labels:
    app: production-app
    version: v1.2.3
spec:
  replicas: 6  # æ ¹æ®è´Ÿè½½è°ƒæ•´
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  selector:
    matchLabels:
      app: production-app
  template:
    metadata:
      labels:
        app: production-app
        version: v1.2.3
        environment: production
    spec:
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - production-app
              topologyKey: kubernetes.io/hostname
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: node-type
                operator: In
                values:
                - production
      containers:
      - name: app
        image: registry.example.com/production-app:v1.2.3
        ports:
        - containerPort: 8080
          name: http
        resources:
          requests:
            memory: "512Mi"
            cpu: "500m"
          limits:
            memory: "1Gi"
            cpu: "1000m"
        livenessProbe:
          httpGet:
            path: /healthz
            port: 8080
          initialDelaySeconds: 60
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /ready
            port: 8080
          initialDelaySeconds: 10
          periodSeconds: 5
          timeoutSeconds: 3
          failureThreshold: 3
        startupProbe:
          httpGet:
            path: /startup
            port: 8080
          initialDelaySeconds: 10
          periodSeconds: 10
          timeoutSeconds: 3
          failureThreshold: 30
```

#### é«˜å¯ç”¨Serviceé…ç½®

```yaml
apiVersion: v1
kind: Service
metadata:
  name: production-service
  namespace: production-services
  labels:
    app: production-app
    environment: production
  annotations:
    # AWS NLBé…ç½®
    service.beta.kubernetes.io/aws-load-balancer-type: "nlb"
    service.beta.kubernetes.io/aws-load-balancer-cross-zone-load-balancing-enabled: "true"
    service.beta.kubernetes.io/aws-load-balancer-healthcheck-protocol: "HTTP"
    service.beta.kubernetes.io/aws-load-balancer-healthcheck-port: "8080"
    service.beta.kubernetes.io/aws-load-balancer-healthcheck-path: "/healthz"
    service.beta.kubernetes.io/aws-load-balancer-healthcheck-interval: "30"
    service.beta.kubernetes.io/aws-load-balancer-healthcheck-timeout: "6"
    service.beta.kubernetes.io/aws-load-balancer-healthcheck-healthy-threshold: "2"
    service.beta.kubernetes.io/aws-load-balancer-healthcheck-unhealthy-threshold: "2"
spec:
  selector:
    app: production-app
  ports:
    - name: http
      protocol: TCP
      port: 80
      targetPort: 8080
    - name: https
      protocol: TCP
      port: 443
      targetPort: 8443
  type: LoadBalancer
  externalTrafficPolicy: Local
  internalTrafficPolicy: Local
  sessionAffinity: None  # ç”Ÿäº§ç¯å¢ƒé€šå¸¸ä¸éœ€è¦ä¼šè¯äº²å’Œæ€§
```

### 2. æ€§èƒ½ä¼˜åŒ–é…ç½®

ä¼˜åŒ–Serviceæ€§èƒ½çš„å…³é”®é…ç½®ã€‚

#### è¿æ¥æ± ä¼˜åŒ–

```yaml
apiVersion: v1
kind: Service
metadata:
  name: optimized-service
  namespace: production-services
  annotations:
    # è´Ÿè½½å‡è¡¡å™¨è¿æ¥ä¼˜åŒ–
    service.beta.kubernetes.io/aws-load-balancer-connection-draining-enabled: "true"
    service.beta.kubernetes.io/aws-load-balancer-connection-draining-timeout: "300"
    service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: "3600"
spec:
  selector:
    app: production-app
  ports:
    - protocol: TCP
      port: 80
      targetPort: 8080
  type: LoadBalancer
```

#### è´Ÿè½½å‡è¡¡ç®—æ³•è°ƒä¼˜

```yaml
apiVersion: v1
kind: Service
metadata:
  name: algorithm-optimized-service
  namespace: production-services
  annotations:
    # GCPè´Ÿè½½å‡è¡¡é…ç½®
    cloud.google.com/neg: '{"ingress": true}'
    # è‡ªå®šä¹‰è´Ÿè½½å‡è¡¡ç®—æ³•
    service.beta.kubernetes.io/aws-load-balancer-target-group-attributes: "stickiness.enabled=true,stickiness.lb_cookie.duration_seconds=3600"
spec:
  selector:
    app: production-app
  ports:
    - protocol: TCP
      port: 80
      targetPort: 8080
  type: LoadBalancer
```

### 3. ç›‘æ§å’Œå‘Šè­¦ä½“ç³»

å»ºç«‹å®Œå–„çš„ç›‘æ§å‘Šè­¦æœºåˆ¶ã€‚

#### ServiceæŒ‡æ ‡æ”¶é›†

```yaml
apiVersion: v1
kind: Service
metadata:
  name: monitored-service
  namespace: production-services
  labels:
    app: production-app
    monitoring: enabled
  annotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "9090"
    prometheus.io/path: "/metrics"
    prometheus.io/scheme: "http"
spec:
  selector:
    app: production-app
  ports:
    - name: http
      protocol: TCP
      port: 80
      targetPort: 8080
    - name: metrics
      protocol: TCP
      port: 9090
      targetPort: 9090
```

#### å…³é”®ç›‘æ§æŒ‡æ ‡

```yaml
# Prometheusç›‘æ§è§„åˆ™
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: service-production-rules
  namespace: production-services
spec:
  groups:
  - name: service.rules
    rules:
    - alert: ServiceDown
      expr: up{job="production-service"} == 0
      for: 2m
      labels:
        severity: critical
      annotations:
        summary: "Service {{ $labels.service }} is down"
        description: "{{ $labels.service }} has been down for more than 2 minutes."
    
    - alert: HighErrorRate
      expr: rate(http_requests_total{status=~"5.."}[5m]) > 0.05
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "High error rate on {{ $labels.service }}"
        description: "Error rate is above 5% for the last 5 minutes."
    
    - alert: HighLatency
      expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 2
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "High latency on {{ $labels.service }}"
        description: "95th percentile latency is above 2 seconds."
```

### 4. ç¾å¤‡å’Œæ¢å¤ç­–ç•¥

åˆ¶å®šå®Œå–„çš„ç¾å¤‡æ¢å¤è®¡åˆ’ã€‚

#### å¤šé›†ç¾¤Serviceéƒ¨ç½²

```yaml
# ä¸»é›†ç¾¤é…ç½®
apiVersion: v1
kind: Service
metadata:
  name: primary-service
  namespace: production-services
  labels:
    app: production-app
    cluster: primary
    disaster-recovery: enabled
spec:
  selector:
    app: production-app
    cluster: primary
  ports:
    - protocol: TCP
      port: 80
      targetPort: 8080
  type: LoadBalancer

---
# å¤‡ç”¨é›†ç¾¤é…ç½®
apiVersion: v1
kind: Service
metadata:
  name: standby-service
  namespace: production-services
  labels:
    app: production-app
    cluster: standby
    disaster-recovery: enabled
spec:
  selector:
    app: production-app
    cluster: standby
  ports:
    - protocol: TCP
      port: 80
      targetPort: 8080
  type: LoadBalancer
```

#### è‡ªåŠ¨æ•…éšœè½¬ç§»é…ç½®

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: dr-coordinator
  namespace: production-services
spec:
  replicas: 1
  selector:
    matchLabels:
      app: dr-coordinator
  template:
    metadata:
      labels:
        app: dr-coordinator
    spec:
      containers:
      - name: coordinator
        image: dr-coordinator:latest
        env:
        - name: PRIMARY_CLUSTER_ENDPOINT
          value: "primary-service.production-services.svc.cluster.local"
        - name: STANDBY_CLUSTER_ENDPOINT
          value: "standby-service.production-services.svc.cluster.local"
        - name: FAILOVER_THRESHOLD
          value: "3"
        - name: HEALTH_CHECK_INTERVAL
          value: "30"
```

### 5. æˆæœ¬ä¼˜åŒ–ç­–ç•¥

ä¼˜åŒ–Serviceç›¸å…³çš„äº‘èµ„æºæˆæœ¬ã€‚

#### èµ„æºé…é¢ç®¡ç†

```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: production-quota
  namespace: production-services
spec:
  hard:
    requests.cpu: "20"
    requests.memory: 40Gi
    limits.cpu: "40"
    limits.memory: 80Gi
    services.loadbalancers: "10"
    services.nodeports: "0"
    persistentvolumeclaims: "20"
```

#### æˆæœ¬ç›‘æ§é…ç½®

```yaml
apiVersion: v1
kind: Service
metadata:
  name: cost-analyzer-service
  namespace: production-services
  annotations:
    # AWSæˆæœ¬åˆ†é…æ ‡ç­¾
    service.beta.kubernetes.io/aws-load-balancer-additional-resource-tags: "Project=Production,CostCenter=Engineering"
spec:
  selector:
    app: cost-analyzer
  ports:
    - protocol: TCP
      port: 80
      targetPort: 8080
  type: LoadBalancer
```

### 6. åˆè§„å’Œå®¡è®¡

æ»¡è¶³ä¼ä¸šåˆè§„è¦æ±‚çš„é…ç½®ã€‚

#### å®¡è®¡æ—¥å¿—é…ç½®

```yaml
apiVersion: audit.k8s.io/v1
kind: Policy
metadata:
  name: production-audit-policy
rules:
- level: RequestResponse
  resources:
  - group: ""
    resources: ["services", "endpoints", "pods"]
  verbs: ["create", "update", "delete", "patch"]
  namespaces: ["production-services"]
  userGroups: ["system:serviceaccounts"]

- level: Metadata
  resources:
  - group: ""
    resources: ["services"]
  verbs: ["get", "list", "watch"]
  namespaces: ["production-services"]
```

#### å˜æ›´ç®¡ç†æµç¨‹

```yaml
apiVersion: v1
kind: Service
metadata:
  name: managed-service
  namespace: production-services
  annotations:
    # å˜æ›´å®¡æ‰¹æµç¨‹
    change-management/approval-required: "true"
    change-management/approvers: "team-lead,sre-manager"
    change-management/change-type: "production-change"
    change-management/impact-level: "high"
spec:
  selector:
    app: production-app
  ports:
    - protocol: TCP
      port: 80
      targetPort: 8080
  type: LoadBalancer
```

---

## ğŸ› ï¸ è¿ç»´æœ€ä½³å®è·µ

### 1. æ ‡å‡†åŒ–å‘½åè§„èŒƒ

```yaml
# æœåŠ¡å‘½åè§„èŒƒ
apiVersion: v1
kind: Service
metadata:
  name: prod-user-service-v1-2-3  # ç¯å¢ƒ-åº”ç”¨-æœåŠ¡-ç‰ˆæœ¬
  namespace: production-services
  labels:
    app.kubernetes.io/name: user-service
    app.kubernetes.io/instance: prod-user-service
    app.kubernetes.io/version: v1.2.3
    app.kubernetes.io/component: api
    app.kubernetes.io/part-of: user-platform
    app.kubernetes.io/managed-by: helm
```

### 2. é…ç½®ç®¡ç†ç­–ç•¥

```bash
# ä½¿ç”¨ConfigMapç®¡ç†é…ç½®
kubectl create configmap service-config \
  --from-file=nginx.conf \
  --from-literal=max_connections=1000 \
  -n production-services

# ä½¿ç”¨Secretç®¡ç†æ•æ„Ÿä¿¡æ¯
kubectl create secret generic service-secrets \
  --from-literal=db_password=secretpassword \
  --from-file=tls.crt=./certs/server.crt \
  -n production-services
```

### 3. è‡ªåŠ¨åŒ–è¿ç»´è„šæœ¬

```bash
#!/bin/bash
# service-health-check.sh

NAMESPACE="production-services"
SERVICES=("user-service" "order-service" "payment-service")

for service in "${SERVICES[@]}"; do
    echo "Checking service: $service"
    
    # æ£€æŸ¥ServiceçŠ¶æ€
    kubectl get service $service -n $NAMESPACE
    
    # æ£€æŸ¥Endpoints
    ENDPOINTS=$(kubectl get endpoints $service -n $NAMESPACE -o jsonpath='{.subsets[*].addresses[*].ip}' | wc -w)
    echo "Active endpoints: $ENDPOINTS"
    
    # æ£€æŸ¥Podå¥åº·çŠ¶æ€
    UNHEALTHY_PODS=$(kubectl get pods -n $NAMESPACE -l app=$service --field-selector=status.phase!=Running | wc -l)
    echo "Unhealthy pods: $UNHEALTHY_PODS"
    
    echo "---"
done
```

---

## ğŸ“Š æ€§èƒ½åŸºå‡†æµ‹è¯•

### 1. è´Ÿè½½æµ‹è¯•é…ç½®

```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: service-load-test
  namespace: production-services
spec:
  template:
    spec:
      containers:
      - name: load-tester
        image: locustio/locust
        args:
        - -f
        - /test-scripts/load_test.py
        - --host
        - http://production-service:80
        - -u
        - "1000"  # å¹¶å‘ç”¨æˆ·æ•°
        - -r
        - "100"   # æ¯ç§’å¯åŠ¨ç”¨æˆ·æ•°
        - -t
        - "10m"   # æµ‹è¯•æ—¶é•¿
        volumeMounts:
        - name: test-scripts
          mountPath: /test-scripts
      volumes:
      - name: test-scripts
        configMap:
          name: load-test-scripts
      restartPolicy: Never
```

### 2. æ€§èƒ½æŒ‡æ ‡æ”¶é›†

```bash
# æ”¶é›†å…³é”®æ€§èƒ½æŒ‡æ ‡
kubectl top services -n production-services

# åˆ†æç½‘ç»œå»¶è¿Ÿ
kubectl exec -it <pod-name> -n production-services -- \
  ping -c 10 production-service.production-services.svc.cluster.local

# ç›‘æ§è¿æ¥æ•°
kubectl exec -it <pod-name> -n production-services -- \
  netstat -an | grep :8080 | wc -l
```

---

## ğŸ”§ æ•…éšœæ’é™¤æ¸…å•

### 1. Serviceä¸å¯è®¿é—®æ’æŸ¥

```bash
# æ£€æŸ¥Serviceé…ç½®
kubectl describe service <service-name> -n production-services

# æ£€æŸ¥Endpoints
kubectl get endpoints <service-name> -n production-services

# æ£€æŸ¥PodçŠ¶æ€
kubectl get pods -n production-services -l app=<app-label>

# æ£€æŸ¥ç½‘ç»œç­–ç•¥
kubectl get networkpolicies -n production-services

# æµ‹è¯•è¿é€šæ€§
kubectl run debug-pod --image=busybox --rm -it -n production-services -- \
  wget -qO- http://<service-name>:<port>
```

### 2. æ€§èƒ½é—®é¢˜æ’æŸ¥

```bash
# æ£€æŸ¥èµ„æºä½¿ç”¨æƒ…å†µ
kubectl top pods -n production-services -l app=<app-label>

# åˆ†ææ…¢è¯·æ±‚
kubectl logs -n production-services -l app=<app-label> --since=1h | \
  grep "slow request" | tail -20

# æ£€æŸ¥èŠ‚ç‚¹èµ„æº
kubectl describe nodes | grep -A 5 "Allocated resources"
```

---

## ğŸ§ª å®è·µç»ƒä¹ 

### ç»ƒä¹ 1ï¼šæ„å»ºé«˜å¯ç”¨æ¶æ„
è®¾è®¡å¹¶éƒ¨ç½²ä¸€ä¸ªå…·å¤‡è‡ªåŠ¨æ•…éšœè½¬ç§»èƒ½åŠ›çš„ä¸‰å‰¯æœ¬Serviceæ¶æ„ã€‚

### ç»ƒä¹ 2ï¼šæ€§èƒ½è°ƒä¼˜å®æˆ˜
å¯¹ç°æœ‰Serviceè¿›è¡Œå‹åŠ›æµ‹è¯•ï¼Œè¯†åˆ«ç“¶é¢ˆå¹¶å®æ–½ä¼˜åŒ–æªæ–½ã€‚

### ç»ƒä¹ 3ï¼šç›‘æ§å‘Šè­¦é…ç½®
å»ºç«‹å®Œæ•´çš„ç›‘æ§å‘Šè­¦ä½“ç³»ï¼Œç¡®ä¿å…³é”®æŒ‡æ ‡å¼‚å¸¸æ—¶èƒ½åŠæ—¶é€šçŸ¥ã€‚

### ç»ƒä¹ 4ï¼šç¾å¤‡æ¼”ç»ƒ
æ‰§è¡Œå®Œæ•´çš„ç¾å¤‡æ¢å¤æ¼”ç»ƒï¼ŒéªŒè¯RTOå’ŒRPOæŒ‡æ ‡ã€‚

---

## ğŸ“‹ æ¸…ç†èµ„æº

```bash
# åˆ é™¤ç”Ÿäº§æµ‹è¯•èµ„æº
kubectl delete namespace production-services

# æˆ–å•ç‹¬æ¸…ç†å„é¡¹èµ„æº
kubectl delete svc --all -n production-services
kubectl delete deploy --all -n production-services
kubectl delete configmap --all -n production-services
kubectl delete secret --all -n production-services
```

---

> **ğŸ’¡ æç¤º**: ç”Ÿäº§ç¯å¢ƒé…ç½®éœ€è¦ç»“åˆå…·ä½“ä¸šåŠ¡åœºæ™¯å’ŒåŸºç¡€è®¾æ–½ç‰¹ç‚¹è¿›è¡Œè°ƒæ•´ï¼Œå»ºè®®åœ¨å®æ–½å‰è¿›è¡Œå……åˆ†çš„æµ‹è¯•å’Œè¯„å®¡ã€‚