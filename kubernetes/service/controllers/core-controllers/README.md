# ğŸš€ Kubernetes Service Controllers ç”Ÿäº§çº§ç®¡ç†

> ä¼ä¸šçº§ Service æ§åˆ¶å™¨éƒ¨ç½²ã€é…ç½®å’Œç®¡ç†å®Œæ•´æŒ‡å—

## ğŸ“‹ æ¡ˆä¾‹æ¦‚è¿°

æœ¬æ¡ˆä¾‹æä¾› Kubernetes Service æ§åˆ¶å™¨çš„ä¼ä¸šçº§ç®¡ç†æ–¹æ¡ˆï¼Œæ¶µç›–ä»åŸºç¡€æ§åˆ¶å™¨åˆ°é«˜çº§æœåŠ¡ç½‘æ ¼çš„å„ç§éƒ¨ç½²æ¨¡å¼ï¼Œç¡®ä¿åœ¨ç”Ÿäº§ç¯å¢ƒä¸­èƒ½å¤Ÿç¨³å®šã€é«˜æ•ˆåœ°ç®¡ç†æœåŠ¡å‘ç°å’Œè´Ÿè½½å‡è¡¡ã€‚

### ğŸ”§ æ ¸å¿ƒèƒ½åŠ›è¦†ç›–

- **åŸºç¡€æ§åˆ¶å™¨**: kube-proxyã€CoreDNSæœåŠ¡å‘ç°
- **æœåŠ¡ç½‘æ ¼**: Istioã€Linkerdã€Consul Connect
- **äº‘åŸç”ŸæœåŠ¡**: AWS Load Balancer Controllerã€GKE Service Networking
- **æ€§èƒ½ä¼˜åŒ–**: è¿æ¥æ± ä¼˜åŒ–ã€è´Ÿè½½å‡è¡¡ç®—æ³•ã€ä¼šè¯ä¿æŒ
- **é«˜å¯ç”¨é…ç½®**: å¤šå‰¯æœ¬éƒ¨ç½²ã€æ•…éšœè½¬ç§»ã€å¥åº·æ£€æŸ¥
- **ç›‘æ§å‘Šè­¦**: æ§åˆ¶å™¨æŒ‡æ ‡ã€æœåŠ¡çŠ¶æ€ç›‘æ§ã€æ€§èƒ½åˆ†æ

### ğŸ¯ é€‚ç”¨åœºæ™¯

- ä¼ä¸šçº§å¾®æœåŠ¡æ¶æ„
- é«˜å¯ç”¨æœåŠ¡éƒ¨ç½²
- å¤šäº‘ç¯å¢ƒæœåŠ¡ç®¡ç†
- æœåŠ¡ç½‘æ ¼é›†æˆ
- æ€§èƒ½æ•æ„Ÿå‹åº”ç”¨

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒå‡†å¤‡

```bash
# æ£€æŸ¥é›†ç¾¤çŠ¶æ€
kubectl cluster-info
kubectl get nodes

# éªŒè¯åŸºç¡€æ§åˆ¶å™¨çŠ¶æ€
kubectl get daemonsets -n kube-system kube-proxy
kubectl get deployments -n kube-system coredns

# åˆ›å»ºæ§åˆ¶å™¨ç®¡ç†å‘½åç©ºé—´
kubectl create namespace service-controllers
```

### 2. åŸºç¡€æ§åˆ¶å™¨éªŒè¯

```bash
# æ£€æŸ¥ kube-proxy çŠ¶æ€
kubectl get daemonsets -n kube-system kube-proxy
kubectl get pods -n kube-system -l k8s-app=kube-proxy

# æ£€æŸ¥ CoreDNS çŠ¶æ€
kubectl get deployments -n kube-system coredns
kubectl get services -n kube-system kube-dns

# éªŒè¯æœåŠ¡å‘ç°åŠŸèƒ½
kubectl run debug --image=busybox --rm -it -- nslookup kubernetes.default
```

---

## ğŸ“š æ ¸å¿ƒæ§åˆ¶å™¨è¯¦è§£

### 1. kube-proxy ç®¡ç†

#### 1.1 kube-proxy æ¨¡å¼é…ç½®

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: kube-proxy
  namespace: kube-system
data:
  config.conf: |
    apiVersion: kubeproxy.config.k8s.io/v1alpha1
    kind: KubeProxyConfiguration
    mode: "iptables"  # æˆ– "ipvs"
    iptables:
      masqueradeAll: false
      masqueradeBit: 14
      minSyncPeriod: 0s
      syncPeriod: 30s
    ipvs:
      excludeCIDRs: null
      minSyncPeriod: 0s
      scheduler: "rr"  # è½®è¯¢è°ƒåº¦
      syncPeriod: 30s
      strictARP: false
    conntrack:
      maxPerCore: 32768
      min: 131072
      tcpCloseWaitTimeout: 1h0m0s
      tcpEstablishedTimeout: 24h0m0s
```

#### 1.2 kube-proxy æ€§èƒ½ä¼˜åŒ–

```yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: kube-proxy
  namespace: kube-system
spec:
  selector:
    matchLabels:
      k8s-app: kube-proxy
  template:
    metadata:
      labels:
        k8s-app: kube-proxy
    spec:
      containers:
        - name: kube-proxy
          image: registry.k8s.io/kube-proxy:v1.28.0
          command:
            - /usr/local/bin/kube-proxy
            - --config=/var/lib/kube-proxy/config.conf
            - --v=2
          resources:
            requests:
              cpu: 100m
              memory: 128Mi
            limits:
              cpu: 200m
              memory: 256Mi
          securityContext:
            privileged: true
          volumeMounts:
            - name: kube-proxy-config
              mountPath: /var/lib/kube-proxy/
            - name: lib-modules
              mountPath: /lib/modules
              readOnly: true
      volumes:
        - name: kube-proxy-config
          configMap:
            name: kube-proxy
        - name: lib-modules
          hostPath:
            path: /lib/modules
```

### 2. CoreDNS æœåŠ¡å‘ç°

#### 2.1 CoreDNS é…ç½®ä¼˜åŒ–

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: coredns
  namespace: kube-system
data:
  Corefile: |
    .:53 {
        errors
        health {
           lameduck 5s
        }
        ready
        kubernetes cluster.local in-addr.arpa ip6.arpa {
           pods insecure
           fallthrough in-addr.arpa ip6.arpa
           ttl 30
        }
        prometheus :9153
        forward . /etc/resolv.conf {
           max_concurrent 1000
        }
        cache 30
        loop
        reload
        loadbalance
    }
```

#### 2.2 CoreDNS æ°´å¹³æ‰©å±•

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: coredns
  namespace: kube-system
spec:
  replicas: 3  # æ ¹æ®é›†ç¾¤è§„æ¨¡è°ƒæ•´
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
  selector:
    matchLabels:
      k8s-app: kube-dns
  template:
    metadata:
      labels:
        k8s-app: kube-dns
    spec:
      priorityClassName: system-cluster-critical
      serviceAccountName: coredns
      containers:
        - name: coredns
          image: registry.k8s.io/coredns/coredns:v1.10.1
          args: [ "-conf", "/etc/coredns/Corefile" ]
          volumeMounts:
            - name: config-volume
              mountPath: /etc/coredns
              readOnly: true
          ports:
            - containerPort: 53
              name: dns
              protocol: UDP
            - containerPort: 53
              name: dns-tcp
              protocol: TCP
            - containerPort: 9153
              name: metrics
              protocol: TCP
          resources:
            requests:
              cpu: 100m
              memory: 128Mi
            limits:
              cpu: 200m
              memory: 256Mi
          livenessProbe:
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
            initialDelaySeconds: 60
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 5
          readinessProbe:
            httpGet:
              path: /ready
              port: 8181
              scheme: HTTP
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              add:
                - NET_BIND_SERVICE
              drop:
                - all
            readOnlyRootFilesystem: true
      volumes:
        - name: config-volume
          configMap:
            name: coredns
            items:
              - key: Corefile
                path: Corefile
```

### 3. æœåŠ¡ç½‘æ ¼æ§åˆ¶å™¨

#### 3.1 Istio æœåŠ¡ç½‘æ ¼éƒ¨ç½²

```yaml
# Istio åŸºç¡€ç»„ä»¶
apiVersion: install.istio.io/v1alpha1
kind: IstioOperator
metadata:
  name: istio-controlplane
  namespace: istio-system
spec:
  profile: default
  components:
    pilot:
      enabled: true
      k8s:
        resources:
          requests:
            cpu: 200m
            memory: 256Mi
          limits:
            cpu: 500m
            memory: 512Mi
    ingressGateways:
      - name: istio-ingressgateway
        enabled: true
        k8s:
          resources:
            requests:
              cpu: 100m
              memory: 128Mi
            limits:
              cpu: 2000m
              memory: 1024Mi
  values:
    global:
      proxy:
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 2000m
            memory: 1024Mi
    pilot:
      autoscaleEnabled: true
      autoscaleMin: 2
      autoscaleMax: 5
```

#### 3.2 Linkerd æœåŠ¡ç½‘æ ¼é…ç½®

```yaml
# Linkerd åŸºç¡€å®‰è£…
apiVersion: linkerd.io/v1alpha2
kind: ServiceProfile
metadata:
  name: web-svc.emojivoto.svc.cluster.local
  namespace: emojivoto
spec:
  routes:
    - condition:
        method: GET
        pathRegex: /api/list
      name: list-emojis
      isRetryable: false
    - condition:
        method: POST
        pathRegex: /api/vote
      name: vote-emoji
      isRetryable: true
```

### 4. äº‘åŸç”ŸæœåŠ¡æ§åˆ¶å™¨

#### 4.1 AWS Load Balancer Controller

```yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: aws-load-balancer-controller
  namespace: kube-system
  annotations:
    eks.amazonaws.com/role-arn: arn:aws:iam::123456789012:role/AWSLoadBalancerControllerIAMRole
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: aws-load-balancer-controller
  namespace: kube-system
spec:
  replicas: 2
  selector:
    matchLabels:
      app.kubernetes.io/name: aws-load-balancer-controller
  template:
    metadata:
      labels:
        app.kubernetes.io/name: aws-load-balancer-controller
    spec:
      serviceAccountName: aws-load-balancer-controller
      containers:
        - name: controller
          image: public.ecr.aws/eks/aws-load-balancer-controller:v2.5.4
          args:
            - --cluster-name=your-cluster-name
            - --ingress-class=alb
            - --enable-waf=false
            - --enable-wafv2=false
            - --enable-shield=false
          ports:
            - containerPort: 8080
              name: webhook
          resources:
            requests:
              cpu: 200m
              memory: 512Mi
            limits:
              cpu: 1000m
              memory: 1024Mi
          livenessProbe:
            httpGet:
              path: /healthz
              port: 61779
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 10
```

---

## ğŸ”§ é«˜çº§é…ç½®é€‰é¡¹

### 1. è‡ªå®šä¹‰è´Ÿè½½å‡è¡¡ç­–ç•¥

```yaml
apiVersion: v1
kind: Service
metadata:
  name: custom-lb-service
  namespace: production
  annotations:
    # AWS ç‰¹å®šé…ç½®
    service.beta.kubernetes.io/aws-load-balancer-type: "nlb"
    service.beta.kubernetes.io/aws-load-balancer-cross-zone-load-balancing-enabled: "true"
    service.beta.kubernetes.io/aws-load-balancer-proxy-protocol: "*"
    
    # å¥åº·æ£€æŸ¥é…ç½®
    service.beta.kubernetes.io/aws-load-balancer-healthcheck-protocol: "HTTP"
    service.beta.kubernetes.io/aws-load-balancer-healthcheck-port: "8080"
    service.beta.kubernetes.io/aws-load-balancer-healthcheck-path: "/health"
    service.beta.kubernetes.io/aws-load-balancer-healthcheck-interval: "30"
    service.beta.kubernetes.io/aws-load-balancer-healthcheck-timeout: "6"
    service.beta.kubernetes.io/aws-load-balancer-healthcheck-healthy-threshold: "2"
    service.beta.kubernetes.io/aws-load-balancer-healthcheck-unhealthy-threshold: "2"
spec:
  type: LoadBalancer
  selector:
    app: myapp
  ports:
    - protocol: TCP
      port: 80
      targetPort: 8080
```

### 2. ä¼šè¯ä¿æŒé…ç½®

```yaml
apiVersion: v1
kind: Service
metadata:
  name: sticky-session-service
  namespace: production
  annotations:
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
spec:
  selector:
    app: myapp
  ports:
    - protocol: TCP
      port: 80
      targetPort: 8080
  type: ClusterIP
  sessionAffinity: ClientIP
  sessionAffinityConfig:
    clientIP:
      timeoutSeconds: 10800  # 3å°æ—¶ä¼šè¯ä¿æŒ
```

### 3. å¤–éƒ¨æœåŠ¡é›†æˆ

```yaml
apiVersion: v1
kind: Service
metadata:
  name: external-database-service
  namespace: production
spec:
  type: ExternalName
  externalName: database.prod.company.internal
---
apiVersion: v1
kind: Endpoints
metadata:
  name: external-database-service
  namespace: production
subsets:
  - addresses:
      - ip: 10.0.1.100
      - ip: 10.0.1.101
    ports:
      - port: 5432
```

---

## ğŸ“Š ç›‘æ§å’Œå‘Šè­¦

### 1. æ§åˆ¶å™¨æŒ‡æ ‡æ”¶é›†

```yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: kube-proxy-monitor
  namespace: monitoring
spec:
  selector:
    matchLabels:
      k8s-app: kube-proxy
  namespaceSelector:
    matchNames:
      - kube-system
  endpoints:
    - port: metrics
      interval: 30s
      path: /metrics
---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: coredns-monitor
  namespace: monitoring
spec:
  selector:
    matchLabels:
      k8s-app: kube-dns
  namespaceSelector:
    matchNames:
      - kube-system
  endpoints:
    - port: metrics
      interval: 30s
      path: /metrics
```

### 2. å‘Šè­¦è§„åˆ™é…ç½®

```yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: service-controller-alerts
  namespace: monitoring
spec:
  groups:
    - name: service-controllers.rules
      rules:
        # CoreDNS å‘Šè­¦
        - alert: CoreDNSDown
          expr: up{job="coredns"} == 0
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: "CoreDNS is down"
            description: "CoreDNS has disappeared from Prometheus target discovery"
        
        # kube-proxy å‘Šè­¦
        - alert: KubeProxyDown
          expr: up{job="kube-proxy"} == 0
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "kube-proxy is down"
            description: "kube-proxy has disappeared from Prometheus target discovery"
        
        # Service ç«¯ç‚¹å‘Šè­¦
        - alert: ServiceWithoutEndpoints
          expr: kube_service_spec_type == 1 unless kube_endpoint_address_available > 0
          for: 10m
          labels:
            severity: warning
          annotations:
            summary: "Service without endpoints"
            description: "Service {{ $labels.namespace }}/{{ $labels.service }} has no endpoints"
```

---

## ğŸš¨ æ•…éšœæ’æŸ¥

### 1. å¸¸è§é—®é¢˜è¯Šæ–­

```bash
# 1. æ£€æŸ¥æ§åˆ¶å™¨çŠ¶æ€
kubectl get daemonsets -n kube-system kube-proxy
kubectl get deployments -n kube-system coredns

# 2. æŸ¥çœ‹æ§åˆ¶å™¨æ—¥å¿—
kubectl logs -n kube-system -l k8s-app=kube-proxy --tail=100
kubectl logs -n kube-system -l k8s-app=kube-dns --tail=100

# 3. éªŒè¯æœåŠ¡å‘ç°
kubectl run debug --image=busybox --rm -it -- nslookup kubernetes.default
kubectl run debug --image=busybox --rm -it -- nslookup <service-name>.<namespace>.svc.cluster.local

# 4. æ£€æŸ¥ç½‘ç»œç­–ç•¥å½±å“
kubectl get networkpolicies --all-namespaces
```

### 2. æ€§èƒ½è°ƒä¼˜

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: kube-proxy-tuning
  namespace: kube-system
data:
  config.conf: |
    apiVersion: kubeproxy.config.k8s.io/v1alpha1
    kind: KubeProxyConfiguration
    mode: "ipvs"
    ipvs:
      scheduler: "lc"  # æœ€å°‘è¿æ¥è°ƒåº¦
      syncPeriod: 15s
      minSyncPeriod: 5s
    conntrack:
      maxPerCore: 65536
      min: 262144
      tcpCloseWaitTimeout: 30m
      tcpEstablishedTimeout: 12h
```

---

## ğŸ§ª å®è·µç»ƒä¹ 

### ç»ƒä¹ 1ï¼šæ§åˆ¶å™¨éƒ¨ç½²ä¼˜åŒ–
éƒ¨ç½²å¹¶ä¼˜åŒ– kube-proxy å’Œ CoreDNS é…ç½®ï¼Œæå‡æœåŠ¡å‘ç°æ€§èƒ½ã€‚

### ç»ƒä¹ 2ï¼šæœåŠ¡ç½‘æ ¼é›†æˆ
é…ç½® Istio æˆ– Linkerd æœåŠ¡ç½‘æ ¼ï¼Œå®ç°é«˜çº§æµé‡ç®¡ç†ã€‚

### ç»ƒä¹ 3ï¼šäº‘åŸç”ŸæœåŠ¡
éƒ¨ç½² AWS Load Balancer Controller å¹¶é…ç½®å¤–éƒ¨è´Ÿè½½å‡è¡¡å™¨ã€‚

### ç»ƒä¹ 4ï¼šæ•…éšœè¯Šæ–­æ¼”ç»ƒ
æ¨¡æ‹Ÿå„ç§æ§åˆ¶å™¨æ•…éšœåœºæ™¯ï¼Œç»ƒä¹ è¯Šæ–­å’Œæ¢å¤æŠ€èƒ½ã€‚

---

## ğŸ“š æ‰©å±•é˜…è¯»

### å®˜æ–¹æ–‡æ¡£
- [Kubernetes Service Controllers](https://kubernetes.io/docs/concepts/architecture/control-plane-node-communication/)
- [CoreDNS å®˜æ–¹æ–‡æ¡£](https://coredns.io/manual/toc/)
- [Istio æœåŠ¡ç½‘æ ¼](https://istio.io/latest/docs/)

### ç›¸å…³æ¡ˆä¾‹
- [Service é«˜çº§ç‰¹æ€§](../advanced-features/)
- [Service ç›‘æ§è¿ç»´](../monitoring-operations/)
- [Service å®‰å…¨åŠ å›º](../security-hardening/)

### è¿›é˜¶ä¸»é¢˜
- å¤šé›†ç¾¤æœåŠ¡å‘ç°
- è¾¹ç¼˜è®¡ç®—æœåŠ¡ç®¡ç†
- AIé©±åŠ¨çš„æœåŠ¡ä¼˜åŒ–
- é›¶ä¿¡ä»»ç½‘ç»œæ¶æ„

---

## ğŸ“‹ æ¸…ç†èµ„æº

```bash
# åˆ é™¤æµ‹è¯•èµ„æº
kubectl delete namespace service-controllers

# é‡ç½®æ§åˆ¶å™¨é…ç½®ï¼ˆè°¨æ…æ“ä½œï¼‰
kubectl delete configmap kube-proxy -n kube-system
kubectl apply -f https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/addons/kube-proxy/kube-proxy-ds.yaml
```

---

> **ğŸ’¡ æç¤º**: Service æ§åˆ¶å™¨æ˜¯ Kubernetes ç½‘ç»œåŠŸèƒ½çš„æ ¸å¿ƒç»„ä»¶ï¼Œåˆç†çš„é…ç½®å’Œç›‘æ§å¯¹é›†ç¾¤ç¨³å®šæ€§è‡³å…³é‡è¦ã€‚