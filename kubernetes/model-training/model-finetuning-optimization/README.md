# æ¨¡å‹å¾®è°ƒä¸ä¼˜åŒ–

## 1. æ¡ˆä¾‹æ¦‚è¿°

æœ¬æ¡ˆä¾‹æä¾›ä»å…¥é—¨åˆ°ç”Ÿäº§ç¯å¢ƒçš„å®Œæ•´å¤§æ¨¡å‹å¾®è°ƒä¸ä¼˜åŒ–å®æˆ˜æŒ‡å—ï¼Œç³»ç»Ÿæ€§åœ°æ¶µç›–å‚æ•°é«˜æ•ˆå¾®è°ƒ(PEFT)ã€æ¨¡å‹é‡åŒ–ã€çŸ¥è¯†è’¸é¦ã€è¡Œä¸šåº”ç”¨ç­‰å…¨ç»´åº¦æŠ€æœ¯ï¼Œå¸®åŠ©åœ¨æœ‰é™èµ„æºä¸‹å®ç°é«˜æ•ˆçš„æ¨¡å‹å®šåˆ¶åŒ–å’Œç”Ÿäº§éƒ¨ç½²ã€‚

### 1.1 æ¡ˆä¾‹ä½“ç³»ç»“æ„

```mermaid
tree
    A[æ¨¡å‹å¾®è°ƒä¸ä¼˜åŒ–]
    â”œâ”€â”€ B[å‚æ•°é«˜æ•ˆå¾®è°ƒæŠ€æœ¯]
    â”‚   â”œâ”€â”€ B1[LoRAä½ç§©é€‚é…]
    â”‚   â”œâ”€â”€ B2[QLoRAé‡åŒ–å¾®è°ƒ]
    â”‚   â”œâ”€â”€ B3[Adapteré€‚é…å™¨]
    â”‚   â”œâ”€â”€ B4[Prefix Tuningå‰ç¼€è°ƒä¼˜]
    â”‚   â””â”€â”€ B5[Prompt Tuningæç¤ºè°ƒä¼˜]
    â”œâ”€â”€ C[æ¨¡å‹é‡åŒ–æŠ€æœ¯]
    â”‚   â”œâ”€â”€ C1[è®­ç»ƒåé‡åŒ–(PTQ)]
    â”‚   â”œâ”€â”€ C2[é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ(QAT)]
    â”‚   â””â”€â”€ C3[GPTQé‡åŒ–]
    â”œâ”€â”€ D[çŸ¥è¯†è’¸é¦æŠ€æœ¯]
    â”‚   â”œâ”€â”€ D1[åŸºç¡€è’¸é¦]
    â”‚   â”œâ”€â”€ D2[ç‰¹å¾è’¸é¦]
    â”‚   â””â”€â”€ D3[å…³ç³»è’¸é¦]
    â”œâ”€â”€ E[è¡Œä¸šåº”ç”¨åœºæ™¯]
    â”‚   â”œâ”€â”€ E1[åŒ»ç–—é¢†åŸŸå¾®è°ƒ]
    â”‚   â”œâ”€â”€ E2[é‡‘èé¢†åŸŸå¾®è°ƒ]
    â”‚   â””â”€â”€ E3[æ³•å¾‹é¢†åŸŸå¾®è°ƒ]
    â”œâ”€â”€ F[ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²]
    â”‚   â”œâ”€â”€ F1[Kubernetesé…ç½®]
    â”‚   â”œâ”€â”€ F2[ç›‘æ§å‘Šè­¦]
    â”‚   â””â”€â”€ F3[æˆæœ¬ä¼˜åŒ–]
    â””â”€â”€ G[æ•ˆæœè¯„ä¼°ä½“ç³»]
        â”œâ”€â”€ G1[æ€§èƒ½æŒ‡æ ‡]
        â”œâ”€â”€ G2[A/Bæµ‹è¯•]
        â””â”€â”€ G3[é£é™©ç®¡æ§]
```

### 1.2 å­¦ä¹ ç›®æ ‡

**æ ¸å¿ƒæŠ€æœ¯æŒæ¡**:
- ç²¾é€šä¸»æµå‚æ•°é«˜æ•ˆå¾®è°ƒæŠ€æœ¯(LoRAã€QLoRAã€Adapterã€Prefix Tuningã€Prompt Tuning)
- æ·±å…¥ç†è§£æ¨¡å‹é‡åŒ–åŸç†å’Œå¤šç§å®è·µæ–¹æ³•
- æŒæ¡çŸ¥è¯†è’¸é¦å’Œæ¨¡å‹å‹ç¼©çš„å®Œæ•´æŠ€æœ¯æ ˆ

**åº”ç”¨èƒ½åŠ›åŸ¹å…»**:
- å…·å¤‡é’ˆå¯¹ä¸åŒè¡Œä¸šçš„æ¨¡å‹å¾®è°ƒå®šåˆ¶èƒ½åŠ›
- æŒæ¡ç”Ÿäº§ç¯å¢ƒçš„éƒ¨ç½²å’Œè¿ç»´æŠ€èƒ½
- å»ºç«‹å®Œå–„çš„å¾®è°ƒæ•ˆæœè¯„ä¼°å’Œé£é™©ç®¡æ§ä½“ç³»

**å·¥ç¨‹å®è·µæå‡**:
- èƒ½å¤Ÿè®¾è®¡å’Œå®æ–½ç«¯åˆ°ç«¯çš„å¾®è°ƒè§£å†³æ–¹æ¡ˆ
- å…·å¤‡æˆæœ¬æ§åˆ¶å’Œæ€§èƒ½ä¼˜åŒ–çš„å·¥ç¨‹æ€ç»´
- æŒæ¡ç›‘æ§å‘Šè­¦å’Œæ•…éšœå¤„ç†çš„æœ€ä½³å®è·µ

### 1.2 é€‚ç”¨äººç¾¤

- éœ€è¦å®šåˆ¶åŒ–å¤§æ¨¡å‹çš„å¼€å‘è€…
- å…³æ³¨æ¨¡å‹æ•ˆç‡å’Œæˆæœ¬çš„å·¥ç¨‹å¸ˆ
- å¯¹æ¨¡å‹å‹ç¼©æŠ€æœ¯æ„Ÿå…´è¶£çš„ç ”ç©¶äººå‘˜

## 2. å‚æ•°é«˜æ•ˆå¾®è°ƒ(PEFT)æŠ€æœ¯

### 2.1 LoRA (Low-Rank Adaptation)

```python
# LoRAå®ç°ç¤ºä¾‹
import torch
import torch.nn as nn
from torch.nn.parameter import Parameter

class LoRALayer(nn.Module):
    def __init__(self, in_features, out_features, rank=8, alpha=16):
        super().__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.rank = rank
        self.alpha = alpha
        
        # åŸå§‹æƒé‡çŸ©é˜µçš„ä½ç§©åˆ†è§£
        self.lora_A = Parameter(torch.zeros(in_features, rank))
        self.lora_B = Parameter(torch.zeros(rank, out_features))
        
        # ç¼©æ”¾å› å­
        self.scaling = alpha / rank
        
        # åˆå§‹åŒ–
        nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))
        nn.init.zeros_(self.lora_B)
    
    def forward(self, x):
        # åŸå§‹å‰å‘ä¼ æ’­ + LoRAé€‚é…
        return x + (x @ self.lora_A @ self.lora_B) * self.scaling

class LoRALinear(nn.Linear):
    def __init__(self, in_features, out_features, rank=8, alpha=16, **kwargs):
        super().__init__(in_features, out_features, **kwargs)
        self.lora_layer = LoRALayer(in_features, out_features, rank, alpha)
    
    def forward(self, x):
        # å†»ç»“åŸå§‹æƒé‡ï¼Œåªè®­ç»ƒLoRAå‚æ•°
        with torch.no_grad():
            original_output = super().forward(x)
        lora_output = self.lora_layer(x)
        return original_output + lora_output

# åœ¨Transformerä¸­åº”ç”¨LoRA
class LoraTransformer(nn.Module):
    def __init__(self, base_model, lora_config):
        super().__init__()
        self.base_model = base_model
        self.lora_config = lora_config
        
        # æ›¿æ¢æŒ‡å®šå±‚ä¸ºLoRAå±‚
        self._replace_with_lora()
    
    def _replace_with_lora(self):
        """å°†æ¨¡å‹ä¸­çš„çº¿æ€§å±‚æ›¿æ¢ä¸ºLoRAå±‚"""
        for name, module in self.base_model.named_modules():
            if isinstance(module, nn.Linear) and self._should_apply_lora(name):
                # åˆ›å»ºLoRAåŒ…è£…å™¨
                lora_linear = LoRALinear(
                    module.in_features,
                    module.out_features,
                    rank=self.lora_config.rank,
                    alpha=self.lora_config.alpha,
                    bias=module.bias is not None
                )
                
                # å¤åˆ¶åŸå§‹æƒé‡
                lora_linear.weight.data = module.weight.data.clone()
                if module.bias is not None:
                    lora_linear.bias.data = module.bias.data.clone()
                
                # æ›¿æ¢æ¨¡å—
                parent_module = self._get_parent_module(name)
                attr_name = name.split('.')[-1]
                setattr(parent_module, attr_name, lora_linear)
    
    def _should_apply_lora(self, name):
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥åœ¨è¯¥å±‚åº”ç”¨LoRA"""
        # é€šå¸¸åœ¨æ³¨æ„åŠ›æœºåˆ¶å’Œå‰é¦ˆç½‘ç»œä¸­åº”ç”¨
        return any(pattern in name for pattern in [
            'q_proj', 'k_proj', 'v_proj', 'o_proj',  # æ³¨æ„åŠ›æŠ•å½±å±‚
            'gate_proj', 'up_proj', 'down_proj'      # MLPå±‚
        ])
    
    def _get_parent_module(self, name):
        """è·å–çˆ¶æ¨¡å—"""
        modules = name.split('.')
        parent = self.base_model
        for module_name in modules[:-1]:
            parent = getattr(parent, module_name)
        return parent

# ä½¿ç”¨ç¤ºä¾‹
def train_lora_model():
    from transformers import AutoModelForCausalLM, AutoTokenizer
    
    # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
    model_name = "meta-llama/Llama-2-7b-hf"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    base_model = AutoModelForCausalLM.from_pretrained(model_name)
    
    # LoRAé…ç½®
    class LoraConfig:
        def __init__(self):
            self.rank = 8
            self.alpha = 16
            self.dropout = 0.1
    
    lora_config = LoraConfig()
    
    # åˆ›å»ºLoRAæ¨¡å‹
    lora_model = LoraTransformer(base_model, lora_config)
    
    # å†»ç»“åŸå§‹æ¨¡å‹å‚æ•°
    for param in lora_model.base_model.parameters():
        param.requires_grad = False
    
    # åªè®­ç»ƒLoRAå‚æ•°
    trainable_params = sum(p.numel() for p in lora_model.parameters() if p.requires_grad)
    total_params = sum(p.numel() for p in lora_model.parameters())
    
    print(f"å¯è®­ç»ƒå‚æ•°: {trainable_params:,} ({trainable_params/total_params*100:.2f}%)")
    print(f"æ€»å‚æ•°: {total_params:,}")
    
    return lora_model, tokenizer
```

### 2.2 QLoRA (Quantized LoRA)

```python
# QLoRAå®ç° - ç»“åˆ4-bité‡åŒ–å’ŒLoRA
import bitsandbytes as bnb
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training

def create_qlora_model(model_name, quantization_config):
    """åˆ›å»ºQLoRAæ¨¡å‹"""
    from transformers import AutoModelForCausalLM, BitsAndBytesConfig
    
    # 4-bité‡åŒ–é…ç½®
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16
    )
    
    # åŠ è½½é‡åŒ–æ¨¡å‹
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        quantization_config=bnb_config,
        device_map="auto"
    )
    
    # å‡†å¤‡æ¨¡å‹è¿›è¡Œk-bitè®­ç»ƒ
    model = prepare_model_for_kbit_training(model)
    
    # LoRAé…ç½®
    lora_config = LoraConfig(
        r=64,
        lora_alpha=16,
        target_modules=[
            "q_proj", "k_proj", "v_proj", "o_proj",
            "gate_proj", "up_proj", "down_proj"
        ],
        lora_dropout=0.1,
        bias="none",
        task_type="CAUSAL_LM"
    )
    
    # åº”ç”¨LoRA
    model = get_peft_model(model, lora_config)
    
    return model

# è®­ç»ƒè„šæœ¬
def train_qlora():
    model = create_qlora_model("meta-llama/Llama-2-7b-hf", {})
    
    # è®­ç»ƒé…ç½®
    training_args = TrainingArguments(
        output_dir="./qlora-results",
        per_device_train_batch_size=4,
        gradient_accumulation_steps=4,
        warmup_steps=100,
        max_steps=1000,
        learning_rate=2e-4,
        fp16=True,
        logging_steps=20,
        save_steps=200,
        optim="paged_adamw_8bit"  # 8-bitä¼˜åŒ–å™¨
    )
    
    # Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        tokenizer=tokenizer
    )
    
    # å¼€å§‹è®­ç»ƒ
    trainer.train()
```

### 2.3 Adapterå¾®è°ƒ

```python
# Adapterå®ç°
class AdapterLayer(nn.Module):
    def __init__(self, hidden_size, bottleneck_size=64, dropout=0.1):
        super().__init__()
        self.hidden_size = hidden_size
        self.bottleneck_size = bottleneck_size
        
        # Adapterç»“æ„ï¼šDown projection -> Non-linearity -> Up projection
        self.down_proj = nn.Linear(hidden_size, bottleneck_size)
        self.activation = nn.GELU()
        self.up_proj = nn.Linear(bottleneck_size, hidden_size)
        self.dropout = nn.Dropout(dropout)
        
        # LayerNorm
        self.adapter_norm = nn.LayerNorm(hidden_size)
        
        # åˆå§‹åŒ–
        nn.init.normal_(self.down_proj.weight, std=1e-3)
        nn.init.zeros_(self.down_proj.bias)
        nn.init.normal_(self.up_proj.weight, std=1e-3)
        nn.init.zeros_(self.up_proj.bias)
    
    def forward(self, x):
        # æ®‹å·®è¿æ¥
        residual = x
        x = self.adapter_norm(x)
        x = self.down_proj(x)
        x = self.activation(x)
        x = self.up_proj(x)
        x = self.dropout(x)
        return residual + x

class AdapterTransformer(nn.Module):
    def __init__(self, base_model, adapter_config):
        super().__init__()
        self.base_model = base_model
        self.adapter_config = adapter_config
        
        # åœ¨æŒ‡å®šä½ç½®æ’å…¥Adapter
        self._insert_adapters()
    
    def _insert_adapters(self):
        """åœ¨Transformerå±‚ä¸­æ’å…¥Adapter"""
        for name, module in self.base_model.named_modules():
            if self._should_insert_adapter(name, module):
                # åˆ›å»ºAdapteråŒ…è£…å™¨
                adapter_wrapper = self._create_adapter_wrapper(module)
                
                # æ›¿æ¢æ¨¡å—
                parent_module = self._get_parent_module(name)
                attr_name = name.split('.')[-1]
                setattr(parent_module, attr_name, adapter_wrapper)
    
    def _should_insert_adapter(self, name, module):
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥æ’å…¥Adapter"""
        # åœ¨æ³¨æ„åŠ›è¾“å‡ºå’ŒFFNè¾“å‡ºåæ’å…¥
        return isinstance(module, nn.Linear) and any(pattern in name for pattern in [
            'attention.output.dense',  # æ³¨æ„åŠ›è¾“å‡º
            'output.dense'             # FFNè¾“å‡º
        ])

# ä½¿ç”¨HuggingFace PEFTåº“çš„Adapter
from peft import AdaLoraConfig, get_peft_model

def create_adapter_model(model_name):
    model = AutoModelForCausalLM.from_pretrained(model_name)
    
    # AdaLoraé…ç½®
    adalora_config = AdaLoraConfig(
        r=8,
        lora_alpha=32,
        target_modules=["q_proj", "v_proj"],
        lora_dropout=0.1,
        bias="none",
        task_type="CAUSAL_LM"
    )
    
    model = get_peft_model(model, adalora_config)
    return model
```

### 2.4 Prefix Tuning (å‰ç¼€è°ƒä¼˜)

```python
# Prefix Tuningå®ç°
class PrefixTuningLayer(nn.Module):
    def __init__(self, config, num_virtual_tokens=20):
        super().__init__()
        self.num_virtual_tokens = num_virtual_tokens
        self.hidden_size = config.hidden_size
        
        # è™šæ‹Ÿtokençš„å‰ç¼€å‚æ•°
        self.prefix_embeddings = nn.Embedding(num_virtual_tokens, self.hidden_size)
        
        # MLPç½‘ç»œç”Ÿæˆkeyå’Œvalueå‰ç¼€
        self.prefix_mlp = nn.Sequential(
            nn.Linear(self.hidden_size, self.hidden_size),
            nn.Tanh(),
            nn.Linear(self.hidden_size, 2 * config.num_hidden_layers * self.hidden_size)
        )
        
        # åˆå§‹åŒ–
        nn.init.normal_(self.prefix_embeddings.weight, std=0.02)
    
    def forward(self, batch_size):
        # ç”Ÿæˆè™šæ‹Ÿtoken
        virtual_tokens = self.prefix_embeddings.weight.unsqueeze(0).expand(batch_size, -1, -1)
        
        # é€šè¿‡MLPç”Ÿæˆå‰ç¼€
        prefix_projections = self.prefix_mlp(virtual_tokens)
        
        # é‡å¡‘ä¸ºkeyå’Œvalueæ ¼å¼
        prefix_projections = prefix_projections.view(
            batch_size, self.num_virtual_tokens, 
            2, config.num_hidden_layers, self.hidden_size
        )
        
        past_key_values = tuple(
            (prefix_projections[:, :, 0, layer_idx], prefix_projections[:, :, 1, layer_idx])
            for layer_idx in range(config.num_hidden_layers)
        )
        
        return past_key_values

class PrefixTuningModel(nn.Module):
    def __init__(self, base_model, num_virtual_tokens=20):
        super().__init__()
        self.base_model = base_model
        self.config = base_model.config
        
        # å†»ç»“åŸå§‹æ¨¡å‹
        for param in self.base_model.parameters():
            param.requires_grad = False
        
        # æ·»åŠ å‰ç¼€è°ƒä¼˜å±‚
        self.prefix_layer = PrefixTuningLayer(self.config, num_virtual_tokens)
        
        # åªè®­ç»ƒå‰ç¼€å‚æ•°
        self.trainable_params = sum(p.numel() for p in self.prefix_layer.parameters())
        self.total_params = sum(p.numel() for p in self.parameters())
        
        print(f"å‰ç¼€è°ƒä¼˜å‚æ•°: {self.trainable_params:,} ({self.trainable_params/self.total_params*100:.2f}%)")
    
    def forward(self, input_ids, attention_mask=None, labels=None):
        batch_size = input_ids.shape[0]
        
        # ç”Ÿæˆå‰ç¼€
        past_key_values = self.prefix_layer(batch_size)
        
        # å‰å‘ä¼ æ’­
        outputs = self.base_model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            past_key_values=past_key_values,
            labels=labels
        )
        
        return outputs
```

### 2.5 Prompt Tuning (æç¤ºè°ƒä¼˜)

```python
# Prompt Tuningå®ç°
class PromptTuningLayer(nn.Module):
    def __init__(self, config, num_virtual_tokens=50):
        super().__init__()
        self.num_virtual_tokens = num_virtual_tokens
        self.hidden_size = config.hidden_size
        
        # å¯å­¦ä¹ çš„æç¤ºtokenåµŒå…¥
        self.prompt_embeddings = nn.Embedding(num_virtual_tokens, self.hidden_size)
        
        # åˆå§‹åŒ–ä¸ºæ¥è¿‘è¯æ±‡è¡¨å¹³å‡å€¼
        self._init_prompt_embeddings(config)
    
    def _init_prompt_embeddings(self, config):
        """åˆå§‹åŒ–æç¤ºåµŒå…¥"""
        # ä»è¯æ±‡è¡¨ä¸­é‡‡æ ·åˆå§‹åŒ–
        vocab_size = config.vocab_size
        sample_indices = torch.randint(0, vocab_size, (self.num_virtual_tokens,))
        
        # è¿™é‡Œéœ€è¦è®¿é—®æ¨¡å‹çš„è¯åµŒå…¥å±‚æ¥è·å–åˆå§‹å€¼
        # åœ¨å®é™…ä½¿ç”¨ä¸­éœ€è¦ä¼ å…¥embeddingå±‚
        pass
    
    def forward(self, input_embeds):
        batch_size = input_embeds.shape[0]
        
        # ç”Ÿæˆæç¤ºåµŒå…¥
        prompt_embeds = self.prompt_embeddings.weight.unsqueeze(0).expand(batch_size, -1, -1)
        
        # æ‹¼æ¥æç¤ºå’Œè¾“å…¥
        combined_embeds = torch.cat([prompt_embeds, input_embeds], dim=1)
        
        return combined_embeds

class PromptTuningModel(nn.Module):
    def __init__(self, base_model, num_virtual_tokens=50):
        super().__init__()
        self.base_model = base_model
        self.config = base_model.config
        
        # å†»ç»“åŸå§‹æ¨¡å‹
        for param in self.base_model.parameters():
            param.requires_grad = False
        
        # æ·»åŠ æç¤ºè°ƒä¼˜å±‚
        self.prompt_layer = PromptTuningLayer(self.config, num_virtual_tokens)
        
        # åªè®­ç»ƒæç¤ºå‚æ•°
        self.trainable_params = sum(p.numel() for p in self.prompt_layer.parameters())
        self.total_params = sum(p.numel() for p in self.parameters())
        
        print(f"æç¤ºè°ƒä¼˜å‚æ•°: {self.trainable_params:,} ({self.trainable_params/self.total_params*100:.2f}%)")
    
    def forward(self, input_ids, attention_mask=None, labels=None):
        # è·å–è¾“å…¥åµŒå…¥
        input_embeds = self.base_model.get_input_embeddings()(input_ids)
        
        # åº”ç”¨æç¤ºè°ƒä¼˜
        combined_embeds = self.prompt_layer(input_embeds)
        
        # æ„é€ æ–°çš„attention mask
        batch_size = input_ids.shape[0]
        prompt_attention_mask = torch.ones(
            batch_size, self.prompt_layer.num_virtual_tokens, 
            device=input_ids.device
        )
        
        if attention_mask is not None:
            combined_attention_mask = torch.cat([prompt_attention_mask, attention_mask], dim=1)
        else:
            combined_attention_mask = None
        
        # å‰å‘ä¼ æ’­
        outputs = self.base_model(
            inputs_embeds=combined_embeds,
            attention_mask=combined_attention_mask,
            labels=labels
        )
        
        return outputs

# ä½¿ç”¨HuggingFace PEFTåº“çš„Prompt Tuning
from peft import PromptTuningConfig, get_peft_model

def create_prompt_tuning_model(model_name, num_virtual_tokens=50):
    from transformers import AutoModelForCausalLM
    
    model = AutoModelForCausalLM.from_pretrained(model_name)
    
    # Prompt Tuningé…ç½®
    prompt_config = PromptTuningConfig(
        task_type="CAUSAL_LM",
        num_virtual_tokens=num_virtual_tokens,
        prompt_tuning_init="TEXT",
        prompt_tuning_init_text="Classify if the sentiment of this review is positive or negative: ",
        tokenizer_name_or_path=model_name
    )
    
    model = get_peft_model(model, prompt_config)
    return model
```

## 3. æ¨¡å‹é‡åŒ–æŠ€æœ¯

### 3.1 Post-Training Quantization (PTQ)

```python
# è®­ç»ƒåé‡åŒ–ç¤ºä¾‹
import torch.quantization as quant

class QuantizableModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(3, 64, 3, padding=1)
        self.relu1 = nn.ReLU()
        self.pool1 = nn.MaxPool2d(2)
        self.conv2 = nn.Conv2d(64, 128, 3, padding=1)
        self.relu2 = nn.ReLU()
        self.pool2 = nn.AdaptiveAvgPool2d((1, 1))
        self.flatten = nn.Flatten()
        self.fc = nn.Linear(128, 10)
    
    def forward(self, x):
        x = self.pool1(self.relu1(self.conv1(x)))
        x = self.pool2(self.relu2(self.conv2(x)))
        x = self.flatten(x)
        x = self.fc(x)
        return x

def post_training_quantization():
    # åˆ›å»ºæ¨¡å‹
    model = QuantizableModel()
    
    # è®¾ç½®é‡åŒ–é…ç½®
    model.qconfig = quant.get_default_qconfig('fbgemm')
    
    # å‡†å¤‡é‡åŒ–
    quant.prepare(model, inplace=True)
    
    # æ ¡å‡† - ä½¿ç”¨æ ¡å‡†æ•°æ®é›†
    calibration_data = get_calibration_dataset()
    model.eval()
    with torch.no_grad():
        for data, _ in calibration_data:
            model(data)
    
    # è½¬æ¢ä¸ºé‡åŒ–æ¨¡å‹
    quant.convert(model, inplace=True)
    
    return model

# ä½¿ç”¨Intel Neural Compressor
from neural_compressor import QuantizationAwareTrainingConfig
from neural_compressor.training import prepare_compression

def intel_quantization_training():
    model = create_model()
    train_dataloader = get_train_dataloader()
    
    # é‡åŒ–æ„ŸçŸ¥è®­ç»ƒé…ç½®
    conf = QuantizationAwareTrainingConfig(
        approach="quant_aware_training",
        recipes={
            "smooth_quant": True,
            "smooth_quant_args": {
                "alpha": 0.5
            }
        }
    )
    
    # å‡†å¤‡å‹ç¼©
    compression_manager = prepare_compression(model, conf)
    compression_manager.callbacks.on_train_begin()
    
    # è®­ç»ƒå¾ªç¯
    model = compression_manager.model
    for epoch in range(num_epochs):
        for batch in train_dataloader:
            # æ­£å¸¸è®­ç»ƒæ­¥éª¤
            loss = train_step(model, batch)
            compression_manager.callbacks.on_step_end()
    
    compression_manager.callbacks.on_train_end()
    return model
```

### 3.2 Quantization-Aware Training (QAT)

```python
# é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ
def quantization_aware_training():
    from torch.quantization import fuse_modules, prepare_qat, convert
    
    # åˆ›å»ºæ¨¡å‹å¹¶èåˆæ¨¡å—
    model = create_model()
    model.eval()
    
    # èåˆå¯ä»¥èåˆçš„æ¨¡å—
    model = fuse_modules(model, [['conv1', 'relu1'], ['conv2', 'relu2']])
    
    # è®¾ç½®QATé…ç½®
    model.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')
    
    # å‡†å¤‡QAT
    model.train()
    prepare_qat(model, inplace=True)
    
    # QATè®­ç»ƒ
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    criterion = nn.CrossEntropyLoss()
    
    for epoch in range(qat_epochs):
        for data, target in train_loader:
            optimizer.zero_grad()
            output = model(data)
            loss = criterion(output, target)
            loss.backward()
            optimizer.step()
    
    # è½¬æ¢ä¸ºé‡åŒ–æ¨¡å‹
    model.eval()
    convert(model, inplace=True)
    
    return model
```

### 3.3 GPTQé‡åŒ–

```python
# GPTQé‡åŒ–å®ç°
import numpy as np
from tqdm import tqdm

class GPTQQuantizer:
    def __init__(self, bits=4, group_size=128):
        self.bits = bits
        self.group_size = group_size
        self.max_q = 2 ** bits - 1
        
    def quantize_weight(self, weight):
        """GPTQé‡åŒ–å•ä¸ªæƒé‡çŸ©é˜µ"""
        # æŒ‰ç»„å¤„ç†æƒé‡
        out_features, in_features = weight.shape
        new_weight = weight.clone()
        
        # è®¡ç®—ç»„æ•°
        num_groups = (in_features + self.group_size - 1) // self.group_size
        
        scales = []
        zeros = []
        
        for i in range(num_groups):
            start_idx = i * self.group_size
            end_idx = min((i + 1) * self.group_size, in_features)
            
            # æå–å½“å‰ç»„
            group_weight = weight[:, start_idx:end_idx]
            
            # è®¡ç®—æ¯ç»„çš„scaleå’Œzero point
            w_max = group_weight.max(dim=1, keepdim=True)[0]
            w_min = group_weight.min(dim=1, keepdim=True)[0]
            
            scale = (w_max - w_min) / self.max_q
            zero_point = (-w_min / scale).round()
            
            # é‡åŒ–
            quant_weight = ((group_weight / scale + zero_point).round().clamp(0, self.max_q))
            
            # åé‡åŒ–éªŒè¯
            dequant_weight = (quant_weight - zero_point) * scale
            
            # æ›´æ–°æƒé‡
            new_weight[:, start_idx:end_idx] = dequant_weight
            
            scales.append(scale)
            zeros.append(zero_point)
        
        return new_weight, scales, zeros

def apply_gptq_quantization(model, dataloader):
    """åº”ç”¨GPTQé‡åŒ–åˆ°æ•´ä¸ªæ¨¡å‹"""
    quantizer = GPTQQuantizer(bits=4, group_size=128)
    
    # æ”¶é›†Hessianä¿¡æ¯
    hessian_dict = {}
    
    # Hookå‡½æ•°æ”¶é›†äºŒé˜¶å¯¼æ•°ä¿¡æ¯
    def hessian_hook(module, grad_input, grad_output):
        if isinstance(module, nn.Linear):
            weight = module.weight.data
            hessian = grad_output[0].pow(2).mean(dim=0)
            hessian_dict[id(module)] = hessian
    
    # æ³¨å†Œhooks
    hooks = []
    for module in model.modules():
        if isinstance(module, nn.Linear):
            hook = module.register_backward_hook(hessian_hook)
            hooks.append(hook)
    
    # å‰å‘ä¼ æ’­æ”¶é›†Hessian
    model.eval()
    for batch in tqdm(dataloader, desc="Collecting Hessian"):
        try:
            outputs = model(**batch)
            loss = outputs.loss
            loss.backward()
        except:
            pass
    
    # ç§»é™¤hooks
    for hook in hooks:
        hook.remove()
    
    # åº”ç”¨é‡åŒ–
    for name, module in model.named_modules():
        if isinstance(module, nn.Linear) and id(module) in hessian_dict:
            # ä½¿ç”¨Hessianä¿¡æ¯è¿›è¡Œæ›´ç²¾ç¡®çš„é‡åŒ–
            hessian = hessian_dict[id(module)]
            original_weight = module.weight.data
            
            # GPTQé‡åŒ–
            quantized_weight, scales, zeros = quantizer.quantize_weight(original_weight)
            
            # æ›´æ–°æ¨¡å—æƒé‡
            module.weight.data = quantized_weight
            
            # ä¿å­˜é‡åŒ–å‚æ•°
            module.scales = scales
            module.zeros = zeros

# ä½¿ç”¨AutoGPTQåº“
from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig

def autogptq_quantization(model_name, quantize_config):
    # é‡åŒ–é…ç½®
    quantize_config = BaseQuantizeConfig(
        bits=4,
        group_size=128,
        damp_percent=0.01,
        desc_act=False,
        static_groups=False,
        sym=True,
        true_sequential=True
    )
    
    # åŠ è½½æ¨¡å‹
    model = AutoGPTQForCausalLM.from_pretrained(model_name, quantize_config)
    
    # å‡†å¤‡æ•°æ®é›†
    examples = get_quantization_examples()
    
    # é‡åŒ–æ¨¡å‹
    model.quantize(examples)
    
    # ä¿å­˜é‡åŒ–æ¨¡å‹
    model.save_quantized("./gptq-quantized-model")
    
    return model
```

## 4. çŸ¥è¯†è’¸é¦æŠ€æœ¯

### 4.1 åŸºç¡€çŸ¥è¯†è’¸é¦

```python
# çŸ¥è¯†è’¸é¦å®ç°
class DistillationTrainer:
    def __init__(self, student_model, teacher_model, temperature=4.0, alpha=0.7):
        self.student = student_model
        self.teacher = teacher_model
        self.temperature = temperature
        self.alpha = alpha
        
        # å†»ç»“æ•™å¸ˆæ¨¡å‹
        for param in self.teacher.parameters():
            param.requires_grad = False
    
    def distillation_loss(self, student_logits, teacher_logits, true_labels):
        """è®¡ç®—è’¸é¦æŸå¤±"""
        # è½¯æ ‡ç­¾æŸå¤± (KLæ•£åº¦)
        soft_student = torch.softmax(student_logits / self.temperature, dim=-1)
        soft_teacher = torch.softmax(teacher_logits / self.temperature, dim=-1)
        
        kl_loss = torch.nn.KLDivLoss(reduction='batchmean')(
            torch.log(soft_student), soft_teacher
        ) * (self.temperature ** 2)
        
        # ç¡¬æ ‡ç­¾æŸå¤±
        hard_loss = torch.nn.CrossEntropyLoss()(student_logits, true_labels)
        
        # ç»„åˆæŸå¤±
        total_loss = self.alpha * kl_loss + (1 - self.alpha) * hard_loss
        return total_loss

def train_distilled_model():
    # åˆ›å»ºæ•™å¸ˆå’Œå­¦ç”Ÿæ¨¡å‹
    teacher = AutoModelForSequenceClassification.from_pretrained("bert-large-uncased")
    student = AutoModelForSequenceClassification.from_pretrained("bert-base-uncased")
    
    # åˆ›å»ºè’¸é¦è®­ç»ƒå™¨
    trainer = DistillationTrainer(student, teacher, temperature=4.0, alpha=0.7)
    
    # è®­ç»ƒå¾ªç¯
    optimizer = torch.optim.Adam(student.parameters(), lr=2e-5)
    
    for epoch in range(num_epochs):
        for batch in train_dataloader:
            # æ•™å¸ˆæ¨¡å‹é¢„æµ‹
            with torch.no_grad():
                teacher_outputs = teacher(**batch)
                teacher_logits = teacher_outputs.logits
            
            # å­¦ç”Ÿæ¨¡å‹é¢„æµ‹
            student_outputs = student(**batch)
            student_logits = student_outputs.logits
            
            # è®¡ç®—è’¸é¦æŸå¤±
            loss = trainer.distillation_loss(
                student_logits, teacher_logits, batch["labels"]
            )
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
```

### 4.2 ç‰¹å¾è’¸é¦

```python
# ç‰¹å¾å±‚è’¸é¦
class FeatureDistillationTrainer:
    def __init__(self, student_model, teacher_model, feature_layers):
        self.student = student_model
        self.teacher = teacher_model
        self.feature_layers = feature_layers
        
        # æ³¨å†Œhookæå–ç‰¹å¾
        self.teacher_features = {}
        self.student_features = {}
        
        self._register_hooks()
    
    def _register_hooks(self):
        """æ³¨å†Œç‰¹å¾æå–hook"""
        # æ•™å¸ˆæ¨¡å‹hook
        for name, module in self.teacher.named_modules():
            if name in self.feature_layers:
                module.register_forward_hook(
                    lambda module, input, output, name=name: 
                    self.teacher_features.update({name: output})
                )
        
        # å­¦ç”Ÿæ¨¡å‹hook
        for name, module in self.student.named_modules():
            if name in self.feature_layers:
                module.register_forward_hook(
                    lambda module, input, output, name=name: 
                    self.student_features.update({name: output})
                )
    
    def feature_loss(self):
        """è®¡ç®—ç‰¹å¾è’¸é¦æŸå¤±"""
        loss = 0
        for layer_name in self.feature_layers:
            if layer_name in self.teacher_features and layer_name in self.student_features:
                teacher_feat = self.teacher_features[layer_name]
                student_feat = self.student_features[layer_name]
                
                # ç‰¹å¾åŒ¹é…æŸå¤±
                feat_loss = torch.nn.MSELoss()(
                    student_feat, teacher_feat.detach()
                )
                loss += feat_loss
        
        return loss

# ä½¿ç”¨ç¤ºä¾‹
def train_with_feature_distillation():
    teacher = create_teacher_model()
    student = create_student_model()
    
    # é€‰æ‹©è¦è’¸é¦çš„ç‰¹å¾å±‚
    feature_layers = ["encoder.layer.6", "encoder.layer.10"]
    
    trainer = FeatureDistillationTrainer(student, teacher, feature_layers)
    
    # è®­ç»ƒ
    for batch in train_dataloader:
        # å‰å‘ä¼ æ’­æ”¶é›†ç‰¹å¾
        _ = teacher(**batch)
        _ = student(**batch)
        
        # è®¡ç®—ç‰¹å¾æŸå¤±
        feat_loss = trainer.feature_loss()
        
        # ç»“åˆå…¶ä»–æŸå¤±è¿›è¡Œè®­ç»ƒ
        total_loss = classification_loss + 0.5 * feat_loss
```

## 5. æ¨¡å‹å‹ç¼©ä¸ä¼˜åŒ–

### 5.1 æƒé‡å‰ªæ

```python
# ç»“æ„åŒ–å‰ªæ
class PruningManager:
    def __init__(self, model, pruning_ratio=0.5):
        self.model = model
        self.pruning_ratio = pruning_ratio
        self.masks = {}
        
    def compute_importance(self, dataloader):
        """è®¡ç®—æƒé‡é‡è¦æ€§"""
        importance_scores = {}
        
        # ä½¿ç”¨æ³°å‹’å±•å¼€è¿‘ä¼¼è®¡ç®—é‡è¦æ€§
        for name, param in self.model.named_parameters():
            if 'weight' in name:
                # åŸºäºæ¢¯åº¦å’Œæƒé‡è®¡ç®—é‡è¦æ€§
                importance = torch.abs(param * param.grad) if param.grad is not None else torch.zeros_like(param)
                importance_scores[name] = importance
        
        return importance_scores
    
    def create_pruning_mask(self, importance_scores):
        """åˆ›å»ºå‰ªææ©ç """
        for name, scores in importance_scores.items():
            # è®¡ç®—é˜ˆå€¼
            threshold = torch.quantile(scores.flatten(), self.pruning_ratio)
            
            # åˆ›å»ºæ©ç 
            mask = (scores > threshold).float()
            self.masks[name] = mask
    
    def apply_pruning(self):
        """åº”ç”¨å‰ªæ"""
        for name, param in self.model.named_parameters():
            if name in self.masks:
                param.data *= self.masks[name]

def iterative_pruning_training():
    model = create_model()
    pruning_manager = PruningManager(model, pruning_ratio=0.3)
    
    for pruning_step in range(5):  # è¿­ä»£å‰ªæ5æ¬¡
        # æ­£å¸¸è®­ç»ƒ
        train_model(model, train_dataloader)
        
        # è®¡ç®—é‡è¦æ€§å¹¶å‰ªæ
        importance_scores = pruning_manager.compute_importance(train_dataloader)
        pruning_manager.create_pruning_mask(importance_scores)
        pruning_manager.apply_pruning()
        
        # å¾®è°ƒ
        fine_tune_model(model, train_dataloader, epochs=2)
```

### 5.2 çŸ¥è¯†å›¾è°±è’¸é¦

```python
# çŸ¥è¯†å›¾è°±è’¸é¦
class KnowledgeGraphDistiller:
    def __init__(self, teacher_model, student_model):
        self.teacher = teacher_model
        self.student = student_model
    
    def extract_relations(self, texts):
        """ä»æ–‡æœ¬ä¸­æå–å…³ç³»ä¸‰å…ƒç»„"""
        # ä½¿ç”¨é¢„è®­ç»ƒçš„å…³ç³»æŠ½å–æ¨¡å‹
        relations = []
        for text in texts:
            # è¿™é‡Œåº”è¯¥æ˜¯å®é™…çš„å…³ç³»æŠ½å–é€»è¾‘
            extracted_relations = self.extract_triples(text)
            relations.extend(extracted_relations)
        return relations
    
    def relation_preservation_loss(self, student_relations, teacher_relations):
        """å…³ç³»ä¿æŒæŸå¤±"""
        # è®¡ç®—å…³ç³»ç›¸ä¼¼åº¦æŸå¤±
        loss = 0
        for s_rel, t_rel in zip(student_relations, teacher_relations):
            # å…³ç³»åµŒå…¥ç›¸ä¼¼åº¦
            rel_sim = torch.cosine_similarity(s_rel, t_rel, dim=-1)
            loss += (1 - rel_sim).mean()
        return loss

# ä½¿ç”¨Transformers.jsè¿›è¡Œè½»é‡åŒ–éƒ¨ç½²
def export_for_edge_deployment(model):
    """å¯¼å‡ºæ¨¡å‹ç”¨äºè¾¹ç¼˜è®¾å¤‡éƒ¨ç½²"""
    
    # 1. æ¨¡å‹é‡åŒ–
    quantized_model = torch.quantization.quantize_dynamic(
        model, {nn.Linear}, dtype=torch.qint8
    )
    
    # 2. ONNXå¯¼å‡º
    dummy_input = torch.randn(1, 512)  # ç¤ºä¾‹è¾“å…¥
    torch.onnx.export(
        quantized_model,
        dummy_input,
        "model_quantized.onnx",
        export_params=True,
        opset_version=11,
        do_constant_folding=True,
        input_names=['input'],
        output_names=['output'],
        dynamic_axes={
            'input': {0: 'batch_size', 1: 'sequence'},
            'output': {0: 'batch_size', 1: 'sequence'}
        }
    )
    
    # 3. TensorFlow Liteè½¬æ¢
    import tensorflow as tf
    
    # è½¬æ¢ONNXåˆ°TensorFlow
    converter = tf.lite.TFLiteConverter.from_saved_model("tf_model")
    converter.optimizations = [tf.lite.Optimize.DEFAULT]
    converter.target_spec.supported_types = [tf.float16]
    
    tflite_model = converter.convert()
    
    with open('model.tflite', 'wb') as f:
        f.write(tflite_model)
```

## 6. å¾®è°ƒæ•ˆæœè¯„ä¼°

### 6.1 è¯„ä¼°æŒ‡æ ‡ä½“ç³»

```python
# å¾®è°ƒæ•ˆæœç»¼åˆè¯„ä¼°
class ModelEvaluator:
    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer
    
    def evaluate_task_performance(self, test_dataset):
        """ä»»åŠ¡æ€§èƒ½è¯„ä¼°"""
        self.model.eval()
        predictions = []
        true_labels = []
        
        with torch.no_grad():
            for batch in test_dataset:
                outputs = self.model(**batch)
                preds = torch.argmax(outputs.logits, dim=-1)
                predictions.extend(preds.cpu().tolist())
                true_labels.extend(batch["labels"].cpu().tolist())
        
        # è®¡ç®—æŒ‡æ ‡
        accuracy = accuracy_score(true_labels, predictions)
        precision = precision_score(true_labels, predictions, average='macro')
        recall = recall_score(true_labels, predictions, average='macro')
        f1 = f1_score(true_labels, predictions, average='macro')
        
        return {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1': f1
        }
    
    def evaluate_efficiency(self):
        """æ•ˆç‡è¯„ä¼°"""
        # æ¨¡å‹å¤§å°
        model_size = sum(p.numel() for p in self.model.parameters()) * 4 / (1024**2)  # MB
        
        # æ¨ç†æ—¶é—´
        dummy_input = self.tokenizer("test text", return_tensors="pt")
        start_time = time.time()
        with torch.no_grad():
            _ = self.model(**dummy_input)
        inference_time = (time.time() - start_time) * 1000  # ms
        
        # å†…å­˜ä½¿ç”¨
        torch.cuda.reset_peak_memory_stats()
        with torch.no_grad():
            _ = self.model(**dummy_input)
        memory_usage = torch.cuda.max_memory_allocated() / (1024**2)  # MB
        
        return {
            'model_size_mb': model_size,
            'inference_time_ms': inference_time,
            'memory_usage_mb': memory_usage
        }
    
    def evaluate_robustness(self, perturbation_tests):
        """é²æ£’æ€§è¯„ä¼°"""
        self.model.eval()
        robustness_scores = []
        
        for perturbation_func in perturbation_tests:
            original_preds = []
            perturbed_preds = []
            
            for text in test_texts:
                # åŸå§‹é¢„æµ‹
                inputs = self.tokenizer(text, return_tensors="pt")
                with torch.no_grad():
                    orig_pred = self.model(**inputs).logits
                original_preds.append(orig_pred)
                
                # æ‰°åŠ¨åé¢„æµ‹
                perturbed_text = perturbation_func(text)
                inputs = self.tokenizer(perturbed_text, return_tensors="pt")
                with torch.no_grad():
                    pert_pred = self.model(**inputs).logits
                perturbed_preds.append(pert_pred)
            
            # è®¡ç®—é¢„æµ‹ä¸€è‡´æ€§
            consistency = self._calculate_consistency(original_preds, perturbed_preds)
            robustness_scores.append(consistency)
        
        return {
            'average_robustness': np.mean(robustness_scores),
            'robustness_by_type': dict(zip(
                [func.__name__ for func in perturbation_tests], 
                robustness_scores
            ))
        }
    
    def _calculate_consistency(self, orig_preds, pert_preds):
        """è®¡ç®—é¢„æµ‹ä¸€è‡´æ€§"""
        consistent = 0
        for orig, pert in zip(orig_preds, pert_preds):
            if torch.argmax(orig) == torch.argmax(pert):
                consistent += 1
        return consistent / len(orig_preds)

# ä½¿ç”¨ç¤ºä¾‹
def comprehensive_evaluation():
    evaluator = ModelEvaluator(model, tokenizer)
    
    # ä»»åŠ¡æ€§èƒ½è¯„ä¼°
    task_metrics = evaluator.evaluate_task_performance(test_dataloader)
    
    # æ•ˆç‡è¯„ä¼°
    efficiency_metrics = evaluator.evaluate_efficiency()
    
    # é²æ£’æ€§è¯„ä¼°
    perturbations = [
        lambda x: x.lower(),  # å°å†™æ‰°åŠ¨
        lambda x: ''.join([c if random.random() > 0.1 else '' for c in x]),  # å­—ç¬¦åˆ é™¤
        lambda x: ' '.join(x.split()[::-1])  # è¯åºé¢ å€’
    ]
    robustness_metrics = evaluator.evaluate_robustness(perturbations)
    
    # ç»¼åˆæŠ¥å‘Š
    report = {
        'task_performance': task_metrics,
        'efficiency': efficiency_metrics,
        'robustness': robustness_metrics,
        'overall_score': self._calculate_overall_score(
            task_metrics, efficiency_metrics, robustness_metrics
        )
    }
    
    return report
```

### 6.2 A/Bæµ‹è¯•æ¡†æ¶

```python
# A/Bæµ‹è¯•å®ç°
class ABTester:
    def __init__(self, model_a, model_b, traffic_split=0.5):
        self.models = {'A': model_a, 'B': model_b}
        self.traffic_split = traffic_split
        self.results = {'A': [], 'B': []}
    
    def route_request(self, request):
        """è·¯ç”±è¯·æ±‚åˆ°ä¸åŒæ¨¡å‹"""
        if random.random() < self.traffic_split:
            return 'A', self.models['A']
        else:
            return 'B', self.models['B']
    
    def collect_metrics(self, model_id, response_time, accuracy, user_feedback):
        """æ”¶é›†æŒ‡æ ‡"""
        self.results[model_id].append({
            'response_time': response_time,
            'accuracy': accuracy,
            'feedback': user_feedback,
            'timestamp': time.time()
        })
    
    def analyze_results(self):
        """åˆ†æA/Bæµ‹è¯•ç»“æœ"""
        analysis = {}
        for model_id in ['A', 'B']:
            results = self.results[model_id]
            if results:
                analysis[model_id] = {
                    'avg_response_time': np.mean([r['response_time'] for r in results]),
                    'accuracy': np.mean([r['accuracy'] for r in results]),
                    'sample_size': len(results)
                }
        
        # ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ
        if len(self.results['A']) > 0 and len(self.results['B']) > 0:
            a_times = [r['response_time'] for r in self.results['A']]
            b_times = [r['response_time'] for r in self.results['B']]
            
            # tæ£€éªŒ
            t_stat, p_value = scipy.stats.ttest_ind(a_times, b_times)
            analysis['statistical_test'] = {
                't_statistic': t_stat,
                'p_value': p_value,
                'significant': p_value < 0.05
            }
        
        return analysis

# åœ¨çº¿è¯„ä¼°ç³»ç»Ÿ
class OnlineEvaluator:
    def __init__(self, reference_model):
        self.reference_model = reference_model
        self.metrics_buffer = []
    
    def evaluate_online(self, input_text, predicted_output, true_output=None):
        """åœ¨çº¿è¯„ä¼°å•ä¸ªé¢„æµ‹"""
        metrics = {}
        
        # å¦‚æœæœ‰çœŸå®æ ‡ç­¾ï¼Œè®¡ç®—å‡†ç¡®æ€§
        if true_output is not None:
            metrics['accuracy'] = self._calculate_accuracy(predicted_output, true_output)
        
        # è®¡ç®—ä¸å‚è€ƒæ¨¡å‹çš„ä¸€è‡´æ€§
        ref_prediction = self._get_reference_prediction(input_text)
        metrics['consistency'] = self._calculate_consistency(predicted_output, ref_prediction)
        
        # è®¡ç®—å“åº”æ—¶é—´
        metrics['response_time'] = self._measure_response_time(input_text)
        
        self.metrics_buffer.append(metrics)
        
        # å®šæœŸæ¸…ç†ç¼“å†²åŒº
        if len(self.metrics_buffer) > 1000:
            self.metrics_buffer = self.metrics_buffer[-500:]
        
        return metrics
    
    def get_summary_statistics(self):
        """è·å–æ±‡æ€»ç»Ÿè®¡"""
        if not self.metrics_buffer:
            return {}
        
        df = pd.DataFrame(self.metrics_buffer)
        return {
            'accuracy_mean': df['accuracy'].mean() if 'accuracy' in df.columns else None,
            'consistency_mean': df['consistency'].mean(),
            'response_time_mean': df['response_time'].mean(),
            'total_samples': len(df)
        }
```

## 7. æœ€ä½³å®è·µæ€»ç»“

### 7.1 å¾®è°ƒç­–ç•¥é€‰æ‹©æŒ‡å—

| åœºæ™¯ | æ¨èæ–¹æ³• | ç†ç”± |
|------|----------|------|
| èµ„æºæåº¦æœ‰é™ | LoRA (r=8-16) | å‚æ•°æ•ˆç‡æœ€é«˜ |
| ä¸­ç­‰èµ„æº | QLoRA | å¹³è¡¡æ•ˆæœå’Œæ•ˆç‡ |
| éœ€è¦æœ€å¤§ç²¾åº¦ | Full fine-tuning | æ•ˆæœæœ€å¥½ä½†èµ„æºæ¶ˆè€—å¤§ |
| å¿«é€ŸåŸå‹éªŒè¯ | Adapter | æ’æ‹”æ–¹ä¾¿ï¼Œé€‚åˆå®éªŒ |

### 7.2 é‡åŒ–ç­–ç•¥å¯¹æ¯”

| é‡åŒ–æ–¹æ³• | ç²¾åº¦æŸå¤± | é€Ÿåº¦æå‡ | å†…å­˜èŠ‚çœ | é€‚ç”¨åœºæ™¯ |
|----------|----------|----------|----------|----------|
| 8-bitåŠ¨æ€é‡åŒ– | ä½(1-2%) | 2-3x | 2x | æ¨ç†éƒ¨ç½² |
| 4-bit GPTQ | ä¸­(3-5%) | 3-4x | 4x | å¤§æ¨¡å‹éƒ¨ç½² |
| 2-bité‡åŒ– | é«˜(8-15%) | 5-6x | 8x | è¾¹ç¼˜è®¾å¤‡ |

### 7.3 ç›‘æ§è¦ç‚¹

âœ… **æ€§èƒ½ç›‘æ§**
- è®­ç»ƒæŸå¤±æ”¶æ•›æ›²çº¿
- éªŒè¯é›†æ€§èƒ½æŒ‡æ ‡
- è®­ç»ƒé€Ÿåº¦å’Œååé‡

âœ… **èµ„æºç›‘æ§**
- GPUå†…å­˜ä½¿ç”¨ç‡
- CPUåˆ©ç”¨ç‡
- ç½‘ç»œå¸¦å®½ä½¿ç”¨

âœ… **è´¨é‡ç›‘æ§**
- æ¢¯åº¦èŒƒæ•°å’Œæ›´æ–°å¹…åº¦
- å‚æ•°ç»Ÿè®¡ä¿¡æ¯
- é¢„æµ‹åˆ†å¸ƒå˜åŒ–

## 8. æ€»ç»“

æœ¬æ¡ˆä¾‹å…¨é¢ä»‹ç»äº†å¤§æ¨¡å‹å¾®è°ƒä¸ä¼˜åŒ–çš„æ ¸å¿ƒæŠ€æœ¯ï¼ŒåŒ…æ‹¬ï¼š

âœ… å‚æ•°é«˜æ•ˆå¾®è°ƒ(LoRAã€QLoRAã€Adapter)  
âœ… æ¨¡å‹é‡åŒ–æŠ€æœ¯(PTQã€QATã€GPTQ)  
âœ… çŸ¥è¯†è’¸é¦å’Œæ¨¡å‹å‹ç¼©  
âœ… å¾®è°ƒæ•ˆæœè¯„ä¼°ä½“ç³»  
âœ… ç”Ÿäº§ç¯å¢ƒæœ€ä½³å®è·µ  

é€šè¿‡æœ¬æ¡ˆä¾‹çš„å­¦ä¹ ï¼Œæ‚¨åº”è¯¥èƒ½å¤Ÿï¼š
- æ ¹æ®å…·ä½“éœ€æ±‚é€‰æ‹©åˆé€‚çš„å¾®è°ƒç­–ç•¥
- å®æ–½é«˜æ•ˆçš„æ¨¡å‹é‡åŒ–å’Œå‹ç¼©
- å»ºç«‹å®Œå–„çš„å¾®è°ƒæ•ˆæœè¯„ä¼°ä½“ç³»
- åœ¨èµ„æºçº¦æŸä¸‹å®ç°æœ€ä¼˜çš„æ¨¡å‹æ€§èƒ½
- æ„å»ºå¯ç›‘æ§ã€å¯ç»´æŠ¤çš„å¾®è°ƒæµç¨‹

ä¸‹ä¸€æ­¥å»ºè®®å­¦ä¹ ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²æ¡ˆä¾‹ï¼Œäº†è§£å¦‚ä½•å°†ä¼˜åŒ–åçš„æ¨¡å‹éƒ¨ç½²åˆ°å®é™…ç”Ÿäº§ç¯å¢ƒä¸­ã€‚# #   8 .   LˆN”^(u:Wof 
 