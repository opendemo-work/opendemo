# ğŸ“Š Kubernetes æœ¬åœ°å¼€å‘ç›‘æ§å’Œæ—¥å¿—æœ€ä½³å®è·µ

> æœ¬åœ° Kubernetes ç¯å¢ƒçš„ç›‘æ§ã€æ—¥å¿—æ”¶é›†ä¸åˆ†æå®Œæ•´æŒ‡å—

## ğŸ¯ ç›‘æ§å’Œæ—¥å¿—æ¦‚è¿°

åœ¨æœ¬åœ° Kubernetes å¼€å‘ç¯å¢ƒä¸­å»ºç«‹å®Œå–„çš„ç›‘æ§å’Œæ—¥å¿—ç³»ç»Ÿï¼Œå¯ä»¥å¸®åŠ©å¼€å‘è€…å¿«é€Ÿå‘ç°é—®é¢˜ã€ä¼˜åŒ–æ€§èƒ½å¹¶æé«˜åº”ç”¨å¯é æ€§ã€‚

### ğŸ“‹ æ ¸å¿ƒç›®æ ‡
- **å®æ—¶ç›‘æ§**ï¼šæŒç»­ç›‘æ§é›†ç¾¤å’Œåº”ç”¨çŠ¶æ€
- **æ—¥å¿—æ”¶é›†**ï¼šé›†ä¸­æ”¶é›†å’Œç®¡ç†åº”ç”¨æ—¥å¿—
- **æ€§èƒ½åˆ†æ**ï¼šåˆ†æç³»ç»Ÿæ€§èƒ½ç“¶é¢ˆ
- **æ•…éšœè¯Šæ–­**ï¼šå¿«é€Ÿå®šä½å’Œè§£å†³é—®é¢˜
- **å®¹é‡è§„åˆ’**ï¼šåŸºäºæ•°æ®è¿›è¡Œèµ„æºè§„åˆ’

## ğŸš€ æœ¬åœ°ç›‘æ§æ ˆéƒ¨ç½²

### 1. Prometheus + Grafana ç›‘æ§æ ˆ

```bash
# éƒ¨ç½² Prometheus ç›‘æ§æ ˆ
setup_prometheus_monitoring() {
    # åˆ›å»ºç›‘æ§å‘½åç©ºé—´
    kubectl create namespace monitoring
    
    # ä½¿ç”¨ Prometheus Operator éƒ¨ç½²
    helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
    helm repo update
    
    # å®‰è£… Prometheus Operator
    helm install prometheus prometheus-community/kube-prometheus-stack \
        --namespace monitoring \
        --set prometheus.prometheusSpec.serviceMonitorSelectorNilUsesHelmValues=false \
        --set prometheus.prometheusSpec.podMonitorSelectorNilUsesHelmValues=false
    
    # ç­‰å¾…ç»„ä»¶å°±ç»ª
    kubectl wait --for=condition=available deployment/prometheus-grafana -n monitoring --timeout=300s
    kubectl wait --for=condition=available statefulset/prometheus-prometheus-kube-prometheus-prometheus -n monitoring --timeout=300s
    
    echo "Prometheus ç›‘æ§æ ˆéƒ¨ç½²å®Œæˆ"
    echo "Grafana è®¿é—®: kubectl port-forward svc/prometheus-grafana -n monitoring 3000:80"
    echo "é»˜è®¤ç”¨æˆ·å: admin, å¯†ç : prom-operator"
}

# æœ¬åœ°å¼€å‘å‹å¥½çš„ Prometheus é…ç½®
create_local_prometheus_config() {
    cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-local-config
  namespace: monitoring
data:
  prometheus.yml: |
    global:
      scrape_interval: 15s
      evaluation_interval: 15s
    
    rule_files:
      - "alert.rules"
    
    alerting:
      alertmanagers:
      - static_configs:
        - targets:
          - alertmanager:9093
    
    scrape_configs:
      - job_name: 'kubernetes-apiservers'
        kubernetes_sd_configs:
        - role: endpoints
        scheme: https
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
        relabel_configs:
        - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
          action: keep
          regex: default;kubernetes;https
    
      - job_name: 'kubernetes-nodes'
        kubernetes_sd_configs:
        - role: node
        scheme: https
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
        relabel_configs:
        - action: labelmap
          regex: __meta_kubernetes_node_label_(.+)
        - target_label: __address__
          replacement: kubernetes.default.svc:443
        - source_labels: [__meta_kubernetes_node_name]
          regex: (.+)
          target_label: __metrics_path__
          replacement: /api/v1/nodes/\${1}/proxy/metrics
    
      - job_name: 'kubernetes-pods'
        kubernetes_sd_configs:
        - role: pod
        relabel_configs:
        - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
          action: keep
          regex: true
        - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
          action: replace
          target_label: __metrics_path__
          regex: (.+)
        - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
          action: replace
          regex: ([^:]+)(?::\d+)?;(\d+)
          replacement: \$1:\$2
          target_label: __address__
        - action: labelmap
          regex: __meta_kubernetes_pod_label_(.+)
        - source_labels: [__meta_kubernetes_namespace]
          action: replace
          target_label: kubernetes_namespace
        - source_labels: [__meta_kubernetes_pod_name]
          action: replace
          target_label: kubernetes_pod_name
EOF
}

# åº”ç”¨ç›‘æ§é…ç½®
setup_application_monitoring() {
    # ä¸ºåº”ç”¨æ·»åŠ  Prometheus æ³¨è§£
    cat <<EOF | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: monitored-app
  namespace: development
spec:
  replicas: 2
  selector:
    matchLabels:
      app: monitored-app
  template:
    metadata:
      labels:
        app: monitored-app
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8080"
        prometheus.io/path: "/metrics"
    spec:
      containers:
      - name: app
        image: myapp:latest
        ports:
        - containerPort: 8080
        resources:
          requests:
            memory: "128Mi"
            cpu: "100m"
          limits:
            memory: "256Mi"
            cpu: "200m"
---
apiVersion: v1
kind: Service
metadata:
  name: monitored-app-service
  namespace: development
  labels:
    app: monitored-app
spec:
  selector:
    app: monitored-app
  ports:
  - port: 80
    targetPort: 8080
EOF
}
```

### 2. Grafana ä»ªè¡¨æ¿é…ç½®

```bash
# åˆ›å»ºè‡ªå®šä¹‰ Grafana ä»ªè¡¨æ¿
create_grafana_dashboards() {
    # åˆ›å»ºèŠ‚ç‚¹ç›‘æ§ä»ªè¡¨æ¿
    cat <<'EOF' | kubectl apply -f -
apiVersion: v1
kind: ConfigMap
metadata:
  name: node-dashboard
  namespace: monitoring
  labels:
    grafana_dashboard: "1"
data:
  node-dashboard.json: |
    {
      "dashboard": {
        "id": null,
        "title": "èŠ‚ç‚¹ç›‘æ§",
        "tags": ["nodes"],
        "timezone": "browser",
        "schemaVersion": 16,
        "version": 0,
        "refresh": "10s",
        "panels": [
          {
            "type": "graph",
            "title": "CPU ä½¿ç”¨ç‡",
            "gridPos": {"x": 0, "y": 0, "w": 12, "h": 8},
            "targets": [
              {
                "expr": "100 - (avg(rate(node_cpu_seconds_total{mode=\"idle\"}[5m])) * 100)",
                "legendFormat": "{{instance}}"
              }
            ]
          },
          {
            "type": "graph",
            "title": "å†…å­˜ä½¿ç”¨ç‡",
            "gridPos": {"x": 12, "y": 0, "w": 12, "h": 8},
            "targets": [
              {
                "expr": "(node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) / node_memory_MemTotal_bytes * 100",
                "legendFormat": "{{instance}}"
              }
            ]
          },
          {
            "type": "stat",
            "title": "Pod æ•°é‡",
            "gridPos": {"x": 0, "y": 8, "w": 8, "h": 6},
            "targets": [
              {
                "expr": "count(kube_pod_info)",
                "instant": true
              }
            ]
          },
          {
            "type": "stat",
            "title": "èŠ‚ç‚¹çŠ¶æ€",
            "gridPos": {"x": 8, "y": 8, "w": 8, "h": 6},
            "targets": [
              {
                "expr": "sum(up{job=\"kubernetes-nodes\"})",
                "instant": true
              }
            ]
          }
        ]
      }
    }
EOF

    # åˆ›å»ºåº”ç”¨ç›‘æ§ä»ªè¡¨æ¿
    cat <<'EOF' | kubectl apply -f -
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-dashboard
  namespace: monitoring
  labels:
    grafana_dashboard: "1"
data:
  app-dashboard.json: |
    {
      "dashboard": {
        "id": null,
        "title": "åº”ç”¨ç›‘æ§",
        "tags": ["applications"],
        "timezone": "browser",
        "schemaVersion": 16,
        "version": 0,
        "refresh": "10s",
        "panels": [
          {
            "type": "graph",
            "title": "HTTP è¯·æ±‚ç‡",
            "gridPos": {"x": 0, "y": 0, "w": 12, "h": 8},
            "targets": [
              {
                "expr": "rate(http_requests_total[5m])",
                "legendFormat": "{{handler}}"
              }
            ]
          },
          {
            "type": "graph",
            "title": "è¯·æ±‚å»¶è¿Ÿ",
            "gridPos": {"x": 12, "y": 0, "w": 12, "h": 8},
            "targets": [
              {
                "expr": "histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))",
                "legendFormat": "{{handler}} 95th percentile"
              }
            ]
          },
          {
            "type": "stat",
            "title": "é”™è¯¯ç‡",
            "gridPos": {"x": 0, "y": 8, "w": 8, "h": 6},
            "targets": [
              {
                "expr": "rate(http_requests_total{code=~\"5..\"}[5m]) / rate(http_requests_total[5m]) * 100",
                "instant": true
              }
            ]
          },
          {
            "type": "stat",
            "title": "åœ¨çº¿å®ä¾‹æ•°",
            "gridPos": {"x": 8, "y": 8, "w": 8, "h": 6},
            "targets": [
              {
                "expr": "count(up{job=\"kubernetes-pods\"} == 1)",
                "instant": true
              }
            ]
          }
        ]
      }
    }
EOF
}
```

## ğŸ“ æ—¥å¿—æ”¶é›†å’Œåˆ†æ

### 1. EFK æ ˆéƒ¨ç½² (Elasticsearch + Fluentd + Kibana)

```bash
# éƒ¨ç½² EFK æ—¥å¿—æ ˆ
setup_efk_stack() {
    # åˆ›å»ºæ—¥å¿—å‘½åç©ºé—´
    kubectl create namespace logging
    
    # éƒ¨ç½² Elasticsearch
    cat <<EOF | kubectl apply -f -
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: elasticsearch
  namespace: logging
spec:
  serviceName: elasticsearch
  replicas: 1
  selector:
    matchLabels:
      app: elasticsearch
  template:
    metadata:
      labels:
        app: elasticsearch
    spec:
      containers:
      - name: elasticsearch
        image: docker.elastic.co/elasticsearch/elasticsearch:8.11.0
        env:
        - name: discovery.type
          value: single-node
        - name: ES_JAVA_OPTS
          value: "-Xms512m -Xmx512m"
        ports:
        - containerPort: 9200
          name: http
        - containerPort: 9300
          name: transport
        volumeMounts:
        - name: elasticsearch-data
          mountPath: /usr/share/elasticsearch/data
  volumeClaimTemplates:
  - metadata:
      name: elasticsearch-data
    spec:
      accessModes: ["ReadWriteOnce"]
      resources:
        requests:
          storage: 10Gi
---
apiVersion: v1
kind: Service
metadata:
  name: elasticsearch
  namespace: logging
spec:
  selector:
    app: elasticsearch
  ports:
  - port: 9200
    targetPort: 9200
EOF

    # éƒ¨ç½² Fluentd
    cat <<EOF | kubectl apply -f -
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluentd
  namespace: logging
  labels:
    app: fluentd
spec:
  selector:
    matchLabels:
      app: fluentd
  template:
    metadata:
      labels:
        app: fluentd
    spec:
      serviceAccount: fluentd
      serviceAccountName: fluentd
      containers:
      - name: fluentd
        image: fluent/fluentd-kubernetes-daemonset:v1.16.1-debian-elasticsearch7-1.0
        env:
        - name: FLUENT_ELASTICSEARCH_HOST
          value: "elasticsearch.logging.svc.cluster.local"
        - name: FLUENT_ELASTICSEARCH_PORT
          value: "9200"
        - name: FLUENT_ELASTICSEARCH_SCHEME
          value: "http"
        - name: FLUENT_UID
          value: "0"
        resources:
          limits:
            memory: 512Mi
          requests:
            cpu: 100m
            memory: 200Mi
        volumeMounts:
        - name: varlog
          mountPath: /var/log
        - name: varlibdockercontainers
          mountPath: /var/lib/docker/containers
          readOnly: true
        - name: fluentd-config
          mountPath: /fluentd/etc
      volumes:
      - name: varlog
        hostPath:
          path: /var/log
      - name: varlibdockercontainers
        hostPath:
          path: /var/lib/docker/containers
      - name: fluentd-config
        configMap:
          name: fluentd-config
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: fluentd-config
  namespace: logging
data:
  fluent.conf: |
    <source>
      @type tail
      path /var/log/containers/*.log
      pos_file /var/log/fluentd-containers.log.pos
      tag kubernetes.*
      read_from_head true
      <parse>
        @type json
        time_format %Y-%m-%dT%H:%M:%S.%NZ
      </parse>
    </source>
    
    <filter kubernetes.**>
      @type kubernetes_metadata
    </filter>
    
    <match kubernetes.**>
      @type elasticsearch
      host "#{ENV['FLUENT_ELASTICSEARCH_HOST']}"
      port "#{ENV['FLUENT_ELASTICSEARCH_PORT']}"
      scheme "#{ENV['FLUENT_ELASTICSEARCH_SCHEME']}"
      logstash_format true
      logstash_prefix kubernetes
      flush_interval 5s
    </match>
EOF

    # åˆ›å»º Fluentd ServiceAccount å’Œ RBAC
    cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: ServiceAccount
metadata:
  name: fluentd
  namespace: logging
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: fluentd
rules:
- apiGroups:
  - ""
  resources:
  - pods
  - namespaces
  verbs:
  - get
  - list
  - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: fluentd
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: fluentd
subjects:
- kind: ServiceAccount
  name: fluentd
  namespace: logging
EOF

    # éƒ¨ç½² Kibana
    cat <<EOF | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kibana
  namespace: logging
spec:
  replicas: 1
  selector:
    matchLabels:
      app: kibana
  template:
    metadata:
      labels:
        app: kibana
    spec:
      containers:
      - name: kibana
        image: docker.elastic.co/kibana/kibana:8.11.0
        env:
        - name: ELASTICSEARCH_HOSTS
          value: "http://elasticsearch:9200"
        ports:
        - containerPort: 5601
---
apiVersion: v1
kind: Service
metadata:
  name: kibana
  namespace: logging
spec:
  selector:
    app: kibana
  ports:
  - port: 5601
    targetPort: 5601
EOF

    echo "EFK æ—¥å¿—æ ˆéƒ¨ç½²å®Œæˆ"
    echo "Kibana è®¿é—®: kubectl port-forward svc/kibana -n logging 5601:5601"
}

# åº”ç”¨æ—¥å¿—é…ç½®
setup_application_logging() {
    # ä¸ºåº”ç”¨é…ç½®ç»“æ„åŒ–æ—¥å¿—
    cat <<'EOF' | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: logging-app
  namespace: development
spec:
  replicas: 2
  selector:
    matchLabels:
      app: logging-app
  template:
    metadata:
      labels:
        app: logging-app
    spec:
      containers:
      - name: app
        image: myapp:latest
        env:
        - name: LOG_FORMAT
          value: "json"
        - name: LOG_LEVEL
          value: "info"
        ports:
        - containerPort: 8080
        volumeMounts:
        - name: log-volume
          mountPath: /var/log/app
      volumes:
      - name: log-volume
        emptyDir: {}
---
apiVersion: v1
kind: Service
metadata:
  name: logging-app-service
  namespace: development
spec:
  selector:
    app: logging-app
  ports:
  - port: 80
    targetPort: 8080
EOF
}
```

### 2. æ—¥å¿—æŸ¥è¯¢å’Œåˆ†æ

```bash
# æ—¥å¿—æŸ¥è¯¢å·¥å…·
create_log_query_tools() {
    # åˆ›å»ºæ—¥å¿—æŸ¥è¯¢è„šæœ¬
    cat <<'EOF' > log-query.sh
#!/bin/bash
# æ—¥å¿—æŸ¥è¯¢å·¥å…·

ELASTICSEARCH_URL="http://localhost:9200"
KIBANA_URL="http://localhost:5601"

# åŸºç¡€æ—¥å¿—æŸ¥è¯¢
query_logs() {
    local query=$1
    local index=${2:-"kubernetes-*"}
    
    curl -s -X GET "$ELASTICSEARCH_URL/$index/_search" \
        -H 'Content-Type: application/json' \
        -d "{
            \"query\": {
                \"query_string\": {
                    \"query\": \"$query\"
                }
            },
            \"sort\": [
                {\"@timestamp\": {\"order\": \"desc\"}}
            ],
            \"size\": 50
        }" | jq '.hits.hits[]._source | {timestamp: .@timestamp, message: .log, pod: .kubernetes.pod_name}'
}

# é”™è¯¯æ—¥å¿—æŸ¥è¯¢
query_error_logs() {
    query_logs "level:error OR ERROR OR Exception" "$1"
}

# ç‰¹å®šåº”ç”¨æ—¥å¿—æŸ¥è¯¢
query_app_logs() {
    local app_name=$1
    local query=${2:-"*"}
    
    query_logs "kubernetes.container_name:$app_name AND ($query)" "kubernetes-*"
}

# å®æ—¶æ—¥å¿—è·Ÿè¸ª
tail_logs() {
    local app_name=$1
    
    while true; do
        query_app_logs "$app_name" "*" | jq -r '.timestamp + " " + .pod + " " + .message'
        sleep 2
    done
}

# æ—¥å¿—ç»Ÿè®¡åˆ†æ
log_statistics() {
    local app_name=$1
    local hours=${2:-24}
    
    echo "=== $app_name æœ€è¿‘ $hours å°æ—¶æ—¥å¿—ç»Ÿè®¡ ==="
    
    # é”™è¯¯æ•°é‡ç»Ÿè®¡
    error_count=$(query_error_logs "kubernetes.container_name:$app_name" | jq length)
    echo "é”™è¯¯æ—¥å¿—æ•°é‡: $error_count"
    
    # æ—¥å¿—çº§åˆ«åˆ†å¸ƒ
    echo "æ—¥å¿—çº§åˆ«åˆ†å¸ƒ:"
    curl -s -X GET "$ELASTICSEARCH_URL/kubernetes-*/_search" \
        -H 'Content-Type: application/json' \
        -d "{
            \"query\": {
                \"term\": {
                    \"kubernetes.container_name.keyword\": \"$app_name\"
                }
            },
            \"aggs\": {
                \"log_levels\": {
                    \"terms\": {
                        \"field\": \"level.keyword\"
                    }
                }
            },
            \"size\": 0
        }" | jq '.aggregations.log_levels.buckets[] | "\(.key): \(.doc_count)"'
}

# ä¸»èœå•
case "$1" in
    "query")
        query_logs "$2" "$3"
        ;;
    "errors")
        query_error_logs "$2"
        ;;
    "app")
        query_app_logs "$2" "$3"
        ;;
    "tail")
        tail_logs "$2"
        ;;
    "stats")
        log_statistics "$2" "$3"
        ;;
    *)
        echo "ç”¨æ³•: $0 {query|errors|app|tail|stats} [å‚æ•°...]"
        echo "ç¤ºä¾‹:"
        echo "  $0 query 'error'"
        echo "  $0 errors my-app"
        echo "  $0 app my-app 'user login'"
        echo "  $0 tail my-app"
        echo "  $0 stats my-app 24"
        ;;
esac
EOF

    chmod +x log-query.sh
    
    # åˆ›å»º Kibana æŸ¥è¯¢æ¨¡æ¿
    cat <<'EOF' > kibana-queries.json
{
  "saved_objects": [
    {
      "id": "app-error-logs",
      "type": "search",
      "attributes": {
        "title": "åº”ç”¨é”™è¯¯æ—¥å¿—",
        "description": "æŸ¥è¯¢åº”ç”¨é”™è¯¯æ—¥å¿—",
        "kibanaSavedObjectMeta": {
          "searchSourceJSON": "{\"index\":\"kubernetes-*\",\"filter\":[],\"query\":{\"query_string\":{\"query\":\"level:error OR ERROR OR Exception\"}}}"
        }
      }
    },
    {
      "id": "slow-requests",
      "type": "search",
      "attributes": {
        "title": "æ…¢è¯·æ±‚åˆ†æ",
        "description": "åˆ†æå“åº”æ—¶é—´è¶…è¿‡1ç§’çš„è¯·æ±‚",
        "kibanaSavedObjectMeta": {
          "searchSourceJSON": "{\"index\":\"kubernetes-*\",\"filter\":[],\"query\":{\"query_string\":{\"query\":\"response_time:>1000\"}}}"
        }
      }
    }
  ]
}
EOF
}
```

## ğŸ”” å‘Šè­¦é…ç½®

### 1. Alertmanager é…ç½®

```bash
# é…ç½® Alertmanager
setup_alerting() {
    # åˆ›å»º Alertmanager é…ç½®
    cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: ConfigMap
metadata:
  name: alertmanager-config
  namespace: monitoring
data:
  alertmanager.yml: |
    global:
      smtp_smarthost: 'smtp.gmail.com:587'
      smtp_from: 'alerts@example.com'
      smtp_auth_username: 'alerts@example.com'
      smtp_auth_password: 'your-app-password'
    
    route:
      group_by: ['alertname']
      group_wait: 10s
      group_interval: 10s
      repeat_interval: 1h
      receiver: 'slack-notifications'
    
    receivers:
    - name: 'slack-notifications'
      slack_configs:
      - api_url: 'https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK'
        channel: '#alerts'
        send_resolved: true
        title: '{{ template "slack.title" . }}'
        text: '{{ template "slack.text" . }}'
    
    templates:
    - '/etc/alertmanager/template/*.tmpl'
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: alertmanager
  namespace: monitoring
spec:
  replicas: 1
  selector:
    matchLabels:
      app: alertmanager
  template:
    metadata:
      labels:
        app: alertmanager
    spec:
      containers:
      - name: alertmanager
        image: prom/alertmanager:v0.26.0
        args:
        - '--config.file=/etc/alertmanager/alertmanager.yml'
        - '--storage.path=/alertmanager'
        ports:
        - containerPort: 9093
        volumeMounts:
        - name: config-volume
          mountPath: /etc/alertmanager
        - name: storage-volume
          mountPath: /alertmanager
      volumes:
      - name: config-volume
        configMap:
          name: alertmanager-config
      - name: storage-volume
        emptyDir: {}
---
apiVersion: v1
kind: Service
metadata:
  name: alertmanager
  namespace: monitoring
spec:
  selector:
    app: alertmanager
  ports:
  - port: 9093
    targetPort: 9093
EOF

    # åˆ›å»ºå‘Šè­¦è§„åˆ™
    cat <<EOF | kubectl apply -f -
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: alert-rules
  namespace: monitoring
spec:
  groups:
  - name: kubernetes-alerts
    rules:
    - alert: HighCPUUsage
      expr: 100 - (avg(rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "High CPU usage detected"
        description: "CPU usage is above 80% for more than 5 minutes"

    - alert: HighMemoryUsage
      expr: (node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) / node_memory_MemTotal_bytes * 100 > 85
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "High memory usage detected"
        description: "Memory usage is above 85% for more than 5 minutes"

    - alert: PodCrashLooping
      expr: rate(kube_pod_container_status_restarts_total[5m]) * 60 * 5 > 0
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "Pod crash looping detected"
        description: "Pod {{ \$labels.pod }} is crash looping"

    - alert: ServiceDown
      expr: up == 0
      for: 2m
      labels:
        severity: critical
      annotations:
        summary: "Service is down"
        description: "Service {{ \$labels.job }} is down"
EOF
}
```

### 2. æœ¬åœ°å‘Šè­¦æµ‹è¯•

```bash
# å‘Šè­¦æµ‹è¯•å·¥å…·
create_alert_test_tools() {
    # åˆ›å»ºå‘Šè­¦æµ‹è¯•è„šæœ¬
    cat <<'EOF' > alert-test.sh
#!/bin/bash
# å‘Šè­¦æµ‹è¯•å·¥å…·

PROMETHEUS_URL="http://localhost:9090"
ALERTMANAGER_URL="http://localhost:9093"

# è§¦å‘æµ‹è¯•å‘Šè­¦
trigger_test_alert() {
    local alert_name=${1:-"TestAlert"}
    local severity=${2:-"warning"}
    
    echo "è§¦å‘æµ‹è¯•å‘Šè­¦: $alert_name (ä¸¥é‡ç¨‹åº¦: $severity)"
    
    # åˆ›å»ºæµ‹è¯•å‘Šè­¦è§„åˆ™
    cat <<TEST_RULE | curl -X POST -H "Content-Type: application/yaml" \
        --data-binary @- "$PROMETHEUS_URL/api/v1/rules"
groups:
- name: test-alerts
  rules:
  - alert: $alert_name
    expr: vector(1)
    labels:
      severity: $severity
    annotations:
      summary: "Test alert"
      description: "This is a test alert triggered at $(date)"
TEST_RULE
}

# æŸ¥çœ‹å½“å‰å‘Šè­¦
view_active_alerts() {
    echo "=== å½“å‰æ´»è·ƒå‘Šè­¦ ==="
    curl -s "$ALERTMANAGER_URL/api/v2/alerts" | jq '.[] | {
        name: .labels.alertname,
        severity: .labels.severity,
        status: .status.state,
        startsAt: .startsAt,
        summary: .annotations.summary
    }'
}

# æŸ¥çœ‹å‘Šè­¦å†å²
view_alert_history() {
    echo "=== å‘Šè­¦å†å² ==="
    curl -s "$ALERTMANAGER_URL/api/v2/alerts?active=false" | jq '.[] | {
        name: .labels.alertname,
        status: .status.state,
        startsAt: .startsAt,
        endsAt: .endsAt,
        summary: .annotations.summary
    }'
}

# æµ‹è¯•é€šçŸ¥æ¸ é“
test_notification_channels() {
    echo "æµ‹è¯•é€šçŸ¥æ¸ é“..."
    
    # æµ‹è¯• Slack é€šçŸ¥
    curl -X POST -H 'Content-Type: application/json' \
        --data '{"text":"ğŸ”” è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•é€šçŸ¥"}' \
        https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK 2>/dev/null && \
        echo "âœ… Slack é€šçŸ¥æµ‹è¯•æˆåŠŸ" || echo "âŒ Slack é€šçŸ¥æµ‹è¯•å¤±è´¥"
    
    # æµ‹è¯•é‚®ä»¶é€šçŸ¥ï¼ˆéœ€è¦é…ç½® SMTPï¼‰
    echo "ğŸ“§ é‚®ä»¶é€šçŸ¥æµ‹è¯•éœ€è¦æ‰‹åŠ¨éªŒè¯"
}

# æ¸…ç†æµ‹è¯•å‘Šè­¦
cleanup_test_alerts() {
    echo "æ¸…ç†æµ‹è¯•å‘Šè­¦..."
    curl -X DELETE "$PROMETHEUS_URL/api/v1/rules/test-alerts"
    echo "æµ‹è¯•å‘Šè­¦å·²æ¸…ç†"
}

# ä¸»èœå•
case "$1" in
    "trigger")
        trigger_test_alert "$2" "$3"
        ;;
    "view")
        view_active_alerts
        ;;
    "history")
        view_alert_history
        ;;
    "test-notifications")
        test_notification_channels
        ;;
    "cleanup")
        cleanup_test_alerts
        ;;
    *)
        echo "ç”¨æ³•: $0 {trigger|view|history|test-notifications|cleanup} [å‚æ•°...]"
        echo "ç¤ºä¾‹:"
        echo "  $0 trigger TestCPUAlert warning"
        echo "  $0 view"
        echo "  $0 history"
        echo "  $0 test-notifications"
        echo "  $0 cleanup"
        ;;
esac
EOF

    chmod +x alert-test.sh
}
```

## ğŸ“ˆ æ€§èƒ½æŒ‡æ ‡æ”¶é›†

### 1. åº”ç”¨æ€§èƒ½ç›‘æ§

```bash
# åº”ç”¨æ€§èƒ½ç›‘æ§é…ç½®
setup_app_performance_monitoring() {
    # åˆ›å»ºåº”ç”¨æ€§èƒ½ç›‘æ§é…ç½®
    cat <<'EOF' | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: perf-monitored-app
  namespace: development
spec:
  replicas: 2
  selector:
    matchLabels:
      app: perf-monitored-app
  template:
    metadata:
      labels:
        app: perf-monitored-app
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8080"
        prometheus.io/path: "/metrics"
    spec:
      containers:
      - name: app
        image: myapp:latest
        ports:
        - containerPort: 8080
        env:
        - name: ENABLE_PROFILING
          value: "true"
        - name: METRICS_ENDPOINT
          value: "/metrics"
        resources:
          requests:
            memory: "256Mi"
            cpu: "200m"
          limits:
            memory: "512Mi"
            cpu: "500m"
        readinessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 10
          periodSeconds: 5
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
---
apiVersion: v1
kind: Service
metadata:
  name: perf-monitored-app-service
  namespace: development
  labels:
    app: perf-monitored-app
spec:
  selector:
    app: perf-monitored-app
  ports:
  - port: 80
    targetPort: 8080
EOF

    # åˆ›å»ºè‡ªå®šä¹‰æŒ‡æ ‡æ”¶é›†å™¨
    cat <<'EOF' > app-metrics-collector.py
#!/usr/bin/env python3
"""
åº”ç”¨æ€§èƒ½æŒ‡æ ‡æ”¶é›†å™¨
"""

import time
import psutil
import requests
from prometheus_client import start_http_server, Gauge, Counter, Histogram

# å®šä¹‰æŒ‡æ ‡
cpu_percent = Gauge('app_cpu_percent', 'CPUä½¿ç”¨ç™¾åˆ†æ¯”')
memory_percent = Gauge('app_memory_percent', 'å†…å­˜ä½¿ç”¨ç™¾åˆ†æ¯”')
request_count = Counter('app_requests_total', 'æ€»è¯·æ±‚æ•°')
request_duration = Histogram('app_request_duration_seconds', 'è¯·æ±‚å¤„ç†æ—¶é—´')

def collect_system_metrics():
    """æ”¶é›†ç³»ç»ŸæŒ‡æ ‡"""
    cpu_percent.set(psutil.cpu_percent())
    memory_percent.set(psutil.virtual_memory().percent)

def monitor_requests():
    """ç›‘æ§HTTPè¯·æ±‚"""
    try:
        start_time = time.time()
        response = requests.get('http://localhost:8080/health', timeout=5)
        duration = time.time() - start_time
        
        request_count.inc()
        request_duration.observe(duration)
        
        return response.status_code == 200
    except Exception as e:
        print(f"è¯·æ±‚ç›‘æ§é”™è¯¯: {e}")
        return False

def main():
    # å¯åŠ¨PrometheusæŒ‡æ ‡æœåŠ¡å™¨
    start_http_server(8080)
    print("æŒ‡æ ‡æ”¶é›†å™¨å·²å¯åŠ¨ï¼Œç›‘å¬ç«¯å£ 8080")
    
    # å®šæœŸæ”¶é›†æŒ‡æ ‡
    while True:
        collect_system_metrics()
        monitor_requests()
        time.sleep(10)

if __name__ == "__main__":
    main()
EOF
}
```

### 2. æ€§èƒ½æµ‹è¯•å’ŒåŸºå‡†

```bash
# æ€§èƒ½æµ‹è¯•å·¥å…·
create_performance_benchmarks() {
    # åˆ›å»ºæ€§èƒ½æµ‹è¯•è„šæœ¬
    cat <<'EOF' > performance-benchmark.sh
#!/bin/bash
# æ€§èƒ½åŸºå‡†æµ‹è¯•å·¥å…·

TEST_DURATION=${1:-60}  # æµ‹è¯•æŒç»­æ—¶é—´ï¼ˆç§’ï¼‰
CONCURRENT_USERS=${2:-10}  # å¹¶å‘ç”¨æˆ·æ•°

echo "=== æ€§èƒ½åŸºå‡†æµ‹è¯• ==="
echo "æµ‹è¯•æŒç»­æ—¶é—´: ${TEST_DURATION}ç§’"
echo "å¹¶å‘ç”¨æˆ·æ•°: ${CONCURRENT_USERS}"

# CPU åŸºå‡†æµ‹è¯•
test_cpu_performance() {
    echo "1. CPU æ€§èƒ½æµ‹è¯•..."
    sysbench --test=cpu --cpu-max-prime=20000 run | grep "total time\|events per second"
}

# å†…å­˜åŸºå‡†æµ‹è¯•
test_memory_performance() {
    echo "2. å†…å­˜æ€§èƒ½æµ‹è¯•..."
    sysbench --test=memory --memory-block-size=1K --memory-total-size=100G run | grep "transferred\|throughput"
}

# ç£ç›˜ I/O æµ‹è¯•
test_disk_performance() {
    echo "3. ç£ç›˜ I/O æ€§èƒ½æµ‹è¯•..."
    sysbench --test=fileio --file-total-size=1G prepare
    sysbench --test=fileio --file-total-size=1G --file-test-mode=rndrw --time=${TEST_DURATION} run | grep "read\|written\|requests"
    sysbench --test=fileio --file-total-size=1G cleanup
}

# ç½‘ç»œæ€§èƒ½æµ‹è¯•
test_network_performance() {
    echo "4. ç½‘ç»œæ€§èƒ½æµ‹è¯•..."
    
    # åˆ›å»ºæµ‹è¯• Pod
    kubectl run network-test --image=nicolaka/netshoot --rm -it --restart=Never -- \
        iperf3 -s -D &  # å¯åŠ¨æœåŠ¡å™¨
    
    sleep 5
    
    # è¿è¡Œå®¢æˆ·ç«¯æµ‹è¯•
    kubectl run network-client --image=nicolaka/netshoot --rm -it --restart=Never -- \
        iperf3 -c network-test -t ${TEST_DURATION} -P ${CONCURRENT_USERS}
}

# åº”ç”¨å“åº”æ—¶é—´æµ‹è¯•
test_app_response_time() {
    echo "5. åº”ç”¨å“åº”æ—¶é—´æµ‹è¯•..."
    
    SERVICE_URL=${SERVICE_URL:-"http://perf-monitored-app-service.development.svc.cluster.local"}
    
    for i in $(seq 1 ${TEST_DURATION}); do
        start_time=$(date +%s%N)
        curl -s -o /dev/null -w "%{http_code}" ${SERVICE_URL}/health
        end_time=$(date +%s%N)
        duration=$((($end_time - $start_time) / 1000000))  # è½¬æ¢ä¸ºæ¯«ç§’
        echo "è¯·æ±‚ $i: ${duration}ms"
        sleep 1
    done
}

# è¿è¡Œæ‰€æœ‰æµ‹è¯•
run_all_tests() {
    test_cpu_performance
    test_memory_performance
    test_disk_performance
    test_network_performance
    test_app_response_time
    
    echo "æ€§èƒ½æµ‹è¯•å®Œæˆ"
    echo "ç»“æœå·²ä¿å­˜åˆ° performance-results-$(date +%Y%m%d-%H%M%S).txt"
}

# ä¸»ç¨‹åº
run_all_tests | tee performance-results-$(date +%Y%m%d-%H%M%S).txt
EOF

    chmod +x performance-benchmark.sh
    
    # åˆ›å»ºæŒç»­æ€§èƒ½ç›‘æ§
    cat <<'EOF' > continuous-monitoring.sh
#!/bin/bash
# æŒç»­æ€§èƒ½ç›‘æ§è„šæœ¬

INTERVAL=${1:-30}  # ç›‘æ§é—´éš”ï¼ˆç§’ï¼‰

echo "å¼€å§‹æŒç»­æ€§èƒ½ç›‘æ§ï¼Œé—´éš”: ${INTERVAL}ç§’"

while true; do
    timestamp=$(date '+%Y-%m-%d %H:%M:%S')
    echo "[$timestamp] === æ€§èƒ½å¿«ç…§ ==="
    
    # æ”¶é›†å…³é”®æŒ‡æ ‡
    echo "CPU ä½¿ç”¨ç‡: $(kubectl top nodes | awk 'NR>1 {print $2}' | head -1)"
    echo "å†…å­˜ä½¿ç”¨ç‡: $(kubectl top nodes | awk 'NR>1 {print $4}' | head -1)"
    echo "Pod æ•°é‡: $(kubectl get pods --all-namespaces --no-headers | wc -l)"
    echo "è¿è¡Œä¸­çš„ Pod: $(kubectl get pods --field-selector=status.phase=Running --all-namespaces --no-headers | wc -l)"
    
    # æ£€æŸ¥åº”ç”¨å¥åº·çŠ¶æ€
    app_status=$(kubectl get pods -n development -l app=perf-monitored-app -o jsonpath='{.items[*].status.phase}' 2>/dev/null)
    echo "åº”ç”¨çŠ¶æ€: $app_status"
    
    sleep $INTERVAL
done
EOF

    chmod +x continuous-monitoring.sh
}
```

## ğŸ¯ ç›‘æ§å’Œæ—¥å¿—æœ€ä½³å®è·µæ€»ç»“

### å®æ–½å»ºè®®
1. **åˆ†å±‚ç›‘æ§**ï¼šä»åŸºç¡€è®¾æ–½åˆ°åº”ç”¨å±‚çš„å…¨æ–¹ä½ç›‘æ§
2. **ç»“æ„åŒ–æ—¥å¿—**ï¼šä½¿ç”¨ç»Ÿä¸€çš„æ—¥å¿—æ ¼å¼ä¾¿äºåˆ†æ
3. **å‘Šè­¦åˆ†çº§**ï¼šåˆç†è®¾ç½®å‘Šè­¦çº§åˆ«å’Œé˜ˆå€¼
4. **æ€§èƒ½åŸºçº¿**ï¼šå»ºç«‹æ­£å¸¸çš„æ€§èƒ½åŸºå‡†ç”¨äºæ¯”è¾ƒ
5. **å®šæœŸå®¡æŸ¥**ï¼šå®šæœŸå®¡æŸ¥ç›‘æ§é…ç½®å’Œå‘Šè­¦è§„åˆ™

### å…³é”®æˆåŠŸå› ç´ 
- é€‰æ‹©åˆé€‚çš„å·¥å…·ç»„åˆ
- è®¾è®¡æ¸…æ™°çš„ç›‘æ§ç­–ç•¥
- å»ºç«‹æœ‰æ•ˆçš„å‘Šè­¦æœºåˆ¶
- åŸ¹å…»æ•°æ®é©±åŠ¨çš„æ–‡åŒ–
- æŒç»­ä¼˜åŒ–å’Œæ”¹è¿›

---

> **ğŸ’¡ æç¤º**: ç›‘æ§å’Œæ—¥å¿—ç³»ç»Ÿåº”è¯¥éšç€åº”ç”¨çš„å‘å±•è€Œæ¼”è¿›ï¼Œå§‹ç»ˆä¿æŒå…¶æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚

**ç‰ˆæœ¬**: v1.0.0  
**æ›´æ–°æ—¶é—´**: 2026å¹´2æœˆ6æ—¥