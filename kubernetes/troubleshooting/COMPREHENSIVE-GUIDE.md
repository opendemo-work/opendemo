# Kubernetesæ•…éšœæ’æŸ¥ç»¼åˆæŒ‡å—

## 1. æ•…éšœæ’æŸ¥ä½“ç³»æ¦‚è¿°

æœ¬æŒ‡å—æä¾›ç³»ç»ŸåŒ–çš„Kubernetesæ•…éšœæ’æŸ¥æ–¹æ³•è®ºï¼Œæ¶µç›–ä»åŸºç¡€ç»„ä»¶åˆ°é«˜çº§åœºæ™¯çš„å®Œæ•´æ’æŸ¥æµç¨‹ã€‚

### 1.1 æ•…éšœæ’æŸ¥æ€ç»´å¯¼å›¾

```mermaid
graph TD
    A[Kubernetesæ•…éšœæ’æŸ¥] --> B[åŸºç¡€è®¾æ–½å±‚]
    A --> C[æ§åˆ¶å¹³é¢å±‚]
    A --> D[å·¥ä½œè´Ÿè½½å±‚]
    A --> E[ç½‘ç»œå±‚]
    A --> F[å­˜å‚¨å±‚]
    
    B --> B1[èŠ‚ç‚¹çŠ¶æ€æ£€æŸ¥]
    B --> B2[Kubeletæ•…éšœ]
    B --> B3[å®¹å™¨è¿è¡Œæ—¶é—®é¢˜]
    B --> B4[ç³»ç»Ÿèµ„æºä¸è¶³]
    
    C --> C1[API Serveré—®é¢˜]
    C --> C2[etcdé›†ç¾¤æ•…éšœ]
    C --> C3[Controller Managerå¼‚å¸¸]
    C --> C4[Schedulerè°ƒåº¦å¤±è´¥]
    
    D --> D1[Podå¯åŠ¨å¤±è´¥]
    D --> D2[å®¹å™¨å´©æºƒé‡å¯]
    D --> D3[åº”ç”¨æ€§èƒ½é—®é¢˜]
    D --> D4[èµ„æºé…é¢é™åˆ¶]
    
    E --> E1[Serviceè¿é€šæ€§]
    E --> E2[DNSè§£æå¤±è´¥]
    E --> E3[ç½‘ç»œç­–ç•¥é˜»æ–­]
    E --> E4[CNIæ’ä»¶é—®é¢˜]
    
    F --> F1[PV/PVCç»‘å®šå¤±è´¥]
    F --> F2[å­˜å‚¨å·æŒ‚è½½å¼‚å¸¸]
    F --> F3[æ•°æ®æŒä¹…åŒ–é—®é¢˜]
    F --> F4[å­˜å‚¨æ€§èƒ½ç“¶é¢ˆ]
```

### 1.2 æ ‡å‡†åŒ–æ’æŸ¥æµç¨‹

```python
# kubernetes_troubleshooting_framework.py
import subprocess
import json
import time
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass
from enum import Enum

class TroubleshootingLayer(Enum):
    """æ•…éšœæ’æŸ¥å±‚çº§"""
    INFRASTRUCTURE = "infrastructure"
    CONTROL_PLANE = "control_plane"
    WORKLOAD = "workload"
    NETWORK = "network"
    STORAGE = "storage"

class IssueSeverity(Enum):
    """é—®é¢˜ä¸¥é‡ç¨‹åº¦"""
    CRITICAL = "critical"
    HIGH = "high"
    MEDIUM = "medium"
    LOW = "low"

@dataclass
class DiagnosticResult:
    """è¯Šæ–­ç»“æœ"""
    layer: TroubleshootingLayer
    issue_type: str
    severity: IssueSeverity
    description: str
    affected_resources: List[str]
    diagnostic_commands: List[str]
    remediation_steps: List[str]
    verification_commands: List[str]

class KubernetesTroubleshooter:
    """Kubernetesæ•…éšœæ’æŸ¥å™¨"""
    
    def __init__(self):
        self.diagnostic_results = []
        self.cluster_info = {}
        
    def collect_cluster_info(self) -> Dict:
        """æ”¶é›†é›†ç¾¤åŸºç¡€ä¿¡æ¯"""
        info = {}
        
        # è·å–é›†ç¾¤ç‰ˆæœ¬
        try:
            version_output = subprocess.check_output(
                ["kubectl", "version", "--short"], 
                text=True, timeout=30
            )
            info['version'] = version_output.strip()
        except Exception as e:
            info['version_error'] = str(e)
        
        # è·å–èŠ‚ç‚¹çŠ¶æ€
        try:
            nodes_output = subprocess.check_output(
                ["kubectl", "get", "nodes", "-o", "wide"], 
                text=True, timeout=30
            )
            info['nodes'] = nodes_output.strip()
        except Exception as e:
            info['nodes_error'] = str(e)
        
        # è·å–å‘½åç©ºé—´åˆ—è¡¨
        try:
            namespaces_output = subprocess.check_output(
                ["kubectl", "get", "namespaces"], 
                text=True, timeout=30
            )
            info['namespaces'] = namespaces_output.strip()
        except Exception as e:
            info['namespaces_error'] = str(e)
        
        self.cluster_info = info
        return info
    
    def diagnose_infrastructure_layer(self) -> List[DiagnosticResult]:
        """è¯Šæ–­åŸºç¡€è®¾æ–½å±‚é—®é¢˜"""
        results = []
        
        # æ£€æŸ¥èŠ‚ç‚¹çŠ¶æ€
        node_issues = self._check_node_status()
        if node_issues:
            results.extend(node_issues)
        
        # æ£€æŸ¥KubeletçŠ¶æ€
        kubelet_issues = self._check_kubelet_status()
        if kubelet_issues:
            results.extend(kubelet_issues)
        
        # æ£€æŸ¥ç³»ç»Ÿèµ„æº
        resource_issues = self._check_system_resources()
        if resource_issues:
            results.extend(resource_issues)
        
        return results
    
    def _check_node_status(self) -> List[DiagnosticResult]:
        """æ£€æŸ¥èŠ‚ç‚¹çŠ¶æ€"""
        issues = []
        
        try:
            # è·å–èŠ‚ç‚¹è¯¦ç»†ä¿¡æ¯
            nodes_json = subprocess.check_output(
                ["kubectl", "get", "nodes", "-o", "json"], 
                text=True, timeout=30
            )
            nodes_data = json.loads(nodes_json)
            
            for node in nodes_data.get('items', []):
                node_name = node['metadata']['name']
                conditions = node['status'].get('conditions', [])
                
                # æ£€æŸ¥ReadyçŠ¶æ€
                ready_condition = next(
                    (c for c in conditions if c['type'] == 'Ready'), 
                    None
                )
                
                if not ready_condition or ready_condition['status'] != 'True':
                    issues.append(DiagnosticResult(
                        layer=TroubleshootingLayer.INFRASTRUCTURE,
                        issue_type="node_not_ready",
                        severity=IssueSeverity.HIGH,
                        description=f"Node {node_name} is not Ready",
                        affected_resources=[node_name],
                        diagnostic_commands=[
                            f"kubectl describe node {node_name}",
                            f"kubectl get events --field-selector involvedObject.name={node_name}"
                        ],
                        remediation_steps=[
                            "Check kubelet logs on the node",
                            "Verify node network connectivity",
                            "Check node resource availability"
                        ],
                        verification_commands=[
                            f"kubectl get node {node_name} -o wide"
                        ]
                    ))
                
                # æ£€æŸ¥å†…å­˜å‹åŠ›
                memory_pressure = next(
                    (c for c in conditions if c['type'] == 'MemoryPressure'), 
                    None
                )
                if memory_pressure and memory_pressure['status'] == 'True':
                    issues.append(DiagnosticResult(
                        layer=TroubleshootingLayer.INFRASTRUCTURE,
                        issue_type="memory_pressure",
                        severity=IssueSeverity.MEDIUM,
                        description=f"Node {node_name} experiencing memory pressure",
                        affected_resources=[node_name],
                        diagnostic_commands=[
                            f"kubectl describe node {node_name}",
                            "free -h",
                            "top -b -n1 | head -20"
                        ],
                        remediation_steps=[
                            "Identify memory-intensive pods",
                            "Consider node scaling or pod eviction",
                            "Optimize application memory usage"
                        ],
                        verification_commands=[
                            f"kubectl top node {node_name}"
                        ]
                    ))
        
        except Exception as e:
            issues.append(DiagnosticResult(
                layer=TroubleshootingLayer.INFRASTRUCTURE,
                issue_type="diagnostic_failure",
                severity=IssueSeverity.LOW,
                description=f"Failed to check node status: {str(e)}",
                affected_resources=[],
                diagnostic_commands=["kubectl get nodes"],
                remediation_steps=["Ensure kubectl is properly configured"],
                verification_commands=[]
            ))
        
        return issues
    
    def _check_kubelet_status(self) -> List[DiagnosticResult]:
        """æ£€æŸ¥KubeletçŠ¶æ€"""
        issues = []
        
        try:
            # æ£€æŸ¥Kubeletè¿›ç¨‹
            kubelet_check = subprocess.run(
                ["systemctl", "is-active", "kubelet"],
                capture_output=True, text=True, timeout=30
            )
            
            if kubelet_check.returncode != 0:
                issues.append(DiagnosticResult(
                    layer=TroubleshootingLayer.INFRASTRUCTURE,
                    issue_type="kubelet_down",
                    severity=IssueSeverity.CRITICAL,
                    description="Kubelet service is not running",
                    affected_resources=["kubelet"],
                    diagnostic_commands=[
                        "systemctl status kubelet",
                        "journalctl -u kubelet -n 50"
                    ],
                    remediation_steps=[
                        "Restart kubelet service: systemctl restart kubelet",
                        "Check kubelet configuration files",
                        "Verify kubelet certificates validity"
                    ],
                    verification_commands=[
                        "systemctl is-active kubelet"
                    ]
                ))
        
        except Exception as e:
            issues.append(DiagnosticResult(
                layer=TroubleshootingLayer.INFRASTRUCTURE,
                issue_type="kubelet_check_failed",
                severity=IssueSeverity.LOW,
                description=f"Failed to check kubelet status: {str(e)}",
                affected_resources=[],
                diagnostic_commands=["systemctl status kubelet"],
                remediation_steps=["Check system permissions"],
                verification_commands=[]
            ))
        
        return issues
    
    def _check_system_resources(self) -> List[DiagnosticResult]:
        """æ£€æŸ¥ç³»ç»Ÿèµ„æº"""
        issues = []
        
        try:
            # æ£€æŸ¥ç£ç›˜ä½¿ç”¨ç‡
            df_output = subprocess.check_output(
                ["df", "-h"], text=True, timeout=30
            )
            
            # ç®€å•çš„ç£ç›˜ä½¿ç”¨ç‡æ£€æŸ¥ï¼ˆè¶…è¿‡85%æŠ¥è­¦ï¼‰
            lines = df_output.strip().split('\n')[1:]  # è·³è¿‡æ ‡é¢˜è¡Œ
            for line in lines:
                parts = line.split()
                if len(parts) >= 5:
                    usage_str = parts[4].rstrip('%')
                    filesystem = parts[0]
                    try:
                        usage_percent = int(usage_str)
                        if usage_percent > 85:
                            issues.append(DiagnosticResult(
                                layer=TroubleshootingLayer.INFRASTRUCTURE,
                                issue_type="disk_space_low",
                                severity=IssueSeverity.HIGH if usage_percent > 95 else IssueSeverity.MEDIUM,
                                description=f"Disk {filesystem} usage is {usage_percent}%",
                                affected_resources=[filesystem],
                                diagnostic_commands=[
                                    f"df -h {filesystem}",
                                    "du -sh /* 2>/dev/null | sort -hr | head -10"
                                ],
                                remediation_steps=[
                                    "Clean up unused files and logs",
                                    "Remove old container images",
                                    "Consider expanding disk space"
                                ],
                                verification_commands=[
                                    "df -h"
                                ]
                            ))
                    except ValueError:
                        continue
        
        except Exception as e:
            issues.append(DiagnosticResult(
                layer=TroubleshootingLayer.INFRASTRUCTURE,
                issue_type="resource_check_failed",
                severity=IssueSeverity.LOW,
                description=f"Failed to check system resources: {str(e)}",
                affected_resources=[],
                diagnostic_commands=["df -h", "free -h"],
                remediation_steps=["Manual resource inspection required"],
                verification_commands=[]
            ))
        
        return issues
    
    def diagnose_control_plane_layer(self) -> List[DiagnosticResult]:
        """è¯Šæ–­æ§åˆ¶å¹³é¢å±‚é—®é¢˜"""
        results = []
        
        # æ£€æŸ¥API Server
        api_issues = self._check_api_server()
        if api_issues:
            results.extend(api_issues)
        
        # æ£€æŸ¥etcdé›†ç¾¤
        etcd_issues = self._check_etcd_cluster()
        if etcd_issues:
            results.extend(etcd_issues)
        
        return results
    
    def _check_api_server(self) -> List[DiagnosticResult]:
        """æ£€æŸ¥API Server"""
        issues = []
        
        try:
            # æ£€æŸ¥API Serverå¥åº·çŠ¶æ€
            health_check = subprocess.run(
                ["kubectl", "get", "--raw", "/healthz"],
                capture_output=True, text=True, timeout=30
            )
            
            if health_check.returncode != 0 or health_check.stdout.strip() != "ok":
                issues.append(DiagnosticResult(
                    layer=TroubleshootingLayer.CONTROL_PLANE,
                    issue_type="api_server_unhealthy",
                    severity=IssueSeverity.CRITICAL,
                    description="API Server health check failed",
                    affected_resources=["api-server"],
                    diagnostic_commands=[
                        "kubectl get componentstatuses",
                        "kubectl get pods -n kube-system | grep apiserver"
                    ],
                    remediation_steps=[
                        "Check API Server pod logs",
                        "Verify API Server certificates",
                        "Review API Server configuration"
                    ],
                    verification_commands=[
                        "kubectl get --raw /healthz"
                    ]
                ))
        
        except Exception as e:
            issues.append(DiagnosticResult(
                layer=TroubleshootingLayer.CONTROL_PLANE,
                issue_type="api_server_check_failed",
                severity=IssueSeverity.HIGH,
                description=f"Failed to check API Server: {str(e)}",
                affected_resources=[],
                diagnostic_commands=["kubectl cluster-info"],
                remediation_steps=["Verify kubectl configuration"],
                verification_commands=[]
            ))
        
        return issues
    
    def _check_etcd_cluster(self) -> List[DiagnosticResult]:
        """æ£€æŸ¥etcdé›†ç¾¤"""
        issues = []
        
        try:
            # æ£€æŸ¥etcd podçŠ¶æ€
            etcd_pods = subprocess.check_output(
                ["kubectl", "get", "pods", "-n", "kube-system", "-l", "component=etcd"],
                text=True, timeout=30
            )
            
            if "Running" not in etcd_pods:
                issues.append(DiagnosticResult(
                    layer=TroubleshootingLayer.CONTROL_PLANE,
                    issue_type="etcd_unhealthy",
                    severity=IssueSeverity.CRITICAL,
                    description="etcd pods are not running properly",
                    affected_resources=["etcd"],
                    diagnostic_commands=[
                        "kubectl get pods -n kube-system -l component=etcd",
                        "kubectl logs -n kube-system -l component=etcd --tail=50"
                    ],
                    remediation_steps=[
                        "Check etcd pod logs for errors",
                        "Verify etcd certificates",
                        "Review etcd cluster member status"
                    ],
                    verification_commands=[
                        "kubectl get pods -n kube-system -l component=etcd"
                    ]
                ))
        
        except Exception as e:
            issues.append(DiagnosticResult(
                layer=TroubleshootingLayer.CONTROL_PLANE,
                issue_type="etcd_check_failed",
                severity=IssueSeverity.HIGH,
                description=f"Failed to check etcd cluster: {str(e)}",
                affected_resources=[],
                diagnostic_commands=["kubectl get pods -n kube-system"],
                remediation_steps=["Manual etcd inspection required"],
                verification_commands=[]
            ))
        
        return issues
    
    def run_comprehensive_diagnosis(self) -> Dict:
        """è¿è¡Œç»¼åˆè¯Šæ–­"""
        print("ğŸš€ Starting comprehensive Kubernetes diagnosis...")
        
        # æ”¶é›†é›†ç¾¤ä¿¡æ¯
        print("ğŸ“‹ Collecting cluster information...")
        cluster_info = self.collect_cluster_info()
        
        # æŒ‰å±‚çº§è¿›è¡Œè¯Šæ–­
        print("ğŸ” Diagnosing infrastructure layer...")
        infra_issues = self.diagnose_infrastructure_layer()
        
        print("ğŸ” Diagnosing control plane layer...")
        control_plane_issues = self.diagnose_control_plane_layer()
        
        # åˆå¹¶æ‰€æœ‰è¯Šæ–­ç»“æœ
        all_issues = infra_issues + control_plane_issues
        self.diagnostic_results = all_issues
        
        # ç”Ÿæˆè¯Šæ–­æŠ¥å‘Š
        report = self._generate_diagnostic_report(cluster_info, all_issues)
        
        return report
    
    def _generate_diagnostic_report(self, cluster_info: Dict, issues: List[DiagnosticResult]) -> Dict:
        """ç”Ÿæˆè¯Šæ–­æŠ¥å‘Š"""
        # æŒ‰ä¸¥é‡ç¨‹åº¦åˆ†ç±»
        critical_issues = [issue for issue in issues if issue.severity == IssueSeverity.CRITICAL]
        high_issues = [issue for issue in issues if issue.severity == IssueSeverity.HIGH]
        medium_issues = [issue for issue in issues if issue.severity == IssueSeverity.MEDIUM]
        low_issues = [issue for issue in issues if issue.severity == IssueSeverity.LOW]
        
        # æŒ‰å±‚çº§åˆ†ç±»
        issues_by_layer = {}
        for issue in issues:
            layer_name = issue.layer.value
            if layer_name not in issues_by_layer:
                issues_by_layer[layer_name] = []
            issues_by_layer[layer_name].append(issue)
        
        report = {
            "timestamp": time.strftime('%Y-%m-%d %H:%M:%S'),
            "cluster_info": cluster_info,
            "summary": {
                "total_issues": len(issues),
                "critical": len(critical_issues),
                "high": len(high_issues),
                "medium": len(medium_issues),
                "low": len(low_issues)
            },
            "issues_by_severity": {
                "critical": [self._serialize_issue(issue) for issue in critical_issues],
                "high": [self._serialize_issue(issue) for issue in high_issues],
                "medium": [self._serialize_issue(issue) for issue in medium_issues],
                "low": [self._serialize_issue(issue) for issue in low_issues]
            },
            "issues_by_layer": {
                layer: [self._serialize_issue(issue) for issue in layer_issues]
                for layer, layer_issues in issues_by_layer.items()
            },
            "recommendations": self._generate_recommendations(issues)
        }
        
        return report
    
    def _serialize_issue(self, issue: DiagnosticResult) -> Dict:
        """åºåˆ—åŒ–è¯Šæ–­ç»“æœ"""
        return {
            "layer": issue.layer.value,
            "issue_type": issue.issue_type,
            "severity": issue.severity.value,
            "description": issue.description,
            "affected_resources": issue.affected_resources,
            "diagnostic_commands": issue.diagnostic_commands,
            "remediation_steps": issue.remediation_steps,
            "verification_commands": issue.verification_commands
        }
    
    def _generate_recommendations(self, issues: List[DiagnosticResult]) -> List[str]:
        """ç”Ÿæˆä¿®å¤å»ºè®®"""
        recommendations = []
        
        # åŸºäºå‘ç°çš„é—®é¢˜ç”Ÿæˆé’ˆå¯¹æ€§å»ºè®®
        if any(issue.layer == TroubleshootingLayer.INFRASTRUCTURE for issue in issues):
            recommendations.append("ğŸ”§ ä¼˜å…ˆè§£å†³åŸºç¡€è®¾æ–½å±‚é—®é¢˜ï¼Œç¡®ä¿èŠ‚ç‚¹å’ŒKubeletæ­£å¸¸è¿è¡Œ")
        
        if any(issue.layer == TroubleshootingLayer.CONTROL_PLANE for issue in issues):
            recommendations.append("âš™ï¸  æ§åˆ¶å¹³é¢é—®é¢˜éœ€è¦ç«‹å³å…³æ³¨ï¼Œå¯èƒ½å½±å“æ•´ä¸ªé›†ç¾¤ç¨³å®šæ€§")
        
        # æ·»åŠ é€šç”¨å»ºè®®
        recommendations.extend([
            "ğŸ“Š å»ºç«‹å®Œå–„çš„ç›‘æ§å‘Šè­¦ä½“ç³»",
            "ğŸ“ åˆ¶å®šæ ‡å‡†çš„æ•…éšœæ’æŸ¥æµç¨‹",
            "ğŸ“š å®šæœŸè¿›è¡Œæ•…éšœæ¼”ç»ƒå’ŒåŸ¹è®­",
            "ğŸ”„ å»ºç«‹è‡ªåŠ¨åŒ–çš„æ•…éšœæ¢å¤æœºåˆ¶"
        ])
        
        return recommendations

# ä½¿ç”¨ç¤ºä¾‹
def main():
    troubleshooter = KubernetesTroubleshooter()
    
    # è¿è¡Œç»¼åˆè¯Šæ–­
    report = troubleshooter.run_comprehensive_diagnosis()
    
    # è¾“å‡ºè¯Šæ–­æŠ¥å‘Š
    print("\n" + "="*60)
    print("ğŸ“‹ KUBERNETES DIAGNOSTIC REPORT")
    print("="*60)
    
    print(f"\nâ° Timestamp: {report['timestamp']}")
    print(f"ğŸ“Š Summary: {report['summary']['total_issues']} issues found")
    print(f"  Critical: {report['summary']['critical']}")
    print(f"  High: {report['summary']['high']}")
    print(f"  Medium: {report['summary']['medium']}")
    print(f"  Low: {report['summary']['low']}")
    
    # æ˜¾ç¤ºæŒ‰ä¸¥é‡ç¨‹åº¦åˆ†ç±»çš„é—®é¢˜
    for severity in ['critical', 'high', 'medium', 'low']:
        issues = report['issues_by_severity'][severity]
        if issues:
            print(f"\nğŸ”´ {severity.upper()} Issues ({len(issues)}):")
            for i, issue in enumerate(issues, 1):
                print(f"  {i}. [{issue['layer']}] {issue['description']}")
                print(f"     Resources: {', '.join(issue['affected_resources'])}")
    
    # æ˜¾ç¤ºä¿®å¤å»ºè®®
    print(f"\nğŸ’¡ Recommendations:")
    for i, rec in enumerate(report['recommendations'], 1):
        print(f"  {i}. {rec}")

if __name__ == "__main__":
    main()
```

## 2. å¸¸è§æ•…éšœåœºæ™¯åŠè§£å†³æ–¹æ¡ˆ

### 2.1 Podç›¸å…³æ•…éšœ

```yaml
# pod-troubleshooting-cheatsheet.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: pod-troubleshooting-guide
  namespace: default
data:
  common-pod-issues: |
    # Podå¸¸è§é—®é¢˜æ’æŸ¥æ¸…å•
    
    ## 1. Podå¤„äºPendingçŠ¶æ€
    ### è¯Šæ–­å‘½ä»¤:
    kubectl describe pod <pod-name>
    kubectl get events --field-selector involvedObject.name=<pod-name>
    
    ### å¸¸è§åŸå› :
    - èµ„æºé…é¢ä¸è¶³
    - èŠ‚ç‚¹é€‰æ‹©å™¨ä¸åŒ¹é…
    - æŒä¹…åŒ–å­˜å‚¨ç»‘å®šå¤±è´¥
    - é•œåƒæ‹‰å–å¤±è´¥
    
    ### è§£å†³æ–¹æ¡ˆ:
    ```yaml
    # è°ƒæ•´èµ„æºè¯·æ±‚
    resources:
      requests:
        cpu: "100m"
        memory: "128Mi"
      limits:
        cpu: "500m"
        memory: "512Mi"
    ```
    
    ## 2. Podä¸æ–­é‡å¯
    ### è¯Šæ–­å‘½ä»¤:
    kubectl logs <pod-name> --previous
    kubectl describe pod <pod-name>
    
    ### å¸¸è§åŸå› :
    - åº”ç”¨å¯åŠ¨å¤±è´¥
    - å¥åº·æ£€æŸ¥å¤±è´¥
    - èµ„æºé™åˆ¶è¿‡ä½
    - é…ç½®é”™è¯¯
    
    ### è§£å†³æ–¹æ¡ˆ:
    ```yaml
    # è°ƒæ•´å¥åº·æ£€æŸ¥é…ç½®
    livenessProbe:
      httpGet:
        path: /health
        port: 8080
      initialDelaySeconds: 60
      periodSeconds: 10
      timeoutSeconds: 5
      failureThreshold: 3
    ```
    
    ## 3. Pod CrashLoopBackOff
    ### è¯Šæ–­å‘½ä»¤:
    kubectl logs <pod-name> --previous
    kubectl exec -it <pod-name> -- /bin/sh
    
    ### å¸¸è§åŸå› :
    - åº”ç”¨ä»£ç é”™è¯¯
    - ä¾èµ–æœåŠ¡ä¸å¯ç”¨
    - ç¯å¢ƒå˜é‡é…ç½®é”™è¯¯
    - æƒé™ä¸è¶³
    
    ### è§£å†³æ–¹æ¡ˆ:
    ```yaml
    # æ·»åŠ è°ƒè¯•é…ç½®
    env:
    - name: DEBUG
      value: "true"
    - name: LOG_LEVEL
      value: "debug"
    ```

  debugging-tools: |
    # å®ç”¨è°ƒè¯•å·¥å…·é›†åˆ
    
    ## 1. ä¸´æ—¶è°ƒè¯•å®¹å™¨
    kubectl debug <pod-name> -it --image=busybox --target=<container-name>
    
    ## 2. ç«¯å£è½¬å‘è°ƒè¯•
    kubectl port-forward <pod-name> 8080:8080
    
    ## 3. èµ„æºä½¿ç”¨æƒ…å†µ
    kubectl top pod <pod-name>
    kubectl top node
    
    ## 4. ç½‘ç»œè¿é€šæ€§æµ‹è¯•
    kubectl exec -it <pod-name> -- ping google.com
    kubectl exec -it <pod-name> -- nslookup kubernetes.default
    
    ## 5. æ–‡ä»¶ç³»ç»Ÿæ£€æŸ¥
    kubectl exec -it <pod-name> -- df -h
    kubectl exec -it <pod-name> -- du -sh /app/logs/

# æ•…éšœæ’æŸ¥è„šæœ¬é›†åˆ
# troubleshooting-scripts.sh
#!/bin/bash

# Podæ•…éšœå¿«é€Ÿè¯Šæ–­è„šæœ¬
diagnose_pod() {
    local pod_name=$1
    local namespace=${2:-default}
    
    echo "ğŸ” Diagnosing pod: $pod_name in namespace: $namespace"
    echo "==========================================="
    
    # 1. åŸºæœ¬çŠ¶æ€æ£€æŸ¥
    echo "1. Pod status:"
    kubectl get pod "$pod_name" -n "$namespace" -o wide
    
    # 2. è¯¦ç»†æè¿°
    echo -e "\n2. Pod description:"
    kubectl describe pod "$pod_name" -n "$namespace"
    
    # 3. ç›¸å…³äº‹ä»¶
    echo -e "\n3. Recent events:"
    kubectl get events -n "$namespace" --field-selector involvedObject.name="$pod_name" --sort-by='.lastTimestamp'
    
    # 4. å®¹å™¨æ—¥å¿—
    echo -e "\n4. Container logs:"
    kubectl logs "$pod_name" -n "$namespace" --tail=50
    
    # 5. ä¸Šä¸€æ¬¡å®¹å™¨æ—¥å¿—ï¼ˆå¦‚æœé‡å¯è¿‡ï¼‰
    echo -e "\n5. Previous container logs (if available):"
    kubectl logs "$pod_name" -n "$namespace" --previous --tail=50 2>/dev/null || echo "No previous logs available"
    
    # 6. èµ„æºä½¿ç”¨æƒ…å†µ
    echo -e "\n6. Resource usage:"
    kubectl top pod "$pod_name" -n "$namespace" 2>/dev/null || echo "Metrics server not available"
}

# èŠ‚ç‚¹æ•…éšœè¯Šæ–­è„šæœ¬
diagnose_node() {
    local node_name=$1
    
    echo "ğŸ” Diagnosing node: $node_name"
    echo "================================"
    
    # 1. èŠ‚ç‚¹åŸºæœ¬çŠ¶æ€
    echo "1. Node status:"
    kubectl get node "$node_name" -o wide
    
    # 2. èŠ‚ç‚¹è¯¦ç»†ä¿¡æ¯
    echo -e "\n2. Node details:"
    kubectl describe node "$node_name"
    
    # 3. èŠ‚ç‚¹ä¸Šçš„Pods
    echo -e "\n3. Pods on this node:"
    kubectl get pods -A -o wide --field-selector spec.nodeName="$node_name"
    
    # 4. èŠ‚ç‚¹èµ„æºä½¿ç”¨
    echo -e "\n4. Node resource usage:"
    kubectl top node "$node_name" 2>/dev/null || echo "Metrics server not available"
}

# ç½‘ç»œæ•…éšœè¯Šæ–­è„šæœ¬
diagnose_network() {
    echo "ğŸ” Network diagnostics"
    echo "======================"
    
    # 1. DNSè§£ææµ‹è¯•
    echo "1. DNS resolution test:"
    kubectl run -it --rm debug-pod --image=busybox --restart=Never -- sh -c "nslookup kubernetes.default"
    
    # 2. æœåŠ¡è¿é€šæ€§æµ‹è¯•
    echo -e "\n2. Service connectivity test:"
    kubectl run -it --rm debug-pod --image=busybox --restart=Never -- sh -c "wget -qO- http://kubernetes.default"
    
    # 3. ç½‘ç»œç­–ç•¥æ£€æŸ¥
    echo -e "\n3. Network policies:"
    kubectl get networkpolicies -A
    
    # 4. CoreDNSçŠ¶æ€
    echo -e "\n4. CoreDNS status:"
    kubectl get pods -n kube-system -l k8s-app=kube-dns
}

# ä½¿ç”¨ç¤ºä¾‹
echo "Kubernetes Troubleshooting Toolkit"
echo "=================================="
echo "Usage:"
echo "  diagnose_pod <pod-name> [namespace]"
echo "  diagnose_node <node-name>"
echo "  diagnose_network"
```

## 3. ç›‘æ§å‘Šè­¦é…ç½®

### 3.1 Prometheuså‘Šè­¦è§„åˆ™

```yaml
# kubernetes-alerts.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: kubernetes-troubleshooting-alerts
  namespace: monitoring
spec:
  groups:
  - name: kubernetes.node
    rules:
    - alert: NodeNotReady
      expr: kube_node_status_condition{condition="Ready",status="true"} == 0
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "Node {{ $labels.node }} is not ready"
        description: "Node has been unready for more than 5 minutes"
    
    - alert: NodeMemoryPressure
      expr: kube_node_status_condition{condition="MemoryPressure",status="true"} == 1
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "Node {{ $labels.node }} has memory pressure"
        description: "Node is experiencing memory pressure"
    
    - alert: NodeDiskPressure
      expr: kube_node_status_condition{condition="DiskPressure",status="true"} == 1
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "Node {{ $labels.node }} has disk pressure"
        description: "Node is experiencing disk pressure"
  
  - name: kubernetes.pod
    rules:
    - alert: PodCrashLooping
      expr: rate(kube_pod_container_status_restarts_total[5m]) > 0.1
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: "Pod {{ $labels.pod }} is crash looping"
        description: "Pod is restarting more than 6 times per hour"
    
    - alert: PodPending
      expr: kube_pod_status_phase{phase="Pending"} == 1
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: "Pod {{ $labels.pod }} is stuck in Pending"
        description: "Pod has been pending for more than 10 minutes"
    
    - alert: PodNotReady
      expr: kube_pod_status_ready{condition="true"} == 0
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Pod {{ $labels.pod }} is not ready"
        description: "Pod has been not ready for more than 5 minutes"
  
  - name: kubernetes.controlplane
    rules:
    - alert: APIServerDown
      expr: up{job="apiserver"} == 0
      for: 2m
      labels:
        severity: critical
      annotations:
        summary: "API Server is down"
        description: "Kubernetes API Server is unreachable"
    
    - alert: EtcdUnavailable
      expr: up{job="etcd"} == 0
      for: 2m
      labels:
        severity: critical
      annotations:
        summary: "etcd is unavailable"
        description: "etcd cluster is not responding"
    
    - alert: SchedulerDown
      expr: up{job="scheduler"} == 0
      for: 2m
      labels:
        severity: critical
      annotations:
        summary: "Scheduler is down"
        description: "Kubernetes scheduler is not running"
```

### 3.2 Grafanaæ•…éšœæ’æŸ¥ä»ªè¡¨æ¿

```json
{
  "dashboard": {
    "id": null,
    "title": "Kubernetes Troubleshooting Dashboard",
    "timezone": "browser",
    "panels": [
      {
        "type": "stat",
        "title": "Cluster Health Overview",
        "gridPos": {"h": 6, "w": 8, "x": 0, "y": 0},
        "targets": [
          {
            "expr": "sum(kube_node_status_condition{condition=\"Ready\",status=\"true\"})",
            "legendFormat": "Ready Nodes"
          },
          {
            "expr": "count(kube_pod_status_phase{phase=\"Running\"})",
            "legendFormat": "Running Pods"
          },
          {
            "expr": "sum(up{job=\"apiserver\"})",
            "legendFormat": "API Servers Up"
          }
        ]
      },
      {
        "type": "graph",
        "title": "Node Resource Usage",
        "gridPos": {"h": 8, "w": 12, "x": 0, "y": 6},
        "targets": [
          {
            "expr": "100 - (avg(rate(node_cpu_seconds_total{mode=\"idle\"}[5m])) * 100)",
            "legendFormat": "CPU Usage %"
          },
          {
            "expr": "(node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) / node_memory_MemTotal_bytes * 100",
            "legendFormat": "Memory Usage %"
          }
        ]
      },
      {
        "type": "table",
        "title": "Recent Pod Issues",
        "gridPos": {"h": 8, "w": 12, "x": 0, "y": 14},
        "targets": [
          {
            "expr": "kube_pod_container_status_restarts_total > 0",
            "format": "table"
          }
        ]
      }
    ]
  }
}
```

è¿™ä¸ªç»¼åˆæ•…éšœæ’æŸ¥æŒ‡å—æä¾›äº†ç³»ç»ŸåŒ–çš„é—®é¢˜è¯Šæ–­æ–¹æ³•ã€å®ç”¨çš„æ’æŸ¥å·¥å…·å’Œç›‘æ§å‘Šè­¦é…ç½®ï¼Œå¸®åŠ©è¿ç»´äººå‘˜å¿«é€Ÿå®šä½å’Œè§£å†³Kubernetesé›†ç¾¤ä¸­çš„å„ç§é—®é¢˜ã€‚