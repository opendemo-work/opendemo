# ç¥ç»ç½‘ç»œåŸºç¡€

## ğŸ¯ æ¡ˆä¾‹æ¦‚è¿°

è¿™æ˜¯ä¸€ä¸ªå…¨é¢å±•ç¤ºç¥ç»ç½‘ç»œåŸºç¡€æ¦‚å¿µçš„ç¤ºä¾‹ï¼Œæ¶µç›–æ„ŸçŸ¥æœºã€å¤šå±‚æ„ŸçŸ¥æœºã€æ¿€æ´»å‡½æ•°ã€åå‘ä¼ æ’­ç­‰æ·±åº¦å­¦ä¹ æ ¸å¿ƒåŸç†ã€‚

## ğŸ“š å­¦ä¹ ç›®æ ‡

é€šè¿‡æœ¬ç¤ºä¾‹ä½ å°†æŒæ¡ï¼š
- æ„ŸçŸ¥æœºçš„å·¥ä½œåŸç†å’Œå±€é™æ€§
- å¤šå±‚æ„ŸçŸ¥æœºçš„ç»“æ„å’Œè®­ç»ƒæ–¹æ³•
- å¸¸ç”¨æ¿€æ´»å‡½æ•°çš„ç‰¹ç‚¹å’Œåº”ç”¨
- åå‘ä¼ æ’­ç®—æ³•çš„æ‰‹åŠ¨å®ç°
- æ·±åº¦å­¦ä¹ åŸºç¡€æ¦‚å¿µçš„ç†è§£

## ğŸ”§ æ ¸å¿ƒçŸ¥è¯†ç‚¹

### 1. æ„ŸçŸ¥æœºåŸç†
- å•å±‚ç¥ç»ç½‘ç»œç»“æ„
- æƒé‡æ›´æ–°è§„åˆ™
- çº¿æ€§å¯åˆ†é—®é¢˜
- é€»è¾‘é—¨å®ç°

### 2. å¤šå±‚æ„ŸçŸ¥æœº
- éšè—å±‚çš„ä½œç”¨
- éçº¿æ€§æ˜ å°„èƒ½åŠ›
- å‰å‘ä¼ æ’­è¿‡ç¨‹
- ç½‘ç»œæ·±åº¦çš„å½±å“

### 3. æ¿€æ´»å‡½æ•°
- Sigmoidå‡½æ•°ç‰¹æ€§
- ReLUåŠå…¶å˜ç§
- Tanhå‡½æ•°
- Softmaxå‡½æ•°

### 4. åå‘ä¼ æ’­
- é“¾å¼æ³•åˆ™åº”ç”¨
- æ¢¯åº¦è®¡ç®—è¿‡ç¨‹
- æƒé‡æ›´æ–°æœºåˆ¶
- æŸå¤±å‡½æ•°ä¼˜åŒ–

## ğŸš€ è¿è¡Œç¤ºä¾‹

```bash
# å®‰è£…ä¾èµ–
pip install numpy matplotlib torch pytest

# è¿è¡Œä¸»ç¨‹åº
python neural_network_basics.py

# è¿è¡Œæµ‹è¯•
python -m pytest test_neural_network.py -v
```

## ğŸ“– ä»£ç è¯¦è§£

### ä¸»è¦ç±»ç»“æ„

```python
class NeuralNetworkBasics:
    def perceptron_demo(self):           # æ„ŸçŸ¥æœºæ¼”ç¤º
    def mlp_classification_demo(self):   # å¤šå±‚æ„ŸçŸ¥æœºæ¼”ç¤º
    def activation_functions_demo(self): # æ¿€æ´»å‡½æ•°æ¼”ç¤º
    def backpropagation_demo(self):      # åå‘ä¼ æ’­æ¼”ç¤º
```

### å…³é”®æŠ€æœ¯ç‚¹æ¼”ç¤º

#### 1. æ„ŸçŸ¥æœºå®ç°
```python
# æ„ŸçŸ¥æœºå‰å‘ä¼ æ’­
z = np.dot(weights, input) + bias
output = step_activation(z)  # é˜¶è·ƒå‡½æ•°

# æƒé‡æ›´æ–°è§„åˆ™
weights += learning_rate * error * input
bias += learning_rate * error
```

#### 2. MLPç½‘ç»œç»“æ„
```python
class SimpleMLP(nn.Module):
    def __init__(self):
        self.hidden = nn.Linear(2, 8)    # è¾“å…¥å±‚åˆ°éšè—å±‚
        self.output = nn.Linear(8, 1)    # éšè—å±‚åˆ°è¾“å‡ºå±‚
        self.activation = nn.ReLU()      # æ¿€æ´»å‡½æ•°
    
    def forward(self, x):
        x = self.activation(self.hidden(x))
        x = torch.sigmoid(self.output(x))
        return x
```

#### 3. åå‘ä¼ æ’­æ‰‹åŠ¨å®ç°
```python
# è¾“å‡ºå±‚æ¢¯åº¦
dL_da2 = -(target - output)
da2_dz2 = output * (1 - output)  # sigmoidå¯¼æ•°
dL_dz2 = dL_da2 * da2_dz2

# æƒé‡æ¢¯åº¦
dL_dW2 = np.outer(hidden_output, dL_dz2)
dL_dW1 = np.outer(input, dL_dz1)

# æ›´æ–°æƒé‡
W2 -= learning_rate * dL_dW2
W1 -= learning_rate * dL_dW1
```

#### 4. æ¿€æ´»å‡½æ•°å¯¹æ¯”
```python
# Sigmoid: Ïƒ(x) = 1/(1+e^(-x))
def sigmoid(x): return 1 / (1 + np.exp(-x))

# ReLU: f(x) = max(0,x)
def relu(x): return np.maximum(0, x)

# Tanh: tanh(x) = (e^x - e^(-x))/(e^x + e^(-x))
def tanh(x): return np.tanh(x)
```

## ğŸ§ª æµ‹è¯•è¦†ç›–

æµ‹è¯•æ–‡ä»¶ `test_neural_network.py` åŒ…å«ä»¥ä¸‹æµ‹è¯•ï¼š

âœ… é˜¶è·ƒæ¿€æ´»å‡½æ•°æµ‹è¯•  
âœ… Sigmoidå‡½æ•°æµ‹è¯•  
âœ… ReLUå‡½æ•°æµ‹è¯•  
âœ… Tanhå‡½æ•°æµ‹è¯•  
âœ… æ„ŸçŸ¥æœºANDé—¨æµ‹è¯•  
âœ… Softmaxå½’ä¸€åŒ–æµ‹è¯•  
âœ… çŸ©é˜µè¿ç®—å½¢çŠ¶æµ‹è¯•  
âœ… PyTorch MLPç»“æ„æµ‹è¯•  
âœ… æ¢¯åº¦è®¡ç®—æµ‹è¯•  
âœ… XORé—®é¢˜çº¿æ€§ä¸å¯åˆ†æµ‹è¯•  

## ğŸ¯ å®é™…åº”ç”¨åœºæ™¯

### 1. å›¾åƒåˆ†ç±»
```python
# CNNä¸­çš„å…¨è¿æ¥å±‚å°±æ˜¯MLP
class ImageClassifier(nn.Module):
    def __init__(self):
        self.conv_layers = nn.Sequential(...)
        self.fc_layers = nn.Sequential(
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Linear(256, 10)  # 10ä¸ªç±»åˆ«
        )
```

### 2. è‡ªç„¶è¯­è¨€å¤„ç†
```python
# Transformerä¸­çš„å‰é¦ˆç½‘ç»œ
class FeedForward(nn.Module):
    def __init__(self, d_model, d_ff):
        self.linear1 = nn.Linear(d_model, d_ff)
        self.activation = nn.ReLU()
        self.linear2 = nn.Linear(d_ff, d_model)
```

### 3. å¼ºåŒ–å­¦ä¹ 
```python
# ç­–ç•¥ç½‘ç»œ
class PolicyNetwork(nn.Module):
    def __init__(self, state_dim, action_dim):
        self.network = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ReLU(),
            nn.Linear(128, action_dim),
            nn.Softmax(dim=-1)
        )
```

## âš¡ æœ€ä½³å®è·µå»ºè®®

### 1. ç½‘ç»œè®¾è®¡
- æ ¹æ®é—®é¢˜å¤æ‚åº¦é€‰æ‹©åˆé€‚çš„ç½‘ç»œæ·±åº¦
- éšè—å±‚å¤§å°é€šå¸¸ä¸ºè¾“å…¥è¾“å‡ºçš„å‡ ä½•å¹³å‡
- ä½¿ç”¨æ‰¹å½’ä¸€åŒ–æé«˜è®­ç»ƒç¨³å®šæ€§

### 2. æ¿€æ´»å‡½æ•°é€‰æ‹©
- éšè—å±‚ä¼˜å…ˆä½¿ç”¨ReLUæˆ–å…¶å˜ç§
- è¾“å‡ºå±‚æ ¹æ®ä»»åŠ¡é€‰æ‹©ï¼šsigmoid(äºŒåˆ†ç±»)ã€softmax(å¤šåˆ†ç±»)
- é¿å…æ¢¯åº¦æ¶ˆå¤±ï¼šä½¿ç”¨æ®‹å·®è¿æ¥æˆ–å½’ä¸€åŒ–

### 3. è®­ç»ƒæŠ€å·§
- åˆç†è®¾ç½®å­¦ä¹ ç‡å’Œä¼˜åŒ–å™¨
- ä½¿ç”¨æ—©åœé˜²æ­¢è¿‡æ‹Ÿåˆ
- ç›‘æ§è®­ç»ƒå’ŒéªŒè¯æŸå¤±æ›²çº¿

## ğŸ” å¸¸è§é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ

### 1. æ¢¯åº¦æ¶ˆå¤±é—®é¢˜
```python
# é—®é¢˜ï¼šæ·±å±‚ç½‘ç»œæ¢¯åº¦æ¥è¿‘é›¶
# è§£å†³ï¼šä½¿ç”¨æ®‹å·®è¿æ¥
class ResidualBlock(nn.Module):
    def forward(self, x):
        residual = x
        out = self.layers(x)
        return out + residual  # æ®‹å·®è¿æ¥
```

### 2. è¿‡æ‹Ÿåˆé—®é¢˜
```python
# é—®é¢˜ï¼šè®­ç»ƒæ•ˆæœå¥½ä½†æµ‹è¯•æ•ˆæœå·®
# è§£å†³ï¼šæ·»åŠ æ­£åˆ™åŒ–
model = nn.Sequential(
    nn.Linear(100, 50),
    nn.Dropout(0.5),  # Dropoutæ­£åˆ™åŒ–
    nn.Linear(50, 10)
)
```

## ğŸ“š æ‰©å±•å­¦ä¹ èµ„æº

### å®˜æ–¹æ–‡æ¡£
- [PyTorchç¥ç»ç½‘ç»œ](https://pytorch.org/docs/stable/nn.html)
- [æ·±åº¦å­¦ä¹ ä¼˜åŒ–å™¨](https://pytorch.org/docs/stable/optim.html)

### æ¨èä¹¦ç±
- ã€Šæ·±åº¦å­¦ä¹ ã€‹- Ian Goodfellow
- ã€ŠåŠ¨æ‰‹å­¦æ·±åº¦å­¦ä¹ ã€‹- ææ²
- ã€Šç¥ç»ç½‘ç»œä¸æ·±åº¦å­¦ä¹ ã€‹- é‚±é”¡é¹

### ç›¸å…³è¯¾ç¨‹
- CS231nå·ç§¯ç¥ç»ç½‘ç»œ
- DeepLearning.AIæ·±åº¦å­¦ä¹ ä¸“é¡¹è¯¾ç¨‹

## ğŸ”„ ç‰ˆæœ¬å†å²

- v1.0.0 (2024-01-15): åˆå§‹ç‰ˆæœ¬ï¼ŒåŒ…å«å®Œæ•´çš„ç¥ç»ç½‘ç»œåŸºç¡€æ¼”ç¤º

---
**æ³¨æ„**: ç¥ç»ç½‘ç»œæ˜¯æ·±åº¦å­¦ä¹ çš„åŸºç¡€ï¼Œæ·±å…¥ç†è§£è¿™äº›æ¦‚å¿µå¯¹æŒæ¡ç°ä»£AIæŠ€æœ¯è‡³å…³é‡è¦ã€‚