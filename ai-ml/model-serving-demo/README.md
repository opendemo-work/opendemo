# AIæ¨¡å‹éƒ¨ç½²ä¸æ¨ç†æœåŠ¡å®æˆ˜æ¼”ç¤º

## ğŸ¯ å­¦ä¹ ç›®æ ‡

é€šè¿‡æœ¬æ¡ˆä¾‹ä½ å°†æŒæ¡ï¼š
- AIæ¨¡å‹éƒ¨ç½²çš„æœ€ä½³å®è·µå’Œæ¶æ„æ¨¡å¼
- æ¨¡å‹ç‰ˆæœ¬ç®¡ç†å’ŒA/Bæµ‹è¯•ç­–ç•¥
- æ‰¹é‡æ¨ç†å’Œå®æ—¶æ¨ç†ä¼˜åŒ–æŠ€æœ¯
- æ¨¡å‹æœåŠ¡çš„ç›‘æ§å’Œå¼¹æ€§ä¼¸ç¼©

## ğŸ› ï¸ ç¯å¢ƒå‡†å¤‡

### ç³»ç»Ÿè¦æ±‚
- Python 3.8+
- Dockerç¯å¢ƒ
- Kubernetesé›†ç¾¤ï¼ˆå¯é€‰ï¼Œç”¨äºç”Ÿäº§éƒ¨ç½²ï¼‰

### ä¾èµ–å®‰è£…
```bash
pip install torch transformers scikit-learn
pip install fastapi uvicorn gunicorn  # Webæ¡†æ¶
pip install celery redis  # å¼‚æ­¥ä»»åŠ¡å¤„ç†
pip install prometheus-client  # ç›‘æ§
pip install pytest  # æµ‹è¯•å·¥å…·
```

## ğŸ“ é¡¹ç›®ç»“æ„

```
model-serving-demo/
â”œâ”€â”€ README.md                           # æœ¬è¯´æ˜æ–‡æ¡£
â”œâ”€â”€ metadata.json                       # å…ƒæ•°æ®é…ç½®
â”œâ”€â”€ api/                               # APIæœåŠ¡
â”‚   â”œâ”€â”€ main.py                        # ä¸»æœåŠ¡å…¥å£
â”‚   â”œâ”€â”€ inference.py                   # æ¨ç†ç«¯ç‚¹
â”‚   â”œâ”€â”€ model_management.py            # æ¨¡å‹ç®¡ç†
â”‚   â””â”€â”€ health_check.py                # å¥åº·æ£€æŸ¥
â”œâ”€â”€ configs/                           # é…ç½®æ–‡ä»¶
â”‚   â”œâ”€â”€ serving_config.yaml            # æœåŠ¡é…ç½®
â”‚   â”œâ”€â”€ model_registry.json            # æ¨¡å‹æ³¨å†Œè¡¨
â”‚   â””â”€â”€ deployment_config.yaml         # éƒ¨ç½²é…ç½®
â”œâ”€â”€ models/                            # æ¨¡å‹æ–‡ä»¶
â”‚   â”œâ”€â”€ registry/                      # æ¨¡å‹æ³¨å†Œè¡¨
â”‚   â”œâ”€â”€ versions/                      # ç‰ˆæœ¬ç®¡ç†
â”‚   â””â”€â”€ artifacts/                     # æ¨¡å‹åˆ¶å“
â”œâ”€â”€ serving/                           # æ¨ç†æœåŠ¡
â”‚   â”œâ”€â”€ sync_inference.py              # åŒæ­¥æ¨ç†
â”‚   â”œâ”€â”€ async_inference.py             # å¼‚æ­¥æ¨ç†
â”‚   â”œâ”€â”€ batch_inference.py             # æ‰¹é‡æ¨ç†
â”‚   â””â”€â”€ streaming_inference.py         # æµå¼æ¨ç†
â”œâ”€â”€ deployment/                        # éƒ¨ç½²é…ç½®
â”‚   â”œâ”€â”€ docker/                        # Dockeré…ç½®
â”‚   â”‚   â”œâ”€â”€ Dockerfile                 # æœåŠ¡é•œåƒ
â”‚   â”‚   â””â”€â”€ docker-compose.yml         # Composeé…ç½®
â”‚   â”œâ”€â”€ kubernetes/                    # Kubernetesé…ç½®
â”‚   â”‚   â”œâ”€â”€ deployment.yaml            # éƒ¨ç½²å®šä¹‰
â”‚   â”‚   â”œâ”€â”€ service.yaml               # æœåŠ¡å®šä¹‰
â”‚   â”‚   â””â”€â”€ hpa.yaml                   # è‡ªåŠ¨ä¼¸ç¼©
â”‚   â””â”€â”€ helm/                          # Helm Chart
â”œâ”€â”€ monitoring/                        # ç›‘æ§é…ç½®
â”‚   â”œâ”€â”€ metrics.py                     # æŒ‡æ ‡æ”¶é›†
â”‚   â”œâ”€â”€ prometheus.yml                 # Prometheusé…ç½®
â”‚   â””â”€â”€ grafana_dashboards/            # Grafanaä»ªè¡¨æ¿
â”œâ”€â”€ tests/                             # æµ‹è¯•æ–‡ä»¶
â”‚   â”œâ”€â”€ integration_tests.py           # é›†æˆæµ‹è¯•
â”‚   â”œâ”€â”€ performance_tests.py           # æ€§èƒ½æµ‹è¯•
â”‚   â””â”€â”€ load_tests.py                  # è´Ÿè½½æµ‹è¯•
â””â”€â”€ notebooks/                         # Jupyterç¬”è®°æœ¬
    â”œâ”€â”€ 01_model_serving_patterns.ipynb  # æœåŠ¡æ¨¡å¼
    â”œâ”€â”€ 02_batch_vs_realtime.ipynb       # æ‰¹é‡vså®æ—¶
    â””â”€â”€ 03_scalability_testing.ipynb     # å¯æ‰©å±•æ€§æµ‹è¯•
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### æ­¥éª¤1ï¼šç¯å¢ƒè®¾ç½®

```bash
# å®‰è£…æ¨¡å‹æœåŠ¡ç›¸å…³ä¾èµ–
pip install -r requirements.txt

# å¯åŠ¨Redisï¼ˆç”¨äºå¼‚æ­¥ä»»åŠ¡ï¼‰
docker run -d --name redis -p 6379:6379 redis:alpine
```

### æ­¥éª¤2ï¼šå¯åŠ¨æ¨¡å‹æœåŠ¡

```bash
# å¯åŠ¨APIæœåŠ¡
uvicorn api.main:app --host 0.0.0.0 --port 8000 --reload

# æˆ–ä½¿ç”¨Gunicornè¿›è¡Œç”Ÿäº§éƒ¨ç½²
gunicorn api.main:app --workers 4 --worker-class uvicorn.workers.UvicornWorker --bind 0.0.0.0:8000
```

### æ­¥éª¤3ï¼šéƒ¨ç½²åˆ°Kubernetes

```bash
# æ„å»ºDockeré•œåƒ
docker build -t model-serving-service:latest .

# éƒ¨ç½²åˆ°Kubernetes
kubectl apply -f deployment/kubernetes/deployment.yaml
kubectl apply -f deployment/kubernetes/service.yaml
kubectl apply -f deployment/kubernetes/hpa.yaml
```

## ğŸ” ä»£ç è¯¦è§£

### æ ¸å¿ƒæ¦‚å¿µè§£æ

#### 1. æ¨¡å‹æœåŠ¡APIå®ç°
```python
# api/main.py
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List, Optional
import torch
import pickle
import time
from prometheus_client import Counter, Histogram

# å®šä¹‰æŒ‡æ ‡
inference_requests_total = Counter('inference_requests_total', 'Total inference requests', ['model_name'])
inference_duration = Histogram('inference_duration_seconds', 'Time spent on inference')

app = FastAPI(title="Model Serving API", version="1.0.0")

class PredictionRequest(BaseModel):
    inputs: List[float]
    model_version: Optional[str] = "latest"

class PredictionResponse(BaseModel):
    prediction: float
    model_version: str
    processing_time: float

@app.post("/predict", response_model=PredictionResponse)
async def predict(request: PredictionRequest):
    """æ¨¡å‹æ¨ç†ç«¯ç‚¹"""
    start_time = time.time()
    
    try:
        # è·å–æ¨¡å‹ï¼ˆç®€åŒ–å®ç°ï¼‰
        model = load_model(request.model_version)
        
        # æ‰§è¡Œæ¨ç†
        inputs_tensor = torch.tensor(request.inputs).unsqueeze(0)
        with torch.no_grad():
            prediction = model(inputs_tensor).item()
        
        processing_time = time.time() - start_time
        
        # è®°å½•æŒ‡æ ‡
        inference_requests_total.labels(model_name=request.model_version).inc()
        inference_duration.observe(processing_time)
        
        return PredictionResponse(
            prediction=prediction,
            model_version=request.model_version,
            processing_time=processing_time
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

def load_model(version: str):
    """åŠ è½½æŒ‡å®šç‰ˆæœ¬çš„æ¨¡å‹"""
    # è¿™é‡Œåº”è¯¥æ˜¯ä»æ¨¡å‹æ³¨å†Œè¡¨åŠ è½½æ¨¡å‹çš„é€»è¾‘
    model_path = f"models/versions/{version}/model.pkl"
    with open(model_path, 'rb') as f:
        return pickle.load(f)
```

#### 2. å®é™…åº”ç”¨ç¤ºä¾‹

##### åœºæ™¯1ï¼šæ‰¹é‡æ¨ç†æœåŠ¡
```python
# serving/batch_inference.py
from celery import Celery
from typing import List
import numpy as np

# é…ç½®Celery
celery_app = Celery('model_serving', broker='redis://localhost:6379')

@celery_app.task
def batch_predict(model_version: str, inputs: List[List[float]]) -> List[float]:
    """æ‰¹é‡æ¨ç†ä»»åŠ¡"""
    # åŠ è½½æ¨¡å‹
    model = load_model(model_version)
    
    # è½¬æ¢ä¸ºtensor
    inputs_tensor = torch.tensor(inputs, dtype=torch.float32)
    
    # æ‰¹é‡æ¨ç†
    with torch.no_grad():
        predictions = model(inputs_tensor)
    
    return predictions.numpy().tolist()

class BatchInferenceService:
    def __init__(self):
        self.batch_size = 32
        self.batch_buffer = []
        self.processing_queue = []
    
    def add_to_batch(self, inputs: List[float], callback_url: str = None):
        """æ·»åŠ åˆ°æ‰¹å¤„ç†é˜Ÿåˆ—"""
        self.batch_buffer.append({
            'inputs': inputs,
            'callback_url': callback_url,
            'timestamp': time.time()
        })
        
        # å¦‚æœè¾¾åˆ°æ‰¹å¤„ç†å¤§å°ï¼Œæäº¤æ‰¹å¤„ç†
        if len(self.batch_buffer) >= self.batch_size:
            self.submit_batch()
    
    def submit_batch(self):
        """æäº¤æ‰¹å¤„ç†ä»»åŠ¡"""
        if not self.batch_buffer:
            return
        
        batch_inputs = [item['inputs'] for item in self.batch_buffer]
        
        # å¼‚æ­¥æ‰§è¡Œæ‰¹å¤„ç†
        task = batch_predict.delay('latest', batch_inputs)
        
        # è®°å½•æ‰¹å¤„ç†ä¿¡æ¯
        batch_info = {
            'task_id': task.id,
            'size': len(batch_inputs),
            'submitted_at': time.time(),
            'callbacks': [item['callback_url'] for item in self.batch_buffer]
        }
        
        self.processing_queue.append(batch_info)
        self.batch_buffer.clear()
```

##### åœºæ™¯2ï¼šæ¨¡å‹ç‰ˆæœ¬ç®¡ç†å’ŒA/Bæµ‹è¯•
```python
# api/model_management.py
from enum import Enum
from typing import Dict, Any
import threading

class ModelStatus(Enum):
    LOADING = "loading"
    LOADED = "loaded"
    SERVING = "serving"
    FAILED = "failed"

class ModelRegistry:
    def __init__(self):
        self.models: Dict[str, Dict[str, Any]] = {}
        self.lock = threading.RLock()
    
    def register_model(self, name: str, version: str, path: str, metadata: Dict = None):
        """æ³¨å†Œæ¨¡å‹"""
        with self.lock:
            if name not in self.models:
                self.models[name] = {}
            
            self.models[name][version] = {
                'path': path,
                'status': ModelStatus.LOADING,
                'metadata': metadata or {},
                'created_at': time.time(),
                'model_instance': None
            }
    
    def load_model(self, name: str, version: str = 'latest'):
        """åŠ è½½æ¨¡å‹åˆ°å†…å­˜"""
        with self.lock:
            if name not in self.models:
                raise ValueError(f"Model {name} not found")
            
            if version == 'latest':
                # è·å–æœ€æ–°ç‰ˆæœ¬
                versions = list(self.models[name].keys())
                version = sorted(versions, reverse=True)[0]
            
            if version not in self.models[name]:
                raise ValueError(f"Version {version} of model {name} not found")
            
            model_info = self.models[name][version]
            
            if model_info['status'] == ModelStatus.LOADED:
                return model_info['model_instance']
            
            # åŠ è½½æ¨¡å‹
            try:
                with open(model_info['path'], 'rb') as f:
                    model_instance = pickle.load(f)
                
                model_info['model_instance'] = model_instance
                model_info['status'] = ModelStatus.LOADED
                
                return model_instance
            except Exception as e:
                model_info['status'] = ModelStatus.FAILED
                raise e

class ABTestRouter:
    def __init__(self, registry: ModelRegistry):
        self.registry = registry
        self.traffic_split = {'model_v1': 0.5, 'model_v2': 0.5}  # 50/50åˆ†æµ
    
    def route_request(self, request_data: Any) -> str:
        """æ ¹æ®A/Bæµ‹è¯•é…ç½®è·¯ç”±è¯·æ±‚"""
        import random
        
        rand_val = random.random()
        cumulative_prob = 0.0
        
        for model_name, prob in self.traffic_split.items():
            cumulative_prob += prob
            if rand_val <= cumulative_prob:
                return model_name
        
        # é»˜è®¤è¿”å›ç¬¬ä¸€ä¸ªæ¨¡å‹
        return list(self.traffic_split.keys())[0]
```

## ğŸ§ª éªŒè¯æµ‹è¯•

### æµ‹è¯•1ï¼šæ¨¡å‹æœåŠ¡åŠŸèƒ½éªŒè¯
```python
#!/usr/bin/env python
# éªŒè¯æ¨¡å‹æœåŠ¡åŠŸèƒ½
import asyncio
import aiohttp
import time
import numpy as np

async def test_model_serving():
    print("=== æ¨¡å‹æœåŠ¡åŠŸèƒ½éªŒè¯ ===")
    
    # æ¨¡æ‹Ÿæ¨¡å‹è¾“å…¥
    test_inputs = np.random.rand(10).tolist()
    
    # å‘é€æ¨ç†è¯·æ±‚
    async with aiohttp.ClientSession() as session:
        url = "http://localhost:8000/predict"
        payload = {
            "inputs": test_inputs,
            "model_version": "latest"
        }
        
        try:
            start_time = time.time()
            async with session.post(url, json=payload) as response:
                result = await response.json()
                elapsed_time = time.time() - start_time
            
            print(f"âœ… æ¨ç†è¯·æ±‚æˆåŠŸ")
            print(f"è¾“å…¥: {test_inputs[:3]}... (å…±{len(test_inputs)}ä¸ªç‰¹å¾)")
            print(f"é¢„æµ‹ç»“æœ: {result['prediction']}")
            print(f"æ¨¡å‹ç‰ˆæœ¬: {result['model_version']}")
            print(f"å¤„ç†æ—¶é—´: {elapsed_time:.4f}s")
            
        except Exception as e:
            print(f"âŒ æ¨ç†è¯·æ±‚å¤±è´¥: {e}")
            print("è¯·ç¡®ä¿æ¨¡å‹æœåŠ¡æ­£åœ¨è¿è¡Œ (uvicorn api.main:app --host 0.0.0.0 --port 8000)")

if __name__ == "__main__":
    asyncio.run(test_model_serving())
```

### æµ‹è¯•2ï¼šæ‰¹é‡æ¨ç†æ€§èƒ½éªŒè¯
```python
#!/usr/bin/env python
# éªŒè¯æ‰¹é‡æ¨ç†æ€§èƒ½
import time
import numpy as np
from concurrent.futures import ThreadPoolExecutor
import asyncio

def simulate_single_inference(inputs):
    """æ¨¡æ‹Ÿå•æ¬¡æ¨ç†"""
    # æ¨¡æ‹Ÿæ¨ç†å»¶è¿Ÿ
    time.sleep(0.01)
    return sum(inputs) / len(inputs)  # ç®€å•çš„è®¡ç®—

def test_batch_vs_single_performance():
    print("=== æ‰¹é‡vså•æ¬¡æ¨ç†æ€§èƒ½éªŒè¯ ===")
    
    # ç”Ÿæˆæµ‹è¯•æ•°æ®
    num_requests = 100
    test_inputs = [np.random.rand(10).tolist() for _ in range(num_requests)]
    
    # æµ‹è¯•å•æ¬¡æ¨ç†æ€§èƒ½
    start_time = time.time()
    single_results = []
    for inputs in test_inputs:
        result = simulate_single_inference(inputs)
        single_results.append(result)
    single_time = time.time() - start_time
    
    # æµ‹è¯•æ‰¹é‡æ¨ç†æ€§èƒ½ï¼ˆæ¨¡æ‹Ÿï¼‰
    start_time = time.time()
    # æ¨¡æ‹Ÿæ‰¹é‡å¤„ç† - ä¸€æ¬¡å¤„ç†æ‰€æœ‰è¾“å…¥
    batch_result = [sum(inputs) / len(inputs) for inputs in test_inputs]
    batch_time = time.time() - start_time
    
    print(f"âœ… æ€§èƒ½å¯¹æ¯”ç»“æœ:")
    print(f"å•æ¬¡æ¨ç†æ€»æ—¶é—´: {single_time:.4f}s")
    print(f"æ‰¹é‡æ¨ç†æ€»æ—¶é—´: {batch_time:.4f}s")
    print(f"æ€§èƒ½æå‡å€æ•°: {single_time/batch_time:.2f}x")
    print(f"å¤„ç†è¯·æ±‚æ•°é‡: {num_requests}")
    
    # éªŒè¯ç»“æœä¸€è‡´æ€§
    results_match = all(abs(a - b) < 1e-10 for a, b in zip(single_results, batch_result))
    print(f"ç»“æœä¸€è‡´æ€§: {'âœ…' if results_match else 'âŒ'}")

if __name__ == "__main__":
    test_batch_vs_single_performance()
```

## â“ å¸¸è§é—®é¢˜

### Q1: å¦‚ä½•å¤„ç†æ¨¡å‹æœåŠ¡çš„é«˜å¹¶å‘è¯·æ±‚ï¼Ÿ
**è§£å†³æ–¹æ¡ˆ**ï¼š
```python
# é«˜å¹¶å‘å¤„ç†ç­–ç•¥
"""
1. å¼‚æ­¥å¤„ç†: ä½¿ç”¨FastAPIå’Œasync/await
2. è¿æ¥æ± : é…ç½®æ•°æ®åº“å’Œå¤–éƒ¨æœåŠ¡è¿æ¥æ± 
3. ç¼“å­˜: ä½¿ç”¨Redisç¼“å­˜é¢‘ç¹è¯·æ±‚çš„ç»“æœ
4. è´Ÿè½½å‡è¡¡: ä½¿ç”¨å¤šå®ä¾‹å’Œè´Ÿè½½å‡è¡¡å™¨
"""
```

### Q2: å¦‚ä½•å®ç°æ¨¡å‹çš„é›¶åœæœºæ›´æ–°ï¼Ÿ
**è§£å†³æ–¹æ¡ˆ**ï¼š
```python
# é›¶åœæœºæ›´æ–°ç­–ç•¥
"""
1. è“ç»¿éƒ¨ç½²: ç»´æŠ¤ä¸¤å¥—ç¯å¢ƒäº¤æ›¿æ›´æ–°
2. æ»šåŠ¨æ›´æ–°: é€æ­¥æ›¿æ¢æ—§å®ä¾‹
3. A/Bæµ‹è¯•: é€æ­¥è½¬ç§»æµé‡
4. å¥åº·æ£€æŸ¥: ç¡®ä¿æ–°ç‰ˆæœ¬æ­£å¸¸å·¥ä½œ
"""
```

## ğŸ“š æ‰©å±•å­¦ä¹ 

### ç›¸å…³æŠ€æœ¯
- **TorchServe**: PyTorchæ¨¡å‹æœåŠ¡
- **TF-Serving**: TensorFlowæ¨¡å‹æœåŠ¡
- **Seldon**: KubernetesåŸç”Ÿæ¨¡å‹æœåŠ¡
- **KServe**: Kubeflowæ¨¡å‹æœåŠ¡

### è¿›é˜¶å­¦ä¹ è·¯å¾„
1. æŒæ¡ä¸åŒæ¨¡å‹æœåŠ¡æ¡†æ¶çš„ç‰¹ç‚¹
2. å­¦ä¹ å®¹å™¨åŒ–éƒ¨ç½²æœ€ä½³å®è·µ
3. ç†è§£æœåŠ¡ç½‘æ ¼åœ¨æ¨¡å‹éƒ¨ç½²ä¸­çš„åº”ç”¨
4. æŒæ¡è‡ªåŠ¨æ‰©ç¼©å®¹å’Œå¼¹æ€§ä¼¸ç¼©

### ä¼ä¸šçº§åº”ç”¨åœºæ™¯
- é«˜å¯ç”¨æ¨¡å‹æœåŠ¡æ¶æ„
- å¤šæ¨¡å‹ç‰ˆæœ¬ç®¡ç†
- A/Bæµ‹è¯•å’Œå®éªŒå¹³å°
- æ¨¡å‹æ€§èƒ½ç›‘æ§å’Œå‘Šè­¦

---
> **ğŸ’¡ æç¤º**: AIæ¨¡å‹éƒ¨ç½²æ˜¯è¿æ¥æ¨¡å‹å¼€å‘å’Œå®é™…åº”ç”¨çš„å…³é”®ç¯èŠ‚ï¼Œéœ€è¦ç»¼åˆè€ƒè™‘æ€§èƒ½ã€å¯æ‰©å±•æ€§ã€å¯é æ€§å’Œç»´æŠ¤æ€§ç­‰å¤šä¸ªæ–¹é¢ï¼Œæ˜¯ç°ä»£AIç³»ç»Ÿä¸å¯æˆ–ç¼ºçš„ç»„æˆéƒ¨åˆ†ã€‚