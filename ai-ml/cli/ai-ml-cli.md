# ğŸ¤– AI/ML å‘½ä»¤è¡Œé€ŸæŸ¥è¡¨ (ai-ml-cli.md)

> AI/MLå¼€å‘å’Œéƒ¨ç½²å¿…å¤‡çš„å‘½ä»¤è¡Œå‚è€ƒæ‰‹å†Œï¼Œæ¶µç›–æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ã€å¤§æ¨¡å‹è®­ç»ƒç­‰æ ¸å¿ƒå·¥å…·é“¾ï¼ŒæŒ‰åŠŸèƒ½åˆ†ç±»æ•´ç†ï¼Œæ–¹ä¾¿å¿«é€ŸæŸ¥æ‰¾å’Œä½¿ç”¨

---

## ğŸ“‹ ç›®å½•ç´¢å¼•

- [Python MLç¯å¢ƒ](#python-mlç¯å¢ƒ)
- [æ·±åº¦å­¦ä¹ æ¡†æ¶](#æ·±åº¦å­¦ä¹ æ¡†æ¶)
- [AutoMLå·¥å…·](#automlå·¥å…·)
- [å¤§æ¨¡å‹è®­ç»ƒ](#å¤§æ¨¡å‹è®­ç»ƒ)
- [æ•°æ®å¤„ç†](#æ•°æ®å¤„ç†)
- [æ¨¡å‹éƒ¨ç½²](#æ¨¡å‹éƒ¨ç½²)
- [å®éªŒè·Ÿè¸ª](#å®éªŒè·Ÿè¸ª)
- [æ€§èƒ½ç›‘æ§](#æ€§èƒ½ç›‘æ§)
- [åˆ†å¸ƒå¼è®­ç»ƒ](#åˆ†å¸ƒå¼è®­ç»ƒ)
- [æ¨¡å‹ä¼˜åŒ–](#æ¨¡å‹ä¼˜åŒ–)
- [æ¨ç†æœåŠ¡](#æ¨ç†æœåŠ¡)
- [å¯è§†åŒ–åˆ†æ](#å¯è§†åŒ–åˆ†æ)
- [æµ‹è¯•éªŒè¯](#æµ‹è¯•éªŒè¯)
- [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)

---

## Python MLç¯å¢ƒ

### ç¯å¢ƒç®¡ç†
```bash
# åˆ›å»ºMLä¸“ç”¨ç¯å¢ƒ
conda create -n ml-env python=3.9
conda activate ml-env

# å®‰è£…åŸºç¡€ç§‘å­¦è®¡ç®—åº“
pip install numpy scipy pandas matplotlib seaborn

# å®‰è£…Jupyterç¯å¢ƒ
pip install jupyter jupyterlab ipywidgets
jupyter notebook --ip=0.0.0.0 --port=8888 --no-browser

# è™šæ‹Ÿç¯å¢ƒç®¡ç†
python -m venv ml_project
source ml_project/bin/activate  # Linux/Mac
ml_project\Scripts\activate     # Windows
```

### æ ¸å¿ƒMLåº“å®‰è£…
```bash
# Scikit-learnç”Ÿæ€ç³»ç»Ÿ
pip install scikit-learn scikit-optimize imbalanced-learn

# æ•°æ®å¤„ç†å’Œç‰¹å¾å·¥ç¨‹
pip install pandas numpy polars dask featuretools

# ç»Ÿè®¡åˆ†æ
pip install statsmodels pingouin

# æ—¶é—´åºåˆ—åˆ†æ
pip install prophet pmdarima tsfresh

# å¼ºåŒ–å­¦ä¹ 
pip install gymnasium stable-baselines3 ray[rllib]
```

### GPUç¯å¢ƒé…ç½®
```bash
# CUDAç¯å¢ƒæ£€æŸ¥
nvidia-smi
nvcc --version

# PyTorch GPUç‰ˆæœ¬
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# TensorFlow GPUç‰ˆæœ¬
pip install tensorflow[and-cuda]

# JAX GPUæ”¯æŒ
pip install "jax[cuda11_pip]" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html
```

---

## æ·±åº¦å­¦ä¹ æ¡†æ¶

### PyTorchç”Ÿæ€
```bash
# æ ¸å¿ƒPyTorchå®‰è£…
pip install torch torchvision torchaudio

# PyTorchæ‰©å±•åº“
pip install torch-geometric  # å›¾ç¥ç»ç½‘ç»œ
pip install pytorch-lightning  # è®­ç»ƒæ¡†æ¶
pip install torchmetrics  # æŒ‡æ ‡è®¡ç®—
pip install torchsummary  # æ¨¡å‹æ‘˜è¦

# è®¡ç®—æœºè§†è§‰
pip install torchvision timm efficientnet-pytorch

# è‡ªç„¶è¯­è¨€å¤„ç†
pip install transformers datasets tokenizers

# ä¼˜åŒ–åŠ é€Ÿ
pip install apex  # NVIDIA Apexæ··åˆç²¾åº¦è®­ç»ƒ
pip install deepspeed  # å¾®è½¯DeepSpeed
pip install flash-attn  # é—ªå­˜æ³¨æ„åŠ›ä¼˜åŒ–
```

### TensorFlow/Kerasç”Ÿæ€
```bash
# TensorFlowæ ¸å¿ƒ
pip install tensorflow

# Kerasæ‰©å±•
pip install keras-tuner  # è¶…å‚æ•°è°ƒä¼˜
pip install tf-agents  # å¼ºåŒ–å­¦ä¹ 
pip install tensorflow-probability  # æ¦‚ç‡å»ºæ¨¡

# TensorFlowæ‰©å±•
pip install tensorflow-addons
pip install tensorflow-datasets
pip install tensorflow-model-optimization

# TensorBoardå·¥å…·
pip install tensorboard tensorboard-plugin-profile
tensorboard --logdir=./logs --bind_all
```

### å…¶ä»–æ·±åº¦å­¦ä¹ æ¡†æ¶
```bash
# JAXç”Ÿæ€
pip install jax jaxlib flax optax

# MXNet
pip install mxnet-cu112

# PaddlePaddle
pip install paddlepaddle-gpu

# MindSpore
pip install mindspore-gpu
```

---

## AutoMLå·¥å…·

### AutoMLæ¡†æ¶
```bash
# AutoGluon
pip install autogluon
python -c "from autogluon.tabular import TabularPredictor; print('AutoGluon ready')"

# H2O.ai
pip install h2o
h2o.init()

# TPOT (é—ä¼ ç¼–ç¨‹)
pip install tpot
python -c "from tpot import TPOTClassifier; print('TPOT ready')"

# Auto-sklearn
pip install auto-sklearn

# MLBox
pip install mlbox
```

### è¶…å‚æ•°ä¼˜åŒ–
```bash
# Optuna
pip install optuna
python -c "import optuna; print('Optuna ready')"

# Ray Tune
pip install ray[tune]
ray start --head

# Hyperopt
pip install hyperopt

# Scikit-Optimize
pip install scikit-optimize

# Ax Platform (Facebook)
pip install ax-platform
```

### ç¥ç»æ¶æ„æœç´¢
```bash
# NASLib
pip install naslib

# DARTS
pip install darts

# EfficientNetæœç´¢
pip install efficientnet-pytorch
```

---

## å¤§æ¨¡å‹è®­ç»ƒ

### LLMè®­ç»ƒæ¡†æ¶
```bash
# Hugging Face Transformers
pip install transformers accelerate datasets peft

# å¤§æ¨¡å‹è®­ç»ƒå·¥å…·
pip install deepspeed  # å¾®è½¯åˆ†å¸ƒå¼è®­ç»ƒ
pip install megatron-lm  # NVIDIAå¤§æ¨¡å‹è®­ç»ƒ
pip install alpa  # Googleè‡ªåŠ¨å¹¶è¡ŒåŒ–

# æ¨¡å‹é‡åŒ–å’Œå‹ç¼©
pip install bitsandbytes  # 4-bité‡åŒ–
pip install auto-gptq  # GPTQé‡åŒ–
```

### è®­ç»ƒè„šæœ¬ç¤ºä¾‹
```bash
# å•æœºå¤šå¡è®­ç»ƒ
torchrun --nproc_per_node=4 train_script.py

# åˆ†å¸ƒå¼è®­ç»ƒå¯åŠ¨
deepspeed --num_gpus=8 train.py --deepspeed ds_config.json

# æ··åˆç²¾åº¦è®­ç»ƒ
python train.py --fp16 --gradient_checkpointing

# æ¢¯åº¦ç´¯ç§¯
python train.py --gradient_accumulation_steps=4
```

### è®­ç»ƒç›‘æ§
```bash
# Weights & Biases
pip install wandb
wandb login YOUR_API_KEY

# MLflow
pip install mlflow
mlflow ui --host 0.0.0.0 --port 5000

# TensorBoard
tensorboard --logdir=./runs --bind_all
```

---

## æ•°æ®å¤„ç†

### æ•°æ®åŠ è½½å’Œé¢„å¤„ç†
```python
# PyTorchæ•°æ®å¤„ç†
import torch
from torch.utils.data import DataLoader, Dataset
from torchvision import transforms

# TensorFlowæ•°æ®å¤„ç†
import tensorflow as tf
from tensorflow.data import Dataset

# HuggingFaceæ•°æ®é›†
from datasets import load_dataset
dataset = load_dataset('glue', 'mrpc')

# Pandasæ•°æ®å¤„ç†
import pandas as pd
df = pd.read_csv('data.csv')
```

### ç‰¹å¾å·¥ç¨‹
```bash
# FeatureToolsè‡ªåŠ¨åŒ–ç‰¹å¾å·¥ç¨‹
pip install featuretools

# æ•°æ®æ¸…æ´—
pip install pandas-profiling

# ç±»åˆ«ç‰¹å¾ç¼–ç 
pip install category_encoders

# ç‰¹å¾é€‰æ‹©
pip install scikit-feature
```

### æ•°æ®å¢å¼º
```bash
# å›¾åƒå¢å¼º
pip install albumentations imgaug

# æ–‡æœ¬å¢å¼º
pip install nlpaug

# éŸ³é¢‘å¢å¼º
pip install audiomentations
```

---

## æ¨¡å‹éƒ¨ç½²

### æ¨¡å‹è½¬æ¢å’Œä¼˜åŒ–
```bash
# ONNXè½¬æ¢
pip install onnx onnxruntime
python -m tf2onnx.convert --saved-model ./model --output model.onnx

# TensorRTä¼˜åŒ–
pip install tensorrt

# OpenVINOä¼˜åŒ–
pip install openvino

# TorchScriptç¼–è¯‘
torch.jit.trace(model, example_inputs)
```

### æ¨ç†æœåŠ¡æ¡†æ¶
```bash
# FastAPIéƒ¨ç½²
pip install fastapi uvicorn
uvicorn main:app --host 0.0.0.0 --port 8000

# Flaskéƒ¨ç½²
pip install flask flask-restful

# Triton Inference Server
docker run --gpus=1 --rm -p 8000:8000 -p 8001:8001 -p 8002:8002 nvcr.io/nvidia/tritonserver:23.01-py3 tritonserver --model-repository=/models

# TorchServe
torch-model-archiver --model-name my_model --version 1.0 --model-file model.py --serialized-file model.pth --handler image_classifier
torchserve --start --model-store model_store
```

### å®¹å™¨åŒ–éƒ¨ç½²
```bash
# Dockeræ„å»º
docker build -t ml-model:v1 .

# Kuberneteséƒ¨ç½²
kubectl apply -f model-deployment.yaml

# Helm Chart
helm install ml-model ./ml-model-chart
```

---

## å®éªŒè·Ÿè¸ª

### Weights & Biases
```bash
# åˆå§‹åŒ–é¡¹ç›®
wandb init --project my-project

# è®°å½•æŒ‡æ ‡
import wandb
wandb.log({"accuracy": 0.95, "loss": 0.1})

# é…ç½®è·Ÿè¸ª
wandb.config.update({"learning_rate": 0.001, "batch_size": 32})
```

### MLflow
```bash
# å¯åŠ¨MLflowæœåŠ¡å™¨
mlflow server --host 0.0.0.0 --port 5000

# Python APIä½¿ç”¨
import mlflow
mlflow.start_run()
mlflow.log_param("learning_rate", 0.01)
mlflow.log_metric("accuracy", 0.95)
mlflow.end_run()
```

### Neptune.ai
```bash
pip install neptune
neptune.init(project="your-project")
```

---

## æ€§èƒ½ç›‘æ§

### GPUç›‘æ§
```bash
# å®æ—¶GPUç›‘æ§
watch -n 1 nvidia-smi

# GPUè¯¦ç»†ä¿¡æ¯
nvidia-smi -q -d MEMORY,UTILIZATION,POWER,CLOCK,COMPUTE

# è¿›ç¨‹GPUä½¿ç”¨
nvidia-smi pmon

# GPUæ‹“æ‰‘æŸ¥çœ‹
nvidia-smi topo -m
```

### ç³»ç»Ÿæ€§èƒ½ç›‘æ§
```bash
# ç³»ç»Ÿèµ„æºç›‘æ§
htop
iotop
nethogs

# å†…å­˜åˆ†æ
python -m memory_profiler script.py

# CPUæ€§èƒ½åˆ†æ
perf record -g python script.py
perf report

# I/Oç›‘æ§
iostat -x 1
```

### è®­ç»ƒæ€§èƒ½åˆ†æ
```bash
# PyTorch Profiler
from torch.profiler import profile, record_function, ProfilerActivity
with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA]) as prof:
    # è®­ç»ƒä»£ç 
    pass
prof.export_chrome_trace("trace.json")

# TensorFlow Profiler
tf.profiler.experimental.start('logdir')
# è®­ç»ƒä»£ç 
tf.profiler.experimental.stop()
```

---

## åˆ†å¸ƒå¼è®­ç»ƒ

### PyTorchåˆ†å¸ƒå¼
```bash
# å•æœºå¤šå¡
torchrun --nproc_per_node=4 train.py

# å¤šæœºå¤šå¡
torchrun --nnodes=2 --nproc_per_node=4 --master_addr="192.168.1.1" --master_port=1234 train.py

# åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ
import torch.distributed as dist
dist.init_process_group(backend='nccl', init_method='env://')
```

### DeepSpeedé…ç½®
```json
{
    "train_batch_size": 32,
    "gradient_accumulation_steps": 1,
    "optimizer": {
        "type": "Adam",
        "params": {
            "lr": 0.001
        }
    },
    "fp16": {
        "enabled": true
    },
    "zero_optimization": {
        "stage": 2
    }
}
```

### Horovod
```bash
pip install horovod[pytorch]
horovodrun -np 4 -H localhost:4 python train.py
```

---

## æ¨¡å‹ä¼˜åŒ–

### æ¨¡å‹é‡åŒ–
```python
# PyTorché‡åŒ–
import torch.quantization as quantization
model_quantized = quantization.quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)

# TensorFlow Liteé‡åŒ–
converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
tflite_quant_model = converter.convert()
```

### æ¨¡å‹å‰ªæ
```python
# PyTorchå‰ªæ
import torch.nn.utils.prune as prune
prune.l1_unstructured(module, name="weight", amount=0.3)

# TensorFlow Model Optimization
import tensorflow_model_optimization as tfmot
prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude
```

### çŸ¥è¯†è’¸é¦
```python
# æ•™å¸ˆ-å­¦ç”Ÿç½‘ç»œè®­ç»ƒ
teacher_model = TeacherModel()
student_model = StudentModel()

# è’¸é¦æŸå¤±å‡½æ•°
def distillation_loss(student_logits, teacher_logits, labels, temperature=3.0):
    soft_targets = F.softmax(teacher_logits / temperature, dim=-1)
    soft_prob = F.log_softmax(student_logits / temperature, dim=-1)
    distill_loss = F.kl_div(soft_prob, soft_targets, reduction='batchmean')
    return distill_loss
```

---

## æ¨ç†æœåŠ¡

### æ¨¡å‹æœåŠ¡åŒ–
```bash
# TorchServeéƒ¨ç½²
torch-model-archiver --model-name resnet50 --version 1.0 \
    --model-file model.py --serialized-file resnet50.pth \
    --handler image_classifier --extra-files index_to_name.json

torchserve --start --model-store model_store --models resnet50=resnet50.mar

# è¯·æ±‚ç¤ºä¾‹
curl -X POST http://localhost:8080/predictions/resnet50 -T image.jpg
```

### APIæœåŠ¡æ„å»º
```python
# FastAPIç¤ºä¾‹
from fastapi import FastAPI, File, UploadFile
import torch
from PIL import Image

app = FastAPI()

@app.post("/predict/")
async def predict(file: UploadFile = File(...)):
    image = Image.open(file.file)
    # é¢„å¤„ç†å’Œæ¨ç†
    return {"prediction": result.tolist()}
```

### æ‰¹é‡æ¨ç†
```python
# æ‰¹é‡å¤„ç†è„šæœ¬
def batch_inference(model, data_loader, batch_size=32):
    results = []
    model.eval()
    with torch.no_grad():
        for batch in data_loader:
            outputs = model(batch)
            results.extend(outputs.cpu().numpy())
    return results
```

---

## å¯è§†åŒ–åˆ†æ

### è®­ç»ƒå¯è§†åŒ–
```python
# MatplotlibåŸºç¡€ç»˜å›¾
import matplotlib.pyplot as plt
plt.plot(epochs, train_loss, label='Training Loss')
plt.plot(epochs, val_loss, label='Validation Loss')
plt.legend()
plt.show()

# Seabornç¾åŒ–
import seaborn as sns
sns.lineplot(data=df, x='epoch', y='loss')

# Plotlyäº¤äº’å¼å›¾è¡¨
import plotly.graph_objects as go
fig = go.Figure()
fig.add_trace(go.Scatter(x=epochs, y=train_acc, name='Train Accuracy'))
```

### æ··æ·†çŸ©é˜µå’ŒROCæ›²çº¿
```python
# Scikit-learnè¯„ä¼°
from sklearn.metrics import confusion_matrix, roc_curve, auc
import seaborn as sns

# æ··æ·†çŸ©é˜µ
cm = confusion_matrix(y_true, y_pred)
sns.heatmap(cm, annot=True, fmt='d')

# ROCæ›²çº¿
fpr, tpr, _ = roc_curve(y_true, y_scores)
roc_auc = auc(fpr, tpr)
```

### ç‰¹å¾é‡è¦æ€§å¯è§†åŒ–
```python
# SHAPå€¼åˆ†æ
import shap
explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(X)
shap.summary_plot(shap_values, X)

# LIMEå±€éƒ¨è§£é‡Š
from lime import lime_tabular
explainer = lime_tabular.LimeTabularExplainer(training_data)
exp = explainer.explain_instance(instance, model.predict_proba)
```

---

## æµ‹è¯•éªŒè¯

### å•å…ƒæµ‹è¯•
```bash
# PyTeståŸºç¡€
pip install pytest pytest-cov
pytest test_model.py -v

# è¦†ç›–ç‡æŠ¥å‘Š
pytest --cov=model --cov-report=html

# å‚æ•°åŒ–æµ‹è¯•
@pytest.mark.parametrize("input,output", [(1, 2), (2, 4)])
def test_double(input, output):
    assert double(input) == output
```

### æ¨¡å‹éªŒè¯
```python
# äº¤å‰éªŒè¯
from sklearn.model_selection import cross_val_score
scores = cross_val_score(model, X, y, cv=5)

# å­¦ä¹ æ›²çº¿
from sklearn.model_selection import learning_curve
train_sizes, train_scores, val_scores = learning_curve(model, X, y)

# éªŒè¯æ›²çº¿
from sklearn.model_selection import validation_curve
param_range = [0.001, 0.01, 0.1, 1.0]
train_scores, val_scores = validation_curve(model, X, y, param_name="C", param_range=param_range)
```

### æ€§èƒ½åŸºå‡†æµ‹è¯•
```python
# æ—¶é—´æ€§èƒ½æµ‹è¯•
import time
start_time = time.time()
result = model.predict(X_test)
end_time = time.time()
print(f"Inference time: {end_time - start_time:.4f} seconds")

# å†…å­˜ä½¿ç”¨ç›‘æ§
import tracemalloc
tracemalloc.start()
# æ¨¡å‹æ¨ç†ä»£ç 
current, peak = tracemalloc.get_traced_memory()
print(f"Current memory usage: {current / 1024 / 1024:.1f} MB")
print(f"Peak memory usage: {peak / 1024 / 1024:.1f} MB")
```

---

## æœ€ä½³å®è·µ

### ä»£ç ç»„ç»‡è§„èŒƒ
```python
# é¡¹ç›®ç»“æ„ç¤ºä¾‹
project/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ utils/
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ configs/
â”œâ”€â”€ notebooks/
â”œâ”€â”€ tests/
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

# é…ç½®ç®¡ç†
import yaml
with open('config.yaml', 'r') as f:
    config = yaml.safe_load(f)
```

### ç‰ˆæœ¬æ§åˆ¶
```bash
# Git LFSå¤§æ–‡ä»¶ç®¡ç†
git lfs install
git lfs track "*.pth"
git add .gitattributes

# DVCæ•°æ®ç‰ˆæœ¬æ§åˆ¶
dvc init
dvc add data/train.csv
dvc push
```

### CI/CDæµæ°´çº¿
```yaml
# GitHub Actionsç¤ºä¾‹
name: ML Pipeline
on: [push]
jobs:
  test:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v2
    - name: Set up Python
      uses: actions/setup-python@v2
      with:
        python-version: 3.9
    - name: Install dependencies
      run: |
        pip install -r requirements.txt
    - name: Run tests
      run: |
        pytest tests/
```

### å®‰å…¨å’Œåˆè§„
```bash
# æ¨¡å‹å®‰å…¨æ£€æŸ¥
pip install modelscan  # æ¨¡å‹æ–‡ä»¶å®‰å…¨æ‰«æ

# æ•°æ®éšç§ä¿æŠ¤
pip install opacus  # å·®åˆ†éšç§è®­ç»ƒ

# æ¨¡å‹å…¬å¹³æ€§è¯„ä¼°
pip install aif360  # AIå…¬å¹³æ€§å·¥å…·åŒ…
```

---