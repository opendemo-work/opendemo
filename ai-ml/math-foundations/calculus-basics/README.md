# å¾®ç§¯åˆ†åŸºç¡€

## ğŸ¯ æ¡ˆä¾‹æ¦‚è¿°

è¿™æ˜¯ä¸€ä¸ªå…¨é¢å±•ç¤ºå¾®ç§¯åˆ†åœ¨AI/MLä¸­åº”ç”¨çš„åŸºç¡€ç¤ºä¾‹ï¼Œæ¶µç›–å¯¼æ•°ã€åå¯¼æ•°ã€æ¢¯åº¦ã€ç§¯åˆ†ã€é“¾å¼æ³•åˆ™ç­‰æ ¸å¿ƒæ•°å­¦æ¦‚å¿µã€‚

## ğŸ“š å­¦ä¹ ç›®æ ‡

é€šè¿‡æœ¬ç¤ºä¾‹ä½ å°†æŒæ¡ï¼š
- å¯¼æ•°çš„å‡ ä½•æ„ä¹‰å’Œè®¡ç®—æ–¹æ³•
- åå¯¼æ•°å’Œæ¢¯åº¦çš„æ¦‚å¿µä¸åº”ç”¨
- æ¢¯åº¦ä¸‹é™ç®—æ³•çš„åŸç†å’Œå®ç°
- æ•°å€¼ç§¯åˆ†çš„å„ç§æ–¹æ³•
- é“¾å¼æ³•åˆ™åœ¨ç¥ç»ç½‘ç»œä¸­çš„åº”ç”¨

## ğŸ”§ æ ¸å¿ƒçŸ¥è¯†ç‚¹

### 1. å¯¼æ•°æ¦‚å¿µ
- å¯¼æ•°çš„å‡ ä½•æ„ä¹‰ï¼ˆåˆ‡çº¿æ–œç‡ï¼‰
- åŸºæœ¬æ±‚å¯¼æ³•åˆ™
- é«˜é˜¶å¯¼æ•°
- å¯¼æ•°åœ¨ä¼˜åŒ–ä¸­çš„åº”ç”¨

### 2. åå¯¼æ•°å’Œæ¢¯åº¦
- å¤šå…ƒå‡½æ•°çš„åå¯¼æ•°
- æ¢¯åº¦å‘é‡çš„è®¡ç®—
- æ¢¯åº¦çš„å‡ ä½•æ„ä¹‰
- æ–¹å‘å¯¼æ•°

### 3. æ¢¯åº¦ä¸‹é™
- æ¢¯åº¦ä¸‹é™ç®—æ³•åŸç†
- å­¦ä¹ ç‡çš„é€‰æ‹©
- æ”¶æ•›æ€§åˆ†æ
- ä¼˜åŒ–ç®—æ³•å˜ç§

### 4. ç§¯åˆ†æ–¹æ³•
- å®šç§¯åˆ†çš„å‡ ä½•æ„ä¹‰
- æ•°å€¼ç§¯åˆ†æ–¹æ³•ï¼ˆæ¢¯å½¢æ³•åˆ™ã€Simpsonæ³•åˆ™ï¼‰
- Monte Carloç§¯åˆ†
- ç§¯åˆ†åœ¨æ¦‚ç‡ä¸­çš„åº”ç”¨

### 5. é“¾å¼æ³•åˆ™
- å¤åˆå‡½æ•°æ±‚å¯¼
- åå‘ä¼ æ’­ç®—æ³•åŸºç¡€
- è‡ªåŠ¨å¾®åˆ†åŸç†

## ğŸš€ è¿è¡Œç¤ºä¾‹

```bash
# å®‰è£…ä¾èµ–
pip install numpy matplotlib scipy pytest

# è¿è¡Œä¸»ç¨‹åº
python calculus_basics.py

# è¿è¡Œæµ‹è¯•
python -m pytest test_calculus.py -v
```

## ğŸ“– ä»£ç è¯¦è§£

### ä¸»è¦ç±»ç»“æ„

```python
class CalculusBasics:
    def derivative_concept(self):         # å¯¼æ•°æ¦‚å¿µæ¼”ç¤º
    def partial_derivatives(self):        # åå¯¼æ•°æ¼”ç¤º
    def gradient_descent_demo(self):      # æ¢¯åº¦ä¸‹é™æ¼”ç¤º
    def numerical_integration(self):      # æ•°å€¼ç§¯åˆ†æ¼”ç¤º
    def chain_rule_demo(self):            # é“¾å¼æ³•åˆ™æ¼”ç¤º
```

### å…³é”®æŠ€æœ¯ç‚¹æ¼”ç¤º

#### 1. å¯¼æ•°è®¡ç®—
```python
# å‡½æ•° f(x) = xÂ²
def f(x):
    return x**2

# å¯¼æ•° f'(x) = 2x
def f_prime(x):
    return 2*x

# åœ¨ x=2 å¤„çš„å¯¼æ•°
derivative_at_2 = f_prime(2)  # 4
```

#### 2. åå¯¼æ•°å’Œæ¢¯åº¦
```python
# äºŒå…ƒå‡½æ•° f(x,y) = xÂ² + yÂ²
def f(x, y):
    return x**2 + y**2

# åå¯¼æ•°
def df_dx(x, y):
    return 2*x

def df_dy(x, y):
    return 2*y

# æ¢¯åº¦å‘é‡
gradient = np.array([df_dx(1, 2), df_dy(1, 2)])  # [2, 4]
```

#### 3. æ¢¯åº¦ä¸‹é™å®ç°
```python
# æŸå¤±å‡½æ•°
def loss_function(x, y):
    return (x - 3)**2 + (y - 2)**2

# æ¢¯åº¦
def gradient(x, y):
    return np.array([2*(x - 3), 2*(y - 2)])

# æ¢¯åº¦ä¸‹é™
point = np.array([0.0, 0.0])
learning_rate = 0.1
for _ in range(100):
    grad = gradient(point[0], point[1])
    point = point - learning_rate * grad
```

#### 4. æ•°å€¼ç§¯åˆ†
```python
# æ¢¯å½¢æ³•åˆ™
x = np.linspace(0, 3, 1000)
y = x**2
integral = np.trapz(y, x)  # çº¦ç­‰äº 9

# Scipyç§¯åˆ†
from scipy import integrate
result, error = integrate.quad(lambda x: x**2, 0, 3)
```

#### 5. é“¾å¼æ³•åˆ™
```python
# å¤åˆå‡½æ•° h(x) = f(g(x)) = (2x + 1)Â³
def g(x): return 2*x + 1
def f(u): return u**3

# é“¾å¼æ³•åˆ™: h'(x) = f'(g(x)) * g'(x)
def dh_dx(x):
    u = g(x)
    return 3*u**2 * 2  # f'(u) * g'(x)
```

## ğŸ§ª æµ‹è¯•è¦†ç›–

æµ‹è¯•æ–‡ä»¶ `test_calculus.py` åŒ…å«ä»¥ä¸‹æµ‹è¯•ï¼š

âœ… åŸºæœ¬å¯¼æ•°è®¡ç®—æµ‹è¯•  
âœ… åå¯¼æ•°è®¡ç®—æµ‹è¯•  
âœ… æ¢¯åº¦è®¡ç®—æµ‹è¯•  
âœ… æ¢¯åº¦ä¸‹é™æ”¶æ•›æ€§æµ‹è¯•  
âœ… æ•°å€¼ç§¯åˆ†æµ‹è¯•  
âœ… é“¾å¼æ³•åˆ™æµ‹è¯•  
âœ… äºŒé˜¶å¯¼æ•°æµ‹è¯•  
âœ… å¤šå…ƒå‡½æ•°æå€¼æµ‹è¯•  

## ğŸ¯ å®é™…åº”ç”¨åœºæ™¯

### 1. æœºå™¨å­¦ä¹ ä¼˜åŒ–
```python
# çº¿æ€§å›å½’çš„æ¢¯åº¦ä¸‹é™
def compute_gradients(X, y, weights):
    predictions = X @ weights
    errors = predictions - y
    gradients = 2 * X.T @ errors / len(y)
    return gradients

# å‚æ•°æ›´æ–°
weights = weights - learning_rate * gradients
```

### 2. ç¥ç»ç½‘ç»œåå‘ä¼ æ’­
```python
# é“¾å¼æ³•åˆ™åœ¨åå‘ä¼ æ’­ä¸­çš„åº”ç”¨
# è¾“å‡ºå±‚æ¢¯åº¦
dL_doutput = 2 * (output - target)

# éšè—å±‚æ¢¯åº¦ï¼ˆé“¾å¼æ³•åˆ™ï¼‰
dL_dhidden = dL_doutput * weights_output * activation_derivative(hidden)
```

### 3. æ¦‚ç‡å¯†åº¦å‡½æ•°
```python
# æ­£æ€åˆ†å¸ƒçš„æ¦‚ç‡è®¡ç®—éœ€è¦ç§¯åˆ†
from scipy.stats import norm
probability = norm.cdf(b) - norm.cdf(a)  # P(a < X < b)
```

## âš¡ æœ€ä½³å®è·µå»ºè®®

### 1. æ•°å€¼ç¨³å®šæ€§
- é€‰æ‹©åˆé€‚çš„å­¦ä¹ ç‡
- ä½¿ç”¨è‡ªé€‚åº”å­¦ä¹ ç‡æ–¹æ³•
- æ³¨æ„æ¢¯åº¦æ¶ˆå¤±å’Œçˆ†ç‚¸é—®é¢˜

### 2. è®¡ç®—æ•ˆç‡
- å‘é‡åŒ–è®¡ç®—ä¼˜äºå¾ªç¯
- åˆ©ç”¨çŸ©é˜µè¿ç®—çš„ä¼˜åŒ–
- é€‰æ‹©åˆé€‚çš„æ•°å€¼ç§¯åˆ†æ–¹æ³•

### 3. å®é™…åº”ç”¨è€ƒè™‘
- ç†è§£æ•°å­¦æ¦‚å¿µçš„ç‰©ç†æ„ä¹‰
- æ ¹æ®é—®é¢˜ç‰¹ç‚¹é€‰æ‹©åˆé€‚çš„ä¼˜åŒ–æ–¹æ³•
- æ³¨æ„æ•°å€¼ç²¾åº¦å’Œè®¡ç®—å¤æ‚åº¦çš„å¹³è¡¡

## ğŸ” å¸¸è§é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ

### 1. æ¢¯åº¦ä¸‹é™ä¸æ”¶æ•›
```python
# é—®é¢˜ï¼šå­¦ä¹ ç‡è¿‡å¤§å¯¼è‡´éœ‡è¡
# è§£å†³ï¼šä½¿ç”¨å­¦ä¹ ç‡è°ƒåº¦æˆ–è‡ªé€‚åº”æ–¹æ³•
from torch.optim import Adam
optimizer = Adam(model.parameters(), lr=0.001)
```

### 2. æ•°å€¼ç§¯åˆ†ç²¾åº¦é—®é¢˜
```python
# é—®é¢˜ï¼šç®€å•æ–¹æ³•ç²¾åº¦ä¸è¶³
# è§£å†³ï¼šä½¿ç”¨é«˜ç²¾åº¦æ–¹æ³•æˆ–å¢åŠ é‡‡æ ·ç‚¹
result, error = integrate.quad(func, a, b, epsabs=1e-10, epsrel=1e-10)
```

## ğŸ“š æ‰©å±•å­¦ä¹ èµ„æº

### å®˜æ–¹æ–‡æ¡£
- [NumPyæ•°å­¦å‡½æ•°](https://numpy.org/doc/stable/reference/routines.math.html)
- [SciPyç§¯åˆ†æ¨¡å—](https://docs.scipy.org/doc/scipy/reference/integrate.html)

### æ¨èä¹¦ç±
- ã€Šå¾®ç§¯åˆ†å­¦æ•™ç¨‹ã€‹- Ğ“.Ğœ. è²èµ«é‡‘å“¥å°”èŒ¨
- ã€Šæ•°å€¼åˆ†æã€‹- Richard L. Burden
- ã€Šæ·±åº¦å­¦ä¹ ã€‹- Ian Goodfellow

### ç›¸å…³è¯¾ç¨‹
- MITå•å˜é‡å¾®ç§¯åˆ†
- æ–¯å¦ç¦å‡¸ä¼˜åŒ–è¯¾ç¨‹

## ğŸ”„ ç‰ˆæœ¬å†å²

- v1.0.0 (2024-01-15): åˆå§‹ç‰ˆæœ¬ï¼ŒåŒ…å«å®Œæ•´çš„å¾®ç§¯åˆ†åŸºç¡€æ¼”ç¤º

---
**æ³¨æ„**: å¾®ç§¯åˆ†æ˜¯æœºå™¨å­¦ä¹ ä¼˜åŒ–ç®—æ³•çš„æ•°å­¦åŸºç¡€ï¼Œæ·±å…¥ç†è§£è¿™äº›æ¦‚å¿µå¯¹æŒæ¡æ·±åº¦å­¦ä¹ è‡³å…³é‡è¦ã€‚