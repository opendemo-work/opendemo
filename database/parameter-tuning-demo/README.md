# æ•°æ®åº“å‚æ•°è°ƒä¼˜å®Œæ•´æŒ‡å—

## ğŸ¯ æ¦‚è¿°

æ•°æ®åº“å‚æ•°è°ƒä¼˜æ˜¯æå‡ç³»ç»Ÿæ€§èƒ½çš„å…³é”®æŠ€æœ¯ï¼Œé€šè¿‡å¯¹é…ç½®å‚æ•°çš„ç²¾ç¡®è°ƒæ•´ï¼Œå¯ä»¥æ˜¾è‘—æ”¹å–„å“åº”æ—¶é—´ã€ååé‡å’Œèµ„æºåˆ©ç”¨ç‡ã€‚æœ¬æŒ‡å—æä¾›ç³»ç»Ÿæ€§çš„å‚æ•°è°ƒä¼˜æ–¹æ³•è®ºå’Œæœ€ä½³å®è·µã€‚

## ğŸ“‹ ç›®å½•

1. [å‚æ•°è°ƒä¼˜åŸºç¡€ç†è®º](#1-å‚æ•°è°ƒä¼˜åŸºç¡€ç†è®º)
2. [MySQLå‚æ•°ä¼˜åŒ–](#2-mysqlå‚æ•°ä¼˜åŒ–)
3. [PostgreSQLè°ƒä¼˜ç­–ç•¥](#3-postgresqlè°ƒä¼˜ç­–ç•¥)
4. [MongoDBæ€§èƒ½å‚æ•°](#4-mongodbæ€§èƒ½å‚æ•°)
5. [Redisè°ƒä¼˜é…ç½®](#5-redisè°ƒä¼˜é…ç½®)
6. [ç³»ç»Ÿæ€§è°ƒä¼˜æ–¹æ³•](#6-ç³»ç»Ÿæ€§è°ƒä¼˜æ–¹æ³•)
7. [è°ƒä¼˜æ•ˆæœéªŒè¯](#7-è°ƒä¼˜æ•ˆæœéªŒè¯)

---

## 1. å‚æ•°è°ƒä¼˜åŸºç¡€ç†è®º

### 1.1 è°ƒä¼˜åŸºæœ¬åŸåˆ™

#### æ€§èƒ½è°ƒä¼˜é‡‘å­—å¡”
```mermaid
graph TD
    A[åº”ç”¨å±‚ä¼˜åŒ–] --> B[æ•°æ®åº“å±‚ä¼˜åŒ–]
    B --> C[æ“ä½œç³»ç»Ÿå±‚ä¼˜åŒ–]
    C --> D[ç¡¬ä»¶å±‚ä¼˜åŒ–]
    
    A1[SQLä¼˜åŒ–] --> A
    A2[ç´¢å¼•è®¾è®¡] --> A
    A3[æ¶æ„è°ƒæ•´] --> A
    
    B1[å‚æ•°è°ƒä¼˜] --> B
    B2[é…ç½®ä¼˜åŒ–] --> B
    B3[å­˜å‚¨å¼•æ“é€‰æ‹©] --> B
    
    C1[å†…æ ¸å‚æ•°] --> C
    C2[æ–‡ä»¶ç³»ç»Ÿ] --> C
    C3[å†…å­˜ç®¡ç†] --> C
    
    D1[CPUä¼˜åŒ–] --> D
    D2[å­˜å‚¨ä¼˜åŒ–] --> D
    D3[ç½‘ç»œä¼˜åŒ–] --> D
```

#### è°ƒä¼˜é»„é‡‘æ³•åˆ™
```yaml
tuning_principles:
  measure_first:
    description: "å…ˆæµ‹é‡å†è°ƒä¼˜"
    importance: "æœ€é«˜"
    practice: "å»ºç«‹åŸºå‡†æµ‹è¯•ï¼Œé‡åŒ–å½“å‰æ€§èƒ½"
  
  change_one_thing:
    description: "ä¸€æ¬¡åªæ”¹å˜ä¸€ä¸ªå‚æ•°"
    importance: "é«˜"
    practice: "éš”ç¦»å˜é‡ï¼Œå‡†ç¡®è¯†åˆ«å› æœå…³ç³»"
  
  monitor_continuously:
    description: "æŒç»­ç›‘æ§è°ƒä¼˜æ•ˆæœ"
    importance: "é«˜"
    practice: "å®æ—¶è·Ÿè¸ªå…³é”®æŒ‡æ ‡å˜åŒ–"
  
  document_everything:
    description: "è¯¦ç»†è®°å½•è°ƒä¼˜è¿‡ç¨‹"
    importance: "ä¸­"
    practice: "è®°å½•å‚æ•°å˜æ›´å’Œæ•ˆæœå¯¹æ¯”"
```

### 1.2 æ€§èƒ½ç“¶é¢ˆè¯†åˆ«

#### ç“¶é¢ˆåˆ†ææ–¹æ³•
```bash
# ç³»ç»Ÿæ€§èƒ½ç“¶é¢ˆè¯Šæ–­è„šæœ¬
diagnose_bottlenecks() {
    echo "=== æ€§èƒ½ç“¶é¢ˆè¯Šæ–­ ==="
    
    # CPUç“¶é¢ˆæ£€æŸ¥
    echo "CPUä½¿ç”¨ç‡åˆ†æ:"
    top -bn1 | head -20
    
    # å†…å­˜ç“¶é¢ˆæ£€æŸ¥
    echo "å†…å­˜ä½¿ç”¨æƒ…å†µ:"
    free -h
    echo "å†…å­˜å‹åŠ›æµ‹è¯•:"
    vmstat 1 5
    
    # I/Oç“¶é¢ˆæ£€æŸ¥
    echo "ç£ç›˜I/Oåˆ†æ:"
    iostat -x 1 5
    
    # ç½‘ç»œç“¶é¢ˆæ£€æŸ¥
    echo "ç½‘ç»œæ€§èƒ½åˆ†æ:"
    sar -n DEV 1 5
    
    # æ•°æ®åº“ç‰¹å®šæ£€æŸ¥
    echo "æ•°æ®åº“æ€§èƒ½æŒ‡æ ‡:"
    mysqladmin extended-status | grep -E "(Threads_connected|Created_tmp_disk_tables|Select_scan)"
}
```

#### è´Ÿè½½ç‰¹å¾åˆ†æ
```sql
-- è´Ÿè½½ç‰¹å¾åˆ†ææŸ¥è¯¢
SELECT 
    -- æŸ¥è¯¢ç±»å‹åˆ†å¸ƒ
    CASE 
        WHEN DIGEST_TEXT LIKE 'SELECT%' THEN 'è¯»æ“ä½œ'
        WHEN DIGEST_TEXT LIKE 'INSERT%' THEN 'æ’å…¥æ“ä½œ'
        WHEN DIGEST_TEXT LIKE 'UPDATE%' THEN 'æ›´æ–°æ“ä½œ'
        WHEN DIGEST_TEXT LIKE 'DELETE%' THEN 'åˆ é™¤æ“ä½œ'
        ELSE 'å…¶ä»–æ“ä½œ'
    END as operation_type,
    COUNT_STAR as execution_count,
    SUM_TIMER_WAIT/1000000000000 as total_time_sec,
    AVG_TIMER_WAIT/1000000000 as avg_time_ms,
    SUM_ROWS_EXAMINED as total_rows_examined,
    SUM_ROWS_SENT as total_rows_sent
FROM performance_schema.events_statements_summary_by_digest
WHERE SCHEMA_NAME = 'your_database'
GROUP BY operation_type
ORDER BY total_time_sec DESC;
```

### 1.3 è°ƒä¼˜æ–¹æ³•è®º

#### ç³»ç»Ÿæ€§è°ƒä¼˜æµç¨‹
```python
# ç³»ç»Ÿæ€§è°ƒä¼˜æ¡†æ¶
class SystematicTuner:
    def __init__(self):
        self.baseline_metrics = {}
        self.tuning_phases = ['assessment', 'hypothesis', 'experiment', 'validation', 'implementation']
    
    def tuning_process(self, database_config):
        """å®Œæ•´çš„è°ƒä¼˜æµç¨‹"""
        results = {}
        
        # 1. ç°çŠ¶è¯„ä¼°
        results['assessment'] = self.assess_current_state(database_config)
        
        # 2. å‡è®¾å½¢æˆ
        results['hypothesis'] = self.form_hypotheses(results['assessment'])
        
        # 3. å®éªŒè®¾è®¡
        results['experiment'] = self.design_experiments(results['hypothesis'])
        
        # 4. éªŒè¯æµ‹è¯•
        results['validation'] = self.validate_changes(results['experiment'])
        
        # 5. å®æ–½éƒ¨ç½²
        results['implementation'] = self.implement_optimizations(results['validation'])
        
        return results
    
    def assess_current_state(self, config):
        """ç°çŠ¶è¯„ä¼°"""
        assessment = {
            'workload_characteristics': self.analyze_workload(config),
            'current_performance': self.measure_performance(config),
            'resource_utilization': self.check_resource_usage(config),
            'configuration_review': self.review_current_settings(config)
        }
        return assessment
```

## 2. MySQLå‚æ•°ä¼˜åŒ–

### 2.1 æ ¸å¿ƒæ€§èƒ½å‚æ•°

#### InnoDBå­˜å‚¨å¼•æ“è°ƒä¼˜
```ini
# my.cnf æ ¸å¿ƒå‚æ•°é…ç½®
[mysqld]

# å†…å­˜ç›¸å…³å‚æ•°
innodb_buffer_pool_size = 12G           # ç‰©ç†å†…å­˜çš„70-80%
innodb_buffer_pool_instances = 8        # å¹¶è¡Œå®ä¾‹æ•°
innodb_log_file_size = 2G               # redoæ—¥å¿—å¤§å°
innodb_log_buffer_size = 64M            # æ—¥å¿—ç¼“å†²åŒº
innodb_flush_log_at_trx_commit = 2      # äº‹åŠ¡æäº¤åˆ·ç›˜ç­–ç•¥

# è¿æ¥å’Œçº¿ç¨‹å‚æ•°
max_connections = 500                   # æœ€å¤§è¿æ¥æ•°
thread_cache_size = 100                 # çº¿ç¨‹ç¼“å­˜å¤§å°
table_open_cache = 4000                 # è¡¨ç¼“å­˜å¤§å°
table_definition_cache = 2000           # è¡¨å®šä¹‰ç¼“å­˜

# æŸ¥è¯¢ä¼˜åŒ–å‚æ•°
query_cache_type = 1                    # æŸ¥è¯¢ç¼“å­˜å¯ç”¨
query_cache_size = 256M                 # æŸ¥è¯¢ç¼“å­˜å¤§å°
tmp_table_size = 256M                   # ä¸´æ—¶è¡¨å¤§å°
max_heap_table_size = 256M              # å†…å­˜è¡¨æœ€å¤§å¤§å°
sort_buffer_size = 2M                   # æ’åºç¼“å†²åŒº
join_buffer_size = 2M                   # è¿æ¥ç¼“å†²åŒº
read_buffer_size = 1M                   # é¡ºåºè¯»å–ç¼“å†²åŒº
read_rnd_buffer_size = 1M               # éšæœºè¯»å–ç¼“å†²åŒº

# æ—¥å¿—å’Œå¤åˆ¶å‚æ•°
sync_binlog = 1                         # binlogåŒæ­¥ç­–ç•¥
binlog_format = ROW                     # binlogæ ¼å¼
expire_logs_days = 7                    # binlogä¿ç•™å¤©æ•°
```

#### å‚æ•°è°ƒä¼˜è„šæœ¬
```bash
# MySQLå‚æ•°ä¼˜åŒ–åŠ©æ‰‹
optimize_mysql_parameters() {
    echo "=== MySQLå‚æ•°ä¼˜åŒ–åˆ†æ ==="
    
    # è·å–ç³»ç»Ÿå†…å­˜ä¿¡æ¯
    total_memory=$(free -g | awk '/^Mem:/{print $2}')
    echo "ç³»ç»Ÿæ€»å†…å­˜: ${total_memory}GB"
    
    # è®¡ç®—æ¨èçš„buffer poolå¤§å°
    recommended_buffer_pool=$((total_memory * 75 / 100))
    echo "æ¨èInnoDB Buffer Poolå¤§å°: ${recommended_buffer_pool}GB"
    
    # åˆ†æå½“å‰é…ç½®
    current_buffer_pool=$(mysql -e "SHOW VARIABLES LIKE 'innodb_buffer_pool_size';" | tail -1 | awk '{print $2/1024/1024/1024}')
    echo "å½“å‰Buffer Poolå¤§å°: ${current_buffer_pool}GB"
    
    # æ£€æŸ¥è¿æ¥ä½¿ç”¨æƒ…å†µ
    current_connections=$(mysql -e "SHOW STATUS LIKE 'Threads_connected';" | tail -1 | awk '{print $2}')
    max_connections=$(mysql -e "SHOW VARIABLES LIKE 'max_connections';" | tail -1 | awk '{print $2}')
    connection_utilization=$(echo "scale=2; $current_connections * 100 / $max_connections" | bc)
    echo "è¿æ¥ä½¿ç”¨ç‡: ${connection_utilization}%"
    
    # ç”Ÿæˆä¼˜åŒ–å»ºè®®
    generate_mysql_recommendations $total_memory $current_buffer_pool $connection_utilization
}
```

### 2.2 ä¸åŒå·¥ä½œè´Ÿè½½ä¼˜åŒ–

#### OLTPå·¥ä½œè´Ÿè½½ä¼˜åŒ–
```ini
# OLTPä¼˜åŒ–é…ç½®
[mysqld]

# é«˜å¹¶å‘è¿æ¥ä¼˜åŒ–
max_connections = 1000
thread_handling = pool-of-threads
thread_cache_size = 200

# äº‹åŠ¡ä¼˜åŒ–
innodb_flush_log_at_trx_commit = 2
innodb_flush_method = O_DIRECT
innodb_io_capacity = 2000
innodb_io_capacity_max = 4000

# é”ä¼˜åŒ–
innodb_lock_wait_timeout = 50
innodb_deadlock_detect = ON

# ç¼“å†²åŒºä¼˜åŒ–
innodb_buffer_pool_size = 24G
innodb_buffer_pool_instances = 16
innodb_log_file_size = 4G
innodb_log_buffer_size = 128M
```

#### OLAPå·¥ä½œè´Ÿè½½ä¼˜åŒ–
```ini
# OLAPä¼˜åŒ–é…ç½®
[mysqld]

# å¤§æŸ¥è¯¢ä¼˜åŒ–
max_allowed_packet = 1G
group_concat_max_len = 1048576
tmp_table_size = 1G
max_heap_table_size = 1G

# æ’åºå’Œèšåˆä¼˜åŒ–
sort_buffer_size = 8M
join_buffer_size = 8M
read_buffer_size = 4M
read_rnd_buffer_size = 8M

# å¹¶è¡ŒæŸ¥è¯¢ä¼˜åŒ–
innodb_buffer_pool_size = 32G
innodb_buffer_pool_instances = 32
innodb_read_io_threads = 16
innodb_write_io_threads = 16

# æŸ¥è¯¢ç¼“å­˜ä¼˜åŒ–
query_cache_type = 1
query_cache_size = 512M
query_cache_limit = 32M
```

### 2.3 å®æ—¶è°ƒä¼˜ç›‘æ§

#### æ€§èƒ½ç›‘æ§è„šæœ¬
```python
# MySQLæ€§èƒ½ç›‘æ§ç±»
class MySQLPerformanceMonitor:
    def __init__(self, connection_config):
        self.connection = self.create_connection(connection_config)
        self.metrics_history = {}
    
    def collect_performance_metrics(self):
        """æ”¶é›†æ€§èƒ½æŒ‡æ ‡"""
        metrics = {
            'buffer_pool': self.get_buffer_pool_stats(),
            'connections': self.get_connection_stats(),
            'queries': self.get_query_stats(),
            'locks': self.get_lock_stats(),
            'replication': self.get_replication_stats()
        }
        return metrics
    
    def get_buffer_pool_stats(self):
        """è·å–ç¼“å†²æ± ç»Ÿè®¡ä¿¡æ¯"""
        cursor = self.connection.cursor()
        cursor.execute("""
            SELECT 
                pool_id,
                ROUND(pool_size * 16384 / 1024 / 1024 / 1024, 2) as pool_size_gb,
                ROUND(free_buffers * 16384 / 1024 / 1024 / 1024, 2) as free_gb,
                ROUND(database_pages * 16384 / 1024 / 1024 / 1024, 2) as data_gb,
                ROUND(old_database_pages * 16384 / 1024 / 1024 / 1024, 2) as old_data_gb,
                ROUND(modified_database_pages * 16384 / 1024 / 1024 / 1024, 2) as dirty_gb
            FROM information_schema.INNODB_BUFFER_POOL_STATS
        """)
        return cursor.fetchall()
    
    def analyze_parameter_effectiveness(self, metrics_history):
        """åˆ†æå‚æ•°æ•ˆæœ"""
        analysis = {}
        
        # ç¼“å†²æ± å‘½ä¸­ç‡åˆ†æ
        buffer_hit_rates = [m['buffer_pool']['hit_rate'] for m in metrics_history[-10:]]
        analysis['buffer_pool_efficiency'] = {
            'current': buffer_hit_rates[-1],
            'trend': self.calculate_trend(buffer_hit_rates),
            'recommendation': self.buffer_pool_recommendation(buffer_hit_rates[-1])
        }
        
        # è¿æ¥ä½¿ç”¨ç‡åˆ†æ
        connection_utilization = [m['connections']['utilization'] for m in metrics_history[-10:]]
        analysis['connection_efficiency'] = {
            'current': connection_utilization[-1],
            'trend': self.calculate_trend(connection_utilization),
            'recommendation': self.connection_recommendation(connection_utilization[-1])
        }
        
        return analysis
```

## 3. PostgreSQLè°ƒä¼˜ç­–ç•¥

### 3.1 æ ¸å¿ƒé…ç½®å‚æ•°

#### postgresql.confä¼˜åŒ–
```conf
# å†…å­˜é…ç½®
shared_buffers = 8GB                    # ç‰©ç†å†…å­˜çš„25%
effective_cache_size = 24GB             # ç‰©ç†å†…å­˜çš„75%
work_mem = 64MB                         # å•ä¸ªæŸ¥è¯¢å·¥ä½œå†…å­˜
maintenance_work_mem = 1GB              # ç»´æŠ¤æ“ä½œå†…å­˜

# WALå’Œæ£€æŸ¥ç‚¹é…ç½®
wal_buffers = 64MB                      # WALç¼“å†²åŒº
checkpoint_segments = 64                # æ£€æŸ¥ç‚¹æ®µæ•°
checkpoint_completion_target = 0.9      # æ£€æŸ¥ç‚¹å®Œæˆç›®æ ‡
checkpoint_timeout = 15min              # æ£€æŸ¥ç‚¹è¶…æ—¶

# æŸ¥è¯¢ä¼˜åŒ–å™¨é…ç½®
random_page_cost = 1.1                  # éšæœºé¡µé¢è®¿é—®æˆæœ¬
seq_page_cost = 1.0                     # é¡ºåºé¡µé¢è®¿é—®æˆæœ¬
effective_io_concurrency = 200          # IOå¹¶å‘åº¦
max_worker_processes = 32               # æœ€å¤§å·¥ä½œè¿›ç¨‹æ•°

# è¿æ¥å’Œå¹¶å‘é…ç½®
max_connections = 200                   # æœ€å¤§è¿æ¥æ•°
superuser_reserved_connections = 3      # è¶…çº§ç”¨æˆ·ä¿ç•™è¿æ¥
max_prepared_transactions = 0           # é¢„å¤„ç†äº‹åŠ¡æ•°

# è‡ªåŠ¨æ¸…ç†é…ç½®
autovacuum = on                         # è‡ªåŠ¨æ¸…ç†å¼€å¯
autovacuum_max_workers = 6              # æœ€å¤§è‡ªåŠ¨æ¸…ç†å·¥ä½œè¿›ç¨‹
autovacuum_naptime = 1min               # è‡ªåŠ¨æ¸…ç†é—´éš”
autovacuum_vacuum_threshold = 50        # æ¸…ç†é˜ˆå€¼
autovacuum_analyze_threshold = 50       # åˆ†æé˜ˆå€¼
```

#### PostgreSQLè°ƒä¼˜è„šæœ¬
```bash
# PostgreSQLå‚æ•°ä¼˜åŒ–åˆ†æ
optimize_postgresql_parameters() {
    echo "=== PostgreSQLå‚æ•°ä¼˜åŒ–åˆ†æ ==="
    
    # è·å–ç³»ç»Ÿä¿¡æ¯
    total_memory=$(free -g | awk '/^Mem:/{print $2}')
    cpu_cores=$(nproc)
    
    echo "ç³»ç»Ÿå†…å­˜: ${total_memory}GB"
    echo "CPUæ ¸å¿ƒæ•°: ${cpu_cores}"
    
    # è¿æ¥åˆ°PostgreSQLè·å–å½“å‰é…ç½®
    psql -c "
        SELECT 
            name,
            setting,
            unit,
            short_desc
        FROM pg_settings 
        WHERE name IN (
            'shared_buffers', 'effective_cache_size', 'work_mem',
            'maintenance_work_mem', 'max_connections', 'checkpoint_segments'
        )
        ORDER BY name;
    "
    
    # åˆ†æå»ºè®®
    echo "åŸºäºç³»ç»Ÿé…ç½®çš„å»ºè®®:"
    echo "- shared_buffers: $(echo "${total_memory} * 0.25" | bc)GB"
    echo "- effective_cache_size: $(echo "${total_memory} * 0.75" | bc)GB"
    echo "- work_mem: æ ¹æ®å¹¶å‘æŸ¥è¯¢æ•°è°ƒæ•´ï¼Œå»ºè®®64-256MB"
    echo "- max_connections: æ ¹æ®åº”ç”¨éœ€æ±‚ï¼Œå»ºè®®100-500"
}
```

### 3.2 æŸ¥è¯¢ä¼˜åŒ–å‚æ•°

#### æŸ¥è¯¢è®¡åˆ’å™¨è°ƒä¼˜
```sql
-- æŸ¥è¯¢ä¼˜åŒ–å™¨å‚æ•°è°ƒæ•´
-- è°ƒæ•´æˆæœ¬å¸¸é‡
SET random_page_cost = 1.1;      -- SSDå­˜å‚¨é€‚åˆè¾ƒä½å€¼
SET seq_page_cost = 1.0;
SET cpu_tuple_cost = 0.01;
SET cpu_index_tuple_cost = 0.005;
SET cpu_operator_cost = 0.0025;

-- å¹¶è¡ŒæŸ¥è¯¢é…ç½®
SET max_parallel_workers_per_gather = 4;
SET parallel_tuple_cost = 0.1;
SET parallel_setup_cost = 1000.0;
SET min_parallel_table_scan_size = '8MB';
SET min_parallel_index_scan_size = '512kB';

-- ç»Ÿè®¡ä¿¡æ¯æ”¶é›†
ALTER TABLE your_table SET STATISTICS 1000;  -- æé«˜é‡‡æ ·ç‡
ANALYZE VERBOSE your_table;  -- æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
```

#### ç´¢å¼•ç›¸å…³å‚æ•°
```conf
# ç´¢å¼•åˆ›å»ºå’Œç»´æŠ¤ä¼˜åŒ–
enable_bitmapscan = on              # ä½å›¾æ‰«æå¯ç”¨
enable_hashagg = on                 # å“ˆå¸Œèšåˆå¯ç”¨
enable_hashjoin = on                # å“ˆå¸Œè¿æ¥å¯ç”¨
enable_indexscan = on               # ç´¢å¼•æ‰«æå¯ç”¨
enable_indexonlyscan = on           # ç´¢å¼•ä»…æ‰«æå¯ç”¨
enable_material = on                # ç‰©åŒ–å¯ç”¨
enable_mergejoin = on               # å½’å¹¶è¿æ¥å¯ç”¨
enable_nestloop = on                # åµŒå¥—å¾ªç¯å¯ç”¨
enable_seqscan = on                 # é¡ºåºæ‰«æå¯ç”¨
enable_sort = on                    # æ’åºå¯ç”¨
enable_tidscan = on                 # TIDæ‰«æå¯ç”¨
```

### 3.3 æ€§èƒ½ç›‘æ§è§†å›¾

#### è‡ªå®šä¹‰ç›‘æ§è§†å›¾
```sql
-- åˆ›å»ºæ€§èƒ½ç›‘æ§è§†å›¾
CREATE VIEW performance_monitoring AS
SELECT 
    datname as database_name,
    usename as user_name,
    application_name,
    client_addr,
    backend_start,
    state,
    wait_event_type,
    wait_event,
    state_change,
    query_start,
    query,
    -- æ€§èƒ½æŒ‡æ ‡
    EXTRACT(EPOCH FROM (now() - query_start)) as query_duration_seconds,
    EXTRACT(EPOCH FROM (now() - state_change)) as state_duration_seconds
FROM pg_stat_activity 
WHERE state != 'idle' 
AND query NOT ILIKE '%pg_stat_activity%';

-- ç¼“å†²åŒºå‘½ä¸­ç‡ç›‘æ§
CREATE VIEW buffer_pool_stats AS
SELECT 
    blks_read,
    blks_hit,
    round(blks_hit::float/(blks_read+blks_hit)*100, 2) as hit_ratio_percent,
    now() as sample_time
FROM pg_stat_database 
WHERE datname = current_database();

-- æŸ¥è¯¢æ€§èƒ½åˆ†æ
CREATE VIEW query_performance AS
SELECT 
    queryid,
    query,
    calls,
    total_time,
    mean_time,
    rows,
    100.0 * shared_blks_hit / nullif(shared_blks_hit + shared_blks_read, 0) as hit_percent
FROM pg_stat_statements 
ORDER BY total_time DESC 
LIMIT 20;
```

## 4. MongoDBæ€§èƒ½å‚æ•°

### 4.1 æ ¸å¿ƒé…ç½®ä¼˜åŒ–

#### mongod.confé…ç½®
```yaml
# MongoDBé…ç½®æ–‡ä»¶ä¼˜åŒ–
storage:
  engine: wiredTiger
  wiredTiger:
    engineConfig:
      cacheSizeGB: 12                    # WiredTigerç¼“å­˜å¤§å°
      blockCompressor: snappy            # å—å‹ç¼©ç®—æ³•
    collectionConfig:
      blockCompressor: snappy
    indexConfig:
      prefixCompression: true

systemLog:
  destination: file
  path: /var/log/mongodb/mongod.log
  logAppend: true
  verbosity: 0

operationProfiling:
  slowOpThresholdMs: 100                # æ…¢æŸ¥è¯¢é˜ˆå€¼
  mode: slowOp                          # æ€§èƒ½åˆ†ææ¨¡å¼

replication:
  oplogSizeMB: 10240                    # oplogå¤§å°(10GB)

processManagement:
  fork: true
  pidFilePath: /var/run/mongodb/mongod.pid

net:
  port: 27017
  bindIp: 0.0.0.0
  maxIncomingConnections: 65536         # æœ€å¤§è¿æ¥æ•°
```

#### MongoDBè°ƒä¼˜å‘½ä»¤
```javascript
// MongoDBå‚æ•°è°ƒä¼˜
use admin

// æŸ¥çœ‹å½“å‰é…ç½®
db.runCommand({ getParameter: "*" })

// è°ƒæ•´WiredTigerç¼“å­˜å¤§å°
db.adminCommand({ 
    "setParameter": 1, 
    "wiredTigerEngineConfigString": "cache_size=12G" 
})

// è®¾ç½®æ…¢æŸ¥è¯¢é˜ˆå€¼
db.setProfilingLevel(1, { slowms: 100 })

// æŸ¥çœ‹æ€§èƒ½ç»Ÿè®¡
db.serverStatus().wiredTiger.cache
db.serverStatus().connections
db.serverStatus().opcounters

// åˆ†ææŸ¥è¯¢æ€§èƒ½
db.system.profile.find().sort({ millis: -1 }).limit(10)
```

### 4.2 å†…å­˜å’Œå­˜å‚¨ä¼˜åŒ–

#### å†…å­˜åˆ†é…ç­–ç•¥
```javascript
// å†…å­˜ä½¿ç”¨åˆ†æ
analyze_memory_usage = function() {
    var serverStatus = db.serverStatus();
    
    // WiredTigerç¼“å­˜ç»Ÿè®¡
    var cache = serverStatus.wiredTiger.cache;
    print("=== WiredTigerç¼“å­˜ä½¿ç”¨æƒ…å†µ ===");
    print("æ€»ç¼“å­˜å¤§å°: " + cache["maximum bytes configured"]);
    print("å½“å‰ä½¿ç”¨: " + cache["bytes currently in the cache"]);
    print("è„é¡µæ¯”ä¾‹: " + cache["percentage overhead"]);
    print("ç¼“å­˜å‘½ä¸­ç‡: " + cache["percentage of bytes read into cache"]);
    
    // è¿æ¥ç»Ÿè®¡
    var connections = serverStatus.connections;
    print("\n=== è¿æ¥ç»Ÿè®¡ ===");
    print("å½“å‰è¿æ¥æ•°: " + connections.current);
    print("å¯ç”¨è¿æ¥æ•°: " + connections.available);
    print("æ€»è¿æ¥æ•°: " + connections.totalCreated);
}

// å­˜å‚¨å¼•æ“ä¼˜åŒ–
optimize_storage_engine = function() {
    // æ£€æŸ¥å­˜å‚¨ä½¿ç”¨æƒ…å†µ
    db.stats()
    
    // åˆ†æé›†åˆå¤§å°
    db.getCollectionNames().forEach(function(collName) {
        var stats = db[collName].stats();
        print("é›†åˆ: " + collName);
        print("  æ–‡æ¡£æ•°: " + stats.count);
        print("  å¤§å°: " + Math.round(stats.size/1024/1024) + " MB");
        print("  å­˜å‚¨å¤§å°: " + Math.round(stats.storageSize/1024/1024) + " MB");
        print("  ç´¢å¼•å¤§å°: " + Math.round(stats.totalIndexSize/1024/1024) + " MB");
        print("---");
    });
}
```

### 4.3 æŸ¥è¯¢å’Œç´¢å¼•ä¼˜åŒ–

#### æŸ¥è¯¢æ€§èƒ½è°ƒä¼˜
```javascript
// æŸ¥è¯¢ä¼˜åŒ–åˆ†æ
analyze_query_performance = function() {
    // å¯ç”¨æŸ¥è¯¢åˆ†æå™¨
    db.setProfilingLevel(2);  // è®°å½•æ‰€æœ‰æ“ä½œ
    
    // æ‰§è¡Œä¸€äº›å…¸å‹æŸ¥è¯¢
    db.collection.find({ status: "active" }).explain("executionStats");
    db.collection.aggregate([
        { $match: { createdAt: { $gte: new Date("2023-01-01") } } },
        { $group: { _id: "$category", count: { $sum: 1 } } }
    ]).explain("executionStats");
    
    // åˆ†ææ…¢æŸ¥è¯¢
    db.system.profile.find({ 
        millis: { $gt: 100 } 
    }).sort({ 
        ts: -1 
    }).limit(10).pretty();
    
    // é‡ç½®åˆ†æçº§åˆ«
    db.setProfilingLevel(0);
}

// ç´¢å¼•ä¼˜åŒ–å»ºè®®
generate_index_recommendations = function() {
    // æŸ¥æ‰¾æœªä½¿ç”¨çš„ç´¢å¼•
    var unusedIndexes = [];
    db.getCollectionNames().forEach(function(collName) {
        var coll = db[collName];
        var indexStats = coll.aggregate([{ $indexStats: {} }]);
        
        indexStats.forEach(function(stat) {
            if (stat.accesses.ops === 0) {
                unusedIndexes.push({
                    collection: collName,
                    index: stat.name,
                    since: stat.accesses.since
                });
            }
        });
    });
    
    print("æœªä½¿ç”¨ç´¢å¼•:");
    unusedIndexes.forEach(function(idx) {
        print("  " + idx.collection + "." + idx.index);
    });
}
```

## 5. Redisè°ƒä¼˜é…ç½®

### 5.1 æ ¸å¿ƒæ€§èƒ½å‚æ•°

#### redis.confä¼˜åŒ–é…ç½®
```conf
# å†…å­˜ç®¡ç†
maxmemory 8gb                           # æœ€å¤§å†…å­˜é™åˆ¶
maxmemory-policy allkeys-lru            # å†…å­˜æ·˜æ±°ç­–ç•¥
lazyfree-lazy-eviction yes              # æƒ°æ€§åˆ é™¤
lazyfree-lazy-expire yes                # æƒ°æ€§è¿‡æœŸ
lazyfree-lazy-server-del yes            # æƒ°æ€§æœåŠ¡å™¨åˆ é™¤

# æŒä¹…åŒ–é…ç½®
save 900 1                              # 900ç§’å†…è‡³å°‘1ä¸ªkeyå˜åŒ–
save 300 10                             # 300ç§’å†…è‡³å°‘10ä¸ªkeyå˜åŒ–
save 60 10000                           # 60ç§’å†…è‡³å°‘10000ä¸ªkeyå˜åŒ–
appendonly yes                          # AOFæŒä¹…åŒ–å¼€å¯
appendfsync everysec                    # AOFåŒæ­¥ç­–ç•¥

# ç½‘ç»œå’Œè¿æ¥
tcp-keepalive 300                       # TCPä¿æ´»æ—¶é—´
timeout 0                               # è¿æ¥è¶…æ—¶(0è¡¨ç¤ºæ°¸ä¸è¶…æ—¶)
tcp-backlog 511                         # TCPç›‘å¬é˜Ÿåˆ—é•¿åº¦
maxclients 10000                        # æœ€å¤§å®¢æˆ·ç«¯è¿æ¥æ•°

# æ€§èƒ½ä¼˜åŒ–
hz 10                                   # æœåŠ¡å™¨é¢‘ç‡
activerehashing yes                     # ä¸»åŠ¨é‡æ–°å“ˆå¸Œ
protected-mode yes                      # ä¿æŠ¤æ¨¡å¼
stop-writes-on-bgsave-error no          # BGSAVEé”™è¯¯æ—¶ä¸é˜»æ­¢å†™å…¥

# é›†ç¾¤é…ç½®
cluster-enabled yes                     # å¯ç”¨é›†ç¾¤æ¨¡å¼
cluster-config-file nodes.conf          # é›†ç¾¤é…ç½®æ–‡ä»¶
cluster-node-timeout 15000              # èŠ‚ç‚¹è¶…æ—¶æ—¶é—´
```

#### Redisè°ƒä¼˜è„šæœ¬
```bash
#!/bin/bash
# Redisæ€§èƒ½è°ƒä¼˜è„šæœ¬

optimize_redis() {
    echo "=== Rediså‚æ•°è°ƒä¼˜åˆ†æ ==="
    
    # è·å–Redisä¿¡æ¯
    redis_info=$(redis-cli INFO)
    
    # å†…å­˜ä½¿ç”¨åˆ†æ
    used_memory=$(echo "$redis_info" | grep "used_memory_human" | cut -d: -f2)
    maxmemory=$(echo "$redis_info" | grep "maxmemory_human" | cut -d: -f2)
    memory_fragmentation=$(echo "$redis_info" | grep "mem_fragmentation_ratio" | cut -d: -f2)
    
    echo "å†…å­˜ä½¿ç”¨æƒ…å†µ:"
    echo "  å·²ä½¿ç”¨å†…å­˜: $used_memory"
    echo "  æœ€å¤§å†…å­˜é™åˆ¶: $maxmemory"
    echo "  å†…å­˜ç¢ç‰‡ç‡: $memory_fragmentation"
    
    # è¿æ¥åˆ†æ
    connected_clients=$(echo "$redis_info" | grep "connected_clients" | cut -d: -f2)
    blocked_clients=$(echo "$redis_info" | grep "blocked_clients" | cut -d: -f2)
    
    echo "è¿æ¥ç»Ÿè®¡:"
    echo "  è¿æ¥å®¢æˆ·ç«¯æ•°: $connected_clients"
    echo "  é˜»å¡å®¢æˆ·ç«¯æ•°: $blocked_clients"
    
    # æ€§èƒ½æŒ‡æ ‡
    instantaneous_ops=$(echo "$redis_info" | grep "instantaneous_ops_per_sec" | cut -d: -f2)
    hit_rate=$(redis-cli INFO | grep "keyspace_hits" | cut -d: -f2)
    miss_rate=$(redis-cli INFO | grep "keyspace_misses" | cut -d: -f2)
    hit_ratio=$(echo "scale=2; $hit_rate / ($hit_rate + $miss_rate) * 100" | bc 2>/dev/null || echo "N/A")
    
    echo "æ€§èƒ½æŒ‡æ ‡:"
    echo "  æ¯ç§’æ“ä½œæ•°: $instantaneous_ops"
    echo "  ç¼“å­˜å‘½ä¸­ç‡: ${hit_ratio}%"
    
    # ç”Ÿæˆä¼˜åŒ–å»ºè®®
    generate_redis_recommendations $memory_fragmentation $hit_ratio $connected_clients
}

generate_redis_recommendations() {
    local fragmentation=$1
    local hit_ratio=$2
    local clients=$3
    
    echo "ä¼˜åŒ–å»ºè®®:"
    
    # å†…å­˜ç¢ç‰‡åŒ–å»ºè®®
    if (( $(echo "$fragmentation > 1.5" | bc -l) )); then
        echo "- å†…å­˜ç¢ç‰‡ç‡è¾ƒé«˜(${fragmentation})ï¼Œå»ºè®®é‡å¯Redisæˆ–è°ƒæ•´maxmemory-policy"
    fi
    
    # ç¼“å­˜å‘½ä¸­ç‡å»ºè®®
    if [[ "$hit_ratio" != "N/A" ]] && (( $(echo "$hit_ratio < 90" | bc -l) )); then
        echo "- ç¼“å­˜å‘½ä¸­ç‡åä½(${hit_ratio}%)ï¼Œå»ºè®®å¢åŠ maxmemoryæˆ–ä¼˜åŒ–æ•°æ®è®¿é—®æ¨¡å¼"
    fi
    
    # è¿æ¥æ•°å»ºè®®
    if (( clients > 8000 )); then
        echo "- è¿æ¥æ•°è¾ƒå¤š(${clients})ï¼Œå»ºè®®æ£€æŸ¥è¿æ¥æ± é…ç½®æˆ–å¢åŠ maxclients"
    fi
}
```

### 5.2 æŒä¹…åŒ–ç­–ç•¥ä¼˜åŒ–

#### RDBå’ŒAOFé…ç½®
```bash
# RDBæŒä¹…åŒ–ä¼˜åŒ–
optimize_rdb() {
    echo "=== RDBæŒä¹…åŒ–ä¼˜åŒ– ==="
    
    # æ£€æŸ¥å½“å‰RDBé…ç½®
    redis-cli CONFIG GET save
    
    # æ ¹æ®å†™å…¥é¢‘ç‡è°ƒæ•´ä¿å­˜ç­–ç•¥
    write_frequency=$(redis-cli INFO | grep "total_commands_processed" | cut -d: -f2)
    if (( write_frequency > 1000000 )); then
        echo "é«˜é¢‘å†™å…¥ï¼Œå»ºè®®è°ƒæ•´RDBä¿å­˜ç­–ç•¥"
        redis-cli CONFIG SET save "60 10000"  # æ›´é¢‘ç¹ä¿å­˜
    elif (( write_frequency < 100000 )); then
        echo "ä½é¢‘å†™å…¥ï¼Œå¯ä»¥å‡å°‘RDBä¿å­˜é¢‘ç‡"
        redis-cli CONFIG SET save "300 10"    # è¾ƒå°‘ä¿å­˜
    fi
}

# AOFæŒä¹…åŒ–ä¼˜åŒ–
optimize_aof() {
    echo "=== AOFæŒä¹…åŒ–ä¼˜åŒ– ==="
    
    # æ£€æŸ¥AOFé…ç½®
    redis-cli CONFIG GET appendonly
    redis-cli CONFIG GET appendfsync
    
    # æ ¹æ®æ€§èƒ½è¦æ±‚è°ƒæ•´
    performance_priority=$(get_performance_priority)  # å‡è®¾çš„å‡½æ•°
    
    if [[ "$performance_priority" == "high" ]]; then
        echo "æ€§èƒ½ä¼˜å…ˆï¼Œè®¾ç½®AOFä¸ºeverysec"
        redis-cli CONFIG SET appendfsync "everysec"
    else
        echo "æ•°æ®å®‰å…¨ä¼˜å…ˆï¼Œè®¾ç½®AOFä¸ºalways"
        redis-cli CONFIG SET appendfsync "always"
    fi
}
```

### 5.3 é›†ç¾¤æ€§èƒ½è°ƒä¼˜

#### Redisé›†ç¾¤é…ç½®
```bash
# Redisé›†ç¾¤æ€§èƒ½ä¼˜åŒ–
optimize_redis_cluster() {
    echo "=== Redisé›†ç¾¤è°ƒä¼˜ ==="
    
    # æ£€æŸ¥é›†ç¾¤çŠ¶æ€
    cluster_info=$(redis-cli CLUSTER INFO)
    echo "é›†ç¾¤ä¿¡æ¯:"
    echo "$cluster_info"
    
    # èŠ‚ç‚¹å¥åº·æ£€æŸ¥
    node_health=$(redis-cli CLUSTER NODES)
    echo "èŠ‚ç‚¹å¥åº·çŠ¶æ€:"
    echo "$node_health" | grep -E "(fail|disconnected)"
    
    # è´Ÿè½½å‡è¡¡æ£€æŸ¥
    slot_distribution=$(redis-cli CLUSTER SLOTS | wc -l)
    echo "æ§½ä½åˆ†å¸ƒ: $slot_distributionä¸ªæ§½ä½"
    
    # ç”Ÿæˆé›†ç¾¤ä¼˜åŒ–å»ºè®®
    if (( slot_distribution < 16384 )); then
        echo "è­¦å‘Š: æ§½ä½åˆ†å¸ƒä¸å‡ï¼Œå»ºè®®é‡æ–°åˆ†ç‰‡"
    fi
}

# é›†ç¾¤ç›‘æ§è„šæœ¬
monitor_cluster_performance() {
    while true; do
        echo "$(date): é›†ç¾¤æ€§èƒ½å¿«ç…§"
        
        # å„èŠ‚ç‚¹æ€§èƒ½æŒ‡æ ‡
        for node in $(redis-cli CLUSTER NODES | awk '{print $2}' | cut -d@ -f1); do
            echo "èŠ‚ç‚¹ $node:"
            redis-cli -h ${node%:*} -p ${node#*:} INFO | grep -E "(used_memory|connected_clients|instantaneous_ops_per_sec)"
        done
        
        sleep 60
    done
}
```

## 6. ç³»ç»Ÿæ€§è°ƒä¼˜æ–¹æ³•

### 6.1 è°ƒä¼˜ç”Ÿå‘½å‘¨æœŸç®¡ç†

#### å®Œæ•´è°ƒä¼˜æµç¨‹
```python
# ç³»ç»Ÿæ€§è°ƒä¼˜ç®¡ç†å™¨
class ComprehensiveTuningManager:
    def __init__(self):
        self.tuning_phases = {
            'discovery': self.discovery_phase,
            'analysis': self.analysis_phase,
            'experimentation': self.experimentation_phase,
            'validation': self.validation_phase,
            'implementation': self.implementation_phase,
            'monitoring': self.monitoring_phase
        }
    
    def execute_complete_tuning_cycle(self, database_config):
        """æ‰§è¡Œå®Œæ•´è°ƒä¼˜å‘¨æœŸ"""
        results = {}
        
        for phase_name, phase_method in self.tuning_phases.items():
            print(f"æ‰§è¡Œè°ƒä¼˜é˜¶æ®µ: {phase_name}")
            results[phase_name] = phase_method(database_config, results)
            
            # é˜¶æ®µé—´éªŒè¯
            if not self.validate_phase_success(phase_name, results[phase_name]):
                print(f"é˜¶æ®µ {phase_name} éªŒè¯å¤±è´¥ï¼Œåœæ­¢è°ƒä¼˜")
                break
        
        return results
    
    def discovery_phase(self, config, previous_results):
        """å‘ç°é˜¶æ®µ"""
        discovery_data = {
            'current_performance': self.measure_baseline_performance(config),
            'workload_characteristics': self.analyze_workload_patterns(config),
            'bottleneck_identification': self.identify_performance_bottlenecks(config),
            'configuration_audit': self.audit_current_configuration(config)
        }
        return discovery_data
    
    def analysis_phase(self, config, previous_results):
        """åˆ†æé˜¶æ®µ"""
        analysis_results = {
            'root_cause_analysis': self.analyze_root_causes(previous_results['discovery']),
            'parameter_impact_assessment': self.assess_parameter_impacts(config),
            'optimization_opportunities': self.identify_optimization_opportunities(previous_results['discovery']),
            'risk_assessment': self.assess_change_risks(config)
        }
        return analysis_results
```

### 6.2 A/Bæµ‹è¯•æ–¹æ³•

#### å‚æ•°å¯¹æ¯”æµ‹è¯•
```python
# A/Bæµ‹è¯•æ¡†æ¶
class ABTestingFramework:
    def __init__(self):
        self.test_groups = {}
        self.metrics_collector = MetricsCollector()
    
    def setup_ab_test(self, parameter_name, control_value, test_value, duration_hours=24):
        """è®¾ç½®A/Bæµ‹è¯•"""
        test_config = {
            'parameter': parameter_name,
            'control_group': {
                'value': control_value,
                'instances': self.select_control_instances()
            },
            'test_group': {
                'value': test_value,
                'instances': self.select_test_instances()
            },
            'duration': duration_hours,
            'metrics': self.define_test_metrics()
        }
        
        return self.execute_test(test_config)
    
    def execute_test(self, test_config):
        """æ‰§è¡Œæµ‹è¯•"""
        # åº”ç”¨å‚æ•°å˜æ›´
        self.apply_parameter_changes(test_config)
        
        # å¼€å§‹ç›‘æ§
        start_time = time.time()
        monitoring_results = []
        
        while (time.time() - start_time) < (test_config['duration'] * 3600):
            metrics = self.collect_test_metrics(test_config)
            monitoring_results.append(metrics)
            time.sleep(300)  # 5åˆ†é’Ÿé‡‡é›†ä¸€æ¬¡
        
        # åˆ†æç»“æœ
        analysis = self.analyze_test_results(monitoring_results, test_config)
        return analysis
    
    def analyze_test_results(self, results, config):
        """åˆ†ææµ‹è¯•ç»“æœ"""
        analysis = {
            'statistical_significance': self.calculate_significance(results),
            'performance_impact': self.calculate_performance_impact(results),
            'resource_utilization': self.compare_resource_usage(results),
            'recommendation': self.generate_recommendation(results, config)
        }
        return analysis
```

### 6.3 è‡ªåŠ¨åŒ–è°ƒä¼˜å·¥å…·

#### æ™ºèƒ½è°ƒä¼˜ç³»ç»Ÿ
```python
# æ™ºèƒ½å‚æ•°è°ƒä¼˜å™¨
class IntelligentTuner:
    def __init__(self, ml_model=None):
        self.ml_model = ml_model or self.load_default_model()
        self.parameter_knowledge_base = self.build_knowledge_base()
    
    def auto_tune_database(self, database_connection, target_metrics):
        """è‡ªåŠ¨è°ƒä¼˜æ•°æ®åº“"""
        # 1. æ”¶é›†å½“å‰çŠ¶æ€
        current_state = self.collect_database_state(database_connection)
        
        # 2. é¢„æµ‹æœ€ä¼˜å‚æ•°
        optimal_parameters = self.predict_optimal_parameters(
            current_state, 
            target_metrics
        )
        
        # 3. ç”Ÿæˆè°ƒä¼˜è®¡åˆ’
        tuning_plan = self.generate_tuning_plan(
            current_state['parameters'],
            optimal_parameters
        )
        
        # 4. æ‰§è¡Œè°ƒä¼˜
        execution_results = self.execute_tuning_plan(
            database_connection,
            tuning_plan
        )
        
        # 5. éªŒè¯æ•ˆæœ
        validation_results = self.validate_tuning_results(
            database_connection,
            target_metrics,
            execution_results
        )
        
        return {
            'plan': tuning_plan,
            'results': execution_results,
            'validation': validation_results
        }
    
    def predict_optimal_parameters(self, current_state, targets):
        """é¢„æµ‹æœ€ä¼˜å‚æ•°é…ç½®"""
        # ä½¿ç”¨æœºå™¨å­¦ä¹ æ¨¡å‹é¢„æµ‹
        features = self.extract_features(current_state)
        predictions = self.ml_model.predict(features)
        
        # ç»“åˆä¸šåŠ¡çº¦æŸè°ƒæ•´
        constrained_predictions = self.apply_business_constraints(
            predictions, 
            targets, 
            current_state
        )
        
        return constrained_predictions
```

## 7. è°ƒä¼˜æ•ˆæœéªŒè¯

### 7.1 åŸºå‡†æµ‹è¯•æ¡†æ¶

#### æ€§èƒ½åŸºå‡†å»ºç«‹
```python
# åŸºå‡†æµ‹è¯•ç®¡ç†å™¨
class BenchmarkManager:
    def __init__(self):
        self.benchmark_tools = {
            'sysbench': self.run_sysbench_test,
            'tpcc': self.run_tpcc_test,
            'custom': self.run_custom_workload
        }
    
    def establish_performance_baseline(self, database_config, test_duration=300):
        """å»ºç«‹æ€§èƒ½åŸºå‡†"""
        baseline_results = {}
        
        # è¿è¡Œå¤šç§åŸºå‡†æµ‹è¯•
        for test_name, test_runner in self.benchmark_tools.items():
            print(f"è¿è¡ŒåŸºå‡†æµ‹è¯•: {test_name}")
            baseline_results[test_name] = test_runner(
                database_config, 
                test_duration
            )
        
        # è®¡ç®—ç»¼åˆè¯„åˆ†
        baseline_results['composite_score'] = self.calculate_composite_score(
            baseline_results
        )
        
        return baseline_results
    
    def run_sysbench_test(self, config, duration):
        """è¿è¡ŒSysBenchæµ‹è¯•"""
        # CPUæ€§èƒ½æµ‹è¯•
        cpu_test = subprocess.run([
            'sysbench', '--test=cpu', '--cpu-max-prime=20000', 'run'
        ], capture_output=True, text=True)
        
        # å†…å­˜æ€§èƒ½æµ‹è¯•
        memory_test = subprocess.run([
            'sysbench', '--test=memory', '--memory-block-size=1K',
            '--memory-total-size=100G', 'run'
        ], capture_output=True, text=True)
        
        # æ•°æ®åº“OLTPæµ‹è¯•
        oltp_test = subprocess.run([
            'sysbench', '--test=oltp', '--db-driver=mysql',
            f"--mysql-host={config['host']}",
            f"--mysql-port={config['port']}",
            f"--mysql-user={config['user']}",
            f"--mysql-password={config['password']}",
            '--oltp-table-size=1000000',
            f'--max-time={duration}',
            '--max-requests=0',
            'run'
        ], capture_output=True, text=True)
        
        return {
            'cpu_results': self.parse_sysbench_output(cpu_test.stdout),
            'memory_results': self.parse_sysbench_output(memory_test.stdout),
            'oltp_results': self.parse_sysbench_output(oltp_test.stdout)
        }
```

### 7.2 æ•ˆæœå¯¹æ¯”åˆ†æ

#### è°ƒä¼˜å‰åå¯¹æ¯”
```python
# è°ƒä¼˜æ•ˆæœåˆ†æå™¨
class TuningEffectAnalyzer:
    def __init__(self):
        self.metrics_analyzer = MetricsAnalyzer()
    
    def compare_before_after(self, before_metrics, after_metrics):
        """å¯¹æ¯”è°ƒä¼˜å‰åæ•ˆæœ"""
        comparison = {}
        
        # å…³é”®æŒ‡æ ‡å¯¹æ¯”
        key_metrics = [
            'response_time_avg',
            'throughput_tps',
            'cpu_utilization',
            'memory_utilization',
            'disk_io_iops'
        ]
        
        for metric in key_metrics:
            before_value = before_metrics.get(metric, 0)
            after_value = after_metrics.get(metric, 0)
            
            improvement = self.calculate_improvement(before_value, after_value)
            statistical_significance = self.test_statistical_significance(
                before_metrics.get(f'{metric}_samples', []),
                after_metrics.get(f'{metric}_samples', [])
            )
            
            comparison[metric] = {
                'before': before_value,
                'after': after_value,
                'improvement': improvement,
                'significance': statistical_significance,
                'confidence_interval': self.calculate_confidence_interval(
                    before_metrics.get(f'{metric}_samples', []),
                    after_metrics.get(f'{metric}_samples', [])
                )
            }
        
        # ç»¼åˆè¯„ä¼°
        comparison['overall_assessment'] = self.assess_overall_improvement(comparison)
        
        return comparison
    
    def calculate_improvement(self, before, after):
        """è®¡ç®—æ”¹è¿›å¹…åº¦"""
        if before == 0:
            return float('inf') if after > 0 else 0
        
        if isinstance(before, (int, float)) and isinstance(after, (int, float)):
            return ((before - after) / before) * 100
        return 0
```

### 7.3 æŒç»­ç›‘æ§ä½“ç³»

#### é•¿æœŸæ•ˆæœè·Ÿè¸ª
```python
# æŒç»­ç›‘æ§ç³»ç»Ÿ
class ContinuousMonitoringSystem:
    def __init__(self):
        self.monitoring_rules = self.load_monitoring_rules()
        self.alerting_system = AlertingSystem()
        self.trend_analyzer = TrendAnalyzer()
    
    def setup_continuous_monitoring(self, database_config):
        """è®¾ç½®æŒç»­ç›‘æ§"""
        monitoring_config = {
            'collection_interval': 60,  # 1åˆ†é’Ÿé‡‡é›†ä¸€æ¬¡
            'retention_period': 30,     # ä¿ç•™30å¤©æ•°æ®
            'alerting_rules': self.monitoring_rules,
            'dashboard_config': self.setup_dashboard()
        }
        
        return self.start_monitoring(database_config, monitoring_config)
    
    def detect_performance_degradation(self, current_metrics, baseline_metrics):
        """æ£€æµ‹æ€§èƒ½é€€åŒ–"""
        degradation_alerts = []
        
        # æ£€æŸ¥å„é¡¹æŒ‡æ ‡æ˜¯å¦è¶…å‡ºé˜ˆå€¼
        thresholds = {
            'response_time': 1.2,      # æ¯”åŸºçº¿æ…¢20%
            'error_rate': 0.05,        # é”™è¯¯ç‡è¶…è¿‡5%
            'throughput': 0.8,         # ååé‡ä½äº80%
            'resource_utilization': 0.95  # èµ„æºä½¿ç”¨ç‡è¶…è¿‡95%
        }
        
        for metric, threshold in thresholds.items():
            current_value = current_metrics.get(metric, 0)
            baseline_value = baseline_metrics.get(metric, 0)
            
            if self.is_degraded(current_value, baseline_value, threshold):
                alert = {
                    'metric': metric,
                    'current_value': current_value,
                    'baseline_value': baseline_value,
                    'degradation_ratio': current_value / baseline_value,
                    'severity': self.calculate_severity(current_value, baseline_value, threshold)
                }
                degradation_alerts.append(alert)
        
        if degradation_alerts:
            self.alerting_system.send_alerts(degradation_alerts)
        
        return degradation_alerts
    
    def generate_tuning_recommendations(self, monitoring_data):
        """åŸºäºç›‘æ§æ•°æ®ç”Ÿæˆè°ƒä¼˜å»ºè®®"""
        trends = self.trend_analyzer.analyze_trends(monitoring_data)
        recommendations = []
        
        # åŸºäºè¶‹åŠ¿ç”Ÿæˆå»ºè®®
        for trend_name, trend_data in trends.items():
            if trend_data['direction'] == 'degrading':
                recommendation = self.generate_recommendation_for_trend(
                    trend_name, 
                    trend_data
                )
                recommendations.append(recommendation)
        
        return recommendations
```

---

## ğŸ” å…³é”®è¦ç‚¹æ€»ç»“

### âœ… è°ƒä¼˜æˆåŠŸè¦ç´ 
- **æ•°æ®é©±åŠ¨å†³ç­–**ï¼šåŸºäºå®é™…æµ‹é‡æ•°æ®è€ŒéçŒœæµ‹è¿›è¡Œè°ƒä¼˜
- **æ¸è¿›å¼æ”¹è¿›**ï¼šå°æ­¥å¿«è·‘ï¼Œé¿å…å¤§è§„æ¨¡åŒæ—¶å˜æ›´
- **å……åˆ†æµ‹è¯•éªŒè¯**ï¼šåœ¨ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²å‰å……åˆ†éªŒè¯æ•ˆæœ
- **æŒç»­ç›‘æ§åé¦ˆ**ï¼šå»ºç«‹é•¿æœŸç›‘æ§æœºåˆ¶è·Ÿè¸ªè°ƒä¼˜æ•ˆæœ

### âš ï¸ å¸¸è§é™·é˜±
- **è¿‡åº¦è°ƒä¼˜**ï¼šå¯èƒ½å¯¼è‡´ç³»ç»Ÿä¸ç¨³å®šæˆ–ç»´æŠ¤å›°éš¾
- **å¿½ç•¥ä¸šåŠ¡å½±å“**ï¼šçº¯ç²¹çš„æŠ€æœ¯æŒ‡æ ‡å¯èƒ½ä¸ç¬¦åˆä¸šåŠ¡éœ€æ±‚
- **ç¼ºä¹å›æ»šè®¡åˆ’**ï¼šå‚æ•°å˜æ›´åº”è¯¥å¯é€†ä¸”æœ‰åº”æ€¥é¢„æ¡ˆ
- **å¿½è§†ç¯å¢ƒå·®å¼‚**ï¼šä¸åŒç¯å¢ƒä¸‹çš„æœ€ä¼˜å‚æ•°å¯èƒ½ä¸åŒ

### ğŸ¯ æœ€ä½³å®è·µå»ºè®®
1. **å»ºç«‹åŸºå‡†æµ‹è¯•**ï¼šè°ƒä¼˜å‰å»ºç«‹å¯é çš„æ€§èƒ½åŸºå‡†
2. **æ–‡æ¡£åŒ–æ‰€æœ‰å˜æ›´**ï¼šè¯¦ç»†è®°å½•å‚æ•°å˜æ›´åŠå…¶æ•ˆæœ
3. **åˆ†é˜¶æ®µå®æ–½**ï¼šé‡è¦çš„è°ƒä¼˜åˆ†å¤šä¸ªé˜¶æ®µé€æ­¥å®æ–½
4. **å»ºç«‹ç›‘æ§å‘Šè­¦**ï¼šå®æ—¶ç›‘æ§å…³é”®æŒ‡æ ‡å˜åŒ–
5. **å®šæœŸå›é¡¾ä¼˜åŒ–**ï¼šå®šæœŸè¯„ä¼°ç°æœ‰é…ç½®çš„æœ‰æ•ˆæ€§

é€šè¿‡ç§‘å­¦çš„å‚æ•°è°ƒä¼˜æ–¹æ³•ï¼Œå¯ä»¥åœ¨ä¸å¢åŠ ç¡¬ä»¶æŠ•å…¥çš„æƒ…å†µä¸‹æ˜¾è‘—æå‡æ•°æ®åº“æ€§èƒ½ï¼Œä¸ºä¼ä¸šåˆ›é€ æ›´å¤§çš„ä¸šåŠ¡ä»·å€¼ã€‚