# æ•°æ®åº“è‡ªåŠ¨æ‰©ç¼©å®¹é…ç½®å®Œæ•´æŒ‡å—

## ğŸ¯ æ¦‚è¿°

æ•°æ®åº“è‡ªåŠ¨æ‰©ç¼©å®¹æ˜¯ç°ä»£äº‘åŸç”Ÿåº”ç”¨å®ç°å¼¹æ€§ä¼¸ç¼©çš„å…³é”®æŠ€æœ¯ï¼Œé€šè¿‡æ™ºèƒ½åŒ–çš„èµ„æºè°ƒé…æœºåˆ¶åº”å¯¹æµé‡æ³¢åŠ¨å’Œæ€§èƒ½éœ€æ±‚å˜åŒ–ã€‚æœ¬æŒ‡å—æ·±å…¥æ¢è®¨æ°´å¹³æ‰©ç¼©å®¹å’Œå‚ç›´æ‰©ç¼©å®¹çš„æŠ€æœ¯å®ç°ï¼Œæä¾›ä»Kubernetesåˆ°äº‘æœåŠ¡å•†çš„å®Œæ•´è‡ªåŠ¨æ‰©ç¼©å®¹è§£å†³æ–¹æ¡ˆã€‚

## ğŸ“‹ ç›®å½•

1. [è‡ªåŠ¨æ‰©ç¼©å®¹åŸºç¡€ç†è®º](#1-è‡ªåŠ¨æ‰©ç¼©å®¹åŸºç¡€ç†è®º)
2. [Kubernetes HPAé…ç½®](#2-kubernetes-hpaé…ç½®)
3. [äº‘æœåŠ¡å•†è‡ªåŠ¨æ‰©ç¼©å®¹](#3-äº‘æœåŠ¡å•†è‡ªåŠ¨æ‰©ç¼©å®¹)
4. [æ•°æ®åº“ä¸“ç”¨æ‰©ç¼©å®¹ç­–ç•¥](#4-æ•°æ®åº“ä¸“ç”¨æ‰©ç¼©å®¹ç­–ç•¥)
5. [æ‰©ç¼©å®¹ç›‘æ§ä¸å‘Šè­¦](#5-æ‰©ç¼©å®¹ç›‘æ§ä¸å‘Šè­¦)
6. [æœ€ä½³å®è·µä¸æ•…éšœå¤„ç†](#6-æœ€ä½³å®è·µä¸æ•…éšœå¤„ç†)

---

## 1. è‡ªåŠ¨æ‰©ç¼©å®¹åŸºç¡€ç†è®º

### 1.1 æ‰©ç¼©å®¹ç±»å‹ä¸åŸç†

#### æ‰©ç¼©å®¹æ¶æ„æ¨¡å¼
```mermaid
graph TD
    A[åº”ç”¨å±‚] --> B[è´Ÿè½½å‡è¡¡å™¨]
    B --> C[è‡ªåŠ¨æ‰©ç¼©å®¹æ§åˆ¶å™¨]
    
    C --> D[æ°´å¹³æ‰©ç¼©å®¹]
    C --> E[å‚ç›´æ‰©ç¼©å®¹]
    
    subgraph "æ°´å¹³æ‰©ç¼©å®¹(Horizontal Scaling)"
        D --> D1[æ–°å¢æ•°æ®åº“å®ä¾‹]
        D --> D2[ç§»é™¤æ•°æ®åº“å®ä¾‹]
        D1 --> D3[æ•°æ®åˆ†ç‰‡]
        D1 --> D4[è¯»å†™åˆ†ç¦»]
        D2 --> D5[è¿æ¥è¿ç§»]
    end
    
    subgraph "å‚ç›´æ‰©ç¼©å®¹(Vertical Scaling)"
        E --> E1[å®ä¾‹è§„æ ¼è°ƒæ•´]
        E --> E2[å­˜å‚¨å®¹é‡æ‰©å±•]
        E1 --> E3[CPU/Memoryå‡çº§]
        E1 --> E4[IOæ€§èƒ½æå‡]
        E2 --> E5[å­˜å‚¨æ‰©å®¹]
    end
    
    subgraph "ç›‘æ§æŒ‡æ ‡å±‚"
        F[CPUä½¿ç”¨ç‡]
        G[å†…å­˜ä½¿ç”¨ç‡]
        H[è¿æ¥æ•°]
        I[æŸ¥è¯¢QPS]
        J[å­˜å‚¨ä½¿ç”¨ç‡]
    end
    
    F --> C
    G --> C
    H --> C
    I --> C
    J --> C
```

#### æ‰©ç¼©å®¹å†³ç­–çŸ©é˜µ
```python
# auto_scaling_decision_engine.py
import numpy as np
from typing import Dict, List, Tuple, Any
from dataclasses import dataclass
from enum import Enum

class ScalingType(Enum):
    HORIZONTAL_UP = "horizontal_up"
    HORIZONTAL_DOWN = "horizontal_down"
    VERTICAL_UP = "vertical_up"
    VERTICAL_DOWN = "vertical_down"
    NO_SCALING = "no_scaling"

@dataclass
class ScalingMetrics:
    cpu_utilization: float
    memory_utilization: float
    connection_count: int
    query_per_second: float
    storage_utilization: float
    response_time: float

@dataclass
class ScalingDecision:
    scaling_type: ScalingType
    confidence: float
    reason: str
    recommended_action: str

class AutoScalingDecisionEngine:
    def __init__(self):
        # æ‰©ç¼©å®¹é˜ˆå€¼é…ç½®
        self.thresholds = {
            'cpu_high': 0.75,
            'cpu_critical': 0.85,
            'memory_high': 0.80,
            'memory_critical': 0.90,
            'connections_high': 0.70,
            'qps_high': 0.80,
            'storage_high': 0.85,
            'response_time_high': 2.0,  # ç§’
            'cooldown_period': 300  # 5åˆ†é’Ÿå†·å´æœŸ
        }
        
        self.scaling_history = []
        self.last_scaling_time = 0
    
    def make_scaling_decision(self, current_metrics: ScalingMetrics, 
                            current_resources: Dict[str, Any]) -> ScalingDecision:
        """åŸºäºå½“å‰æŒ‡æ ‡åšå‡ºæ‰©ç¼©å®¹å†³ç­–"""
        
        # æ£€æŸ¥å†·å´æœŸ
        import time
        current_time = time.time()
        if current_time - self.last_scaling_time < self.thresholds['cooldown_period']:
            return ScalingDecision(
                scaling_type=ScalingType.NO_SCALING,
                confidence=1.0,
                reason="å†·å´æœŸå†…ï¼Œæš‚ä¸æ‰§è¡Œæ‰©ç¼©å®¹",
                recommended_action="ç»´æŒå½“å‰èµ„æºé…ç½®"
            )
        
        # å¤šç»´åº¦æŒ‡æ ‡åˆ†æ
        scaling_factors = self._analyze_scaling_factors(current_metrics, current_resources)
        
        # ç»¼åˆå†³ç­–
        decision = self._make_comprehensive_decision(scaling_factors, current_resources)
        
        # è®°å½•å†³ç­–å†å²
        self.scaling_history.append({
            'timestamp': current_time,
            'metrics': current_metrics.__dict__,
            'decision': decision.__dict__
        })
        
        return decision
    
    def _analyze_scaling_factors(self, metrics: ScalingMetrics, 
                               resources: Dict[str, Any]) -> Dict[str, float]:
        """åˆ†æå„ä¸ªç»´åº¦çš„æ‰©ç¼©å®¹å› å­"""
        factors = {}
        
        # CPUä½¿ç”¨ç‡å› å­
        if metrics.cpu_utilization > self.thresholds['cpu_critical']:
            factors['cpu'] = 1.0
        elif metrics.cpu_utilization > self.thresholds['cpu_high']:
            factors['cpu'] = 0.7
        else:
            factors['cpu'] = 0.0
        
        # å†…å­˜ä½¿ç”¨ç‡å› å­
        if metrics.memory_utilization > self.thresholds['memory_critical']:
            factors['memory'] = 1.0
        elif metrics.memory_utilization > self.thresholds['memory_high']:
            factors['memory'] = 0.7
        else:
            factors['memory'] = 0.0
        
        # è¿æ¥æ•°å› å­
        max_connections = resources.get('max_connections', 1000)
        connection_ratio = metrics.connection_count / max_connections
        if connection_ratio > self.thresholds['connections_high']:
            factors['connections'] = 0.8
        else:
            factors['connections'] = 0.0
        
        # QPSå› å­
        max_qps = resources.get('max_qps', 10000)
        qps_ratio = metrics.query_per_second / max_qps
        if qps_ratio > self.thresholds['qps_high']:
            factors['qps'] = 0.9
        else:
            factors['qps'] = 0.0
        
        # å­˜å‚¨ä½¿ç”¨ç‡å› å­
        if metrics.storage_utilization > self.thresholds['storage_high']:
            factors['storage'] = 0.6
        else:
            factors['storage'] = 0.0
        
        # å“åº”æ—¶é—´å› å­
        if metrics.response_time > self.thresholds['response_time_high']:
            factors['response_time'] = 0.8
        else:
            factors['response_time'] = 0.0
        
        return factors
    
    def _make_comprehensive_decision(self, factors: Dict[str, float], 
                                   resources: Dict[str, Any]) -> ScalingDecision:
        """ç»¼åˆå¤šä¸ªå› å­åšå‡ºæœ€ç»ˆå†³ç­–"""
        
        # è®¡ç®—ç»¼åˆè¯„åˆ†
        total_score = sum(factors.values())
        max_possible_score = len(factors)  # æœ€å¤§å¯èƒ½å¾—åˆ†ä¸ºå› å­æ•°é‡
        
        # å½’ä¸€åŒ–è¯„åˆ†
        normalized_score = total_score / max_possible_score if max_possible_score > 0 else 0
        
        # ç¡®å®šæ‰©ç¼©å®¹ç±»å‹
        if normalized_score > 0.7:  # é«˜å‹åŠ›
            # æ£€æŸ¥æ˜¯å¦æ”¯æŒæ°´å¹³æ‰©å±•
            if resources.get('supports_horizontal_scaling', False) and factors.get('qps', 0) > 0.8:
                scaling_type = ScalingType.HORIZONTAL_UP
                reason = "é«˜è´Ÿè½½ä¸”æ”¯æŒæ°´å¹³æ‰©å±•ï¼Œå»ºè®®å¢åŠ å®ä¾‹"
            else:
                scaling_type = ScalingType.VERTICAL_UP
                reason = "é«˜è´Ÿè½½ä½†ä¸æ”¯æŒæ°´å¹³æ‰©å±•ï¼Œå»ºè®®å‡çº§å®ä¾‹è§„æ ¼"
                
        elif normalized_score > 0.4:  # ä¸­ç­‰å‹åŠ›
            # æ£€æŸ¥æ˜¯å¦æœ‰ç¼©å‡ç©ºé—´
            if (resources.get('current_replicas', 1) > resources.get('min_replicas', 1) and 
                factors.get('qps', 0) < 0.3):
                scaling_type = ScalingType.HORIZONTAL_DOWN
                reason = "è´Ÿè½½è¾ƒä½ä¸”å®ä¾‹æ•°è¶…è¿‡æœ€å°å€¼ï¼Œå»ºè®®å‡å°‘å®ä¾‹"
            elif (resources.get('current_spec', '') != resources.get('min_spec', '') and
                  max(factors.values()) < 0.3):
                scaling_type = ScalingType.VERTICAL_DOWN
                reason = "è´Ÿè½½è¾ƒä½ä¸”è§„æ ¼é«˜äºæœ€å°å€¼ï¼Œå»ºè®®é™çº§å®ä¾‹"
            else:
                scaling_type = ScalingType.NO_SCALING
                reason = "è´Ÿè½½é€‚ä¸­ï¼Œæ— éœ€æ‰©ç¼©å®¹"
        else:  # ä½å‹åŠ›
            scaling_type = ScalingType.NO_SCALING
            reason = "è´Ÿè½½è¾ƒä½ï¼Œç»´æŒå½“å‰é…ç½®"
        
        # è®¡ç®—ç½®ä¿¡åº¦
        confidence = min(1.0, normalized_score + 0.2)  # åŸºç¡€ç½®ä¿¡åº¦åŠ ä¸Šä¸€äº›ä½™é‡
        
        # ç”Ÿæˆæ¨èåŠ¨ä½œ
        recommended_action = self._generate_recommended_action(scaling_type, factors, resources)
        
        return ScalingDecision(
            scaling_type=scaling_type,
            confidence=confidence,
            reason=reason,
            recommended_action=recommended_action
        )
    
    def _generate_recommended_action(self, scaling_type: ScalingType, 
                                   factors: Dict[str, float], 
                                   resources: Dict[str, Any]) -> str:
        """ç”Ÿæˆå…·ä½“çš„æ¨èåŠ¨ä½œ"""
        
        if scaling_type == ScalingType.HORIZONTAL_UP:
            current_replicas = resources.get('current_replicas', 1)
            max_replicas = resources.get('max_replicas', 10)
            increment = min(2, max_replicas - current_replicas)  # æ¯æ¬¡æœ€å¤šå¢åŠ 2ä¸ªå®ä¾‹
            return f"å°†å®ä¾‹æ•°ä» {current_replicas} å¢åŠ åˆ° {current_replicas + increment}"
        
        elif scaling_type == ScalingType.HORIZONTAL_DOWN:
            current_replicas = resources.get('current_replicas', 1)
            min_replicas = resources.get('min_replicas', 1)
            decrement = min(1, current_replicas - min_replicas)  # æ¯æ¬¡æœ€å¤šå‡å°‘1ä¸ªå®ä¾‹
            return f"å°†å®ä¾‹æ•°ä» {current_replicas} å‡å°‘åˆ° {current_replicas - decrement}"
        
        elif scaling_type == ScalingType.VERTICAL_UP:
            current_spec = resources.get('current_spec', 'small')
            upgrade_options = {
                'small': 'medium',
                'medium': 'large',
                'large': 'xlarge'
            }
            new_spec = upgrade_options.get(current_spec, current_spec)
            return f"å°†å®ä¾‹è§„æ ¼ä» {current_spec} å‡çº§åˆ° {new_spec}"
        
        elif scaling_type == ScalingType.VERTICAL_DOWN:
            current_spec = resources.get('current_spec', 'small')
            downgrade_options = {
                'xlarge': 'large',
                'large': 'medium',
                'medium': 'small'
            }
            new_spec = downgrade_options.get(current_spec, current_spec)
            return f"å°†å®ä¾‹è§„æ ¼ä» {current_spec} é™çº§åˆ° {new_spec}"
        
        else:
            return "ç»´æŒå½“å‰èµ„æºé…ç½®ä¸å˜"
    
    def predict_future_scaling_needs(self, historical_data: List[Dict]) -> List[Dict[str, Any]]:
        """é¢„æµ‹æœªæ¥çš„æ‰©ç¼©å®¹éœ€æ±‚"""
        predictions = []
        
        # ç®€å•çš„æ—¶é—´åºåˆ—é¢„æµ‹
        if len(historical_data) >= 24:  # è‡³å°‘éœ€è¦24å°æ—¶çš„æ•°æ®
            # æŒ‰å°æ—¶èšåˆæ•°æ®
            hourly_stats = self._aggregate_hourly_stats(historical_data)
            
            # é¢„æµ‹æœªæ¥24å°æ—¶çš„éœ€æ±‚
            for hour in range(24):
                predicted_metrics = self._predict_hourly_metrics(hourly_stats, hour)
                current_resources = {}  # å½“å‰èµ„æºé…ç½®ä¿¡æ¯
                decision = self.make_scaling_decision(predicted_metrics, current_resources)
                
                predictions.append({
                    'hour': hour,
                    'predicted_metrics': predicted_metrics.__dict__,
                    'scaling_decision': decision.__dict__
                })
        
        return predictions
    
    def _aggregate_hourly_stats(self, historical_data: List[Dict]) -> Dict[int, List[ScalingMetrics]]:
        """æŒ‰å°æ—¶èšåˆå†å²ç»Ÿè®¡æ•°æ®"""
        hourly_stats = {}
        
        for record in historical_data:
            hour = record['timestamp'].hour
            metrics = ScalingMetrics(**record['metrics'])
            
            if hour not in hourly_stats:
                hourly_stats[hour] = []
            hourly_stats[hour].append(metrics)
        
        return hourly_stats
    
    def _predict_hourly_metrics(self, hourly_stats: Dict[int, List[ScalingMetrics]], 
                              target_hour: int) -> ScalingMetrics:
        """é¢„æµ‹æŒ‡å®šå°æ—¶çš„æŒ‡æ ‡"""
        # ç®€å•çš„ç§»åŠ¨å¹³å‡é¢„æµ‹
        recent_hours = [(target_hour - i) % 24 for i in range(1, 8)]  # æœ€è¿‘7ä¸ªå°æ—¶
        
        predictions = {
            'cpu_utilization': [],
            'memory_utilization': [],
            'connection_count': [],
            'query_per_second': [],
            'storage_utilization': [],
            'response_time': []
        }
        
        for hour in recent_hours:
            if hour in hourly_stats and hourly_stats[hour]:
                # å–è¯¥å°æ—¶çš„å¹³å‡å€¼
                hour_metrics = hourly_stats[hour]
                avg_cpu = np.mean([m.cpu_utilization for m in hour_metrics])
                avg_memory = np.mean([m.memory_utilization for m in hour_metrics])
                avg_connections = np.mean([m.connection_count for m in hour_metrics])
                avg_qps = np.mean([m.query_per_second for m in hour_metrics])
                avg_storage = np.mean([m.storage_utilization for m in hour_metrics])
                avg_response = np.mean([m.response_time for m in hour_metrics])
                
                predictions['cpu_utilization'].append(avg_cpu)
                predictions['memory_utilization'].append(avg_memory)
                predictions['connection_count'].append(avg_connections)
                predictions['query_per_second'].append(avg_qps)
                predictions['storage_utilization'].append(avg_storage)
                predictions['response_time'].append(avg_response)
        
        # è¿”å›é¢„æµ‹çš„å¹³å‡å€¼
        return ScalingMetrics(
            cpu_utilization=np.mean(predictions['cpu_utilization']) if predictions['cpu_utilization'] else 0.5,
            memory_utilization=np.mean(predictions['memory_utilization']) if predictions['memory_utilization'] else 0.5,
            connection_count=int(np.mean(predictions['connection_count'])) if predictions['connection_count'] else 100,
            query_per_second=int(np.mean(predictions['query_per_second'])) if predictions['query_per_second'] else 1000,
            storage_utilization=np.mean(predictions['storage_utilization']) if predictions['storage_utilization'] else 0.5,
            response_time=np.mean(predictions['response_time']) if predictions['response_time'] else 1.0
        )

# ä½¿ç”¨ç¤ºä¾‹
engine = AutoScalingDecisionEngine()

# æ¨¡æ‹Ÿå½“å‰æŒ‡æ ‡
current_metrics = ScalingMetrics(
    cpu_utilization=0.82,
    memory_utilization=0.75,
    connection_count=800,
    query_per_second=5000,
    storage_utilization=0.60,
    response_time=1.5
)

# å½“å‰èµ„æºé…ç½®
current_resources = {
    'current_replicas': 3,
    'max_replicas': 10,
    'min_replicas': 2,
    'current_spec': 'medium',
    'max_connections': 1000,
    'max_qps': 10000,
    'supports_horizontal_scaling': True
}

# åšå‡ºæ‰©ç¼©å®¹å†³ç­–
decision = engine.make_scaling_decision(current_metrics, current_resources)
print(f"æ‰©ç¼©å®¹å†³ç­–: {decision.scaling_type.value}")
print(f"ç½®ä¿¡åº¦: {decision.confidence:.2f}")
print(f"åŸå› : {decision.reason}")
print(f"æ¨èåŠ¨ä½œ: {decision.recommended_action}")
```

### 1.2 æ‰©ç¼©å®¹ç­–ç•¥æ¨¡å‹

#### åŠ¨æ€æ‰©ç¼©å®¹ç­–ç•¥
```yaml
# auto-scaling-strategy.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: database-autoscaler
  namespace: database-system
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: StatefulSet
    name: database-cluster
  minReplicas: 2
  maxReplicas: 10
  
  # å¤šæŒ‡æ ‡æ‰©ç¼©å®¹ç­–ç•¥
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  
  - type: Pods
    pods:
      metric:
        name: database_connections
      target:
        type: AverageValue
        averageValue: "500"
  
  - type: External
    external:
      metric:
        name: database_qps
      target:
        type: AverageValue
        averageValue: "2000"
  
  # è¡Œä¸ºé…ç½®
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 10
        periodSeconds: 60
      - type: Pods
        value: 1
        periodSeconds: 60
      selectPolicy: Min
    
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60
      - type: Pods
        value: 2
        periodSeconds: 60
      selectPolicy: Max
```

## 2. Kubernetes HPAé…ç½®

### 2.1 HPAåŸºç¡€é…ç½®

#### æ•°æ®åº“HPAé…ç½®æ¨¡æ¿
```yaml
# database-hpa.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: mysql-hpa
  namespace: database
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: StatefulSet
    name: mysql-cluster
  minReplicas: 3
  maxReplicas: 15
  
  # å¤åˆæŒ‡æ ‡é…ç½®
  metrics:
  # CPUä½¿ç”¨ç‡æŒ‡æ ‡
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 75
  
  # å†…å­˜ä½¿ç”¨ç‡æŒ‡æ ‡
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 85
  
  # è‡ªå®šä¹‰æŒ‡æ ‡ - æ•°æ®åº“è¿æ¥æ•°
  - type: Pods
    pods:
      metric:
        name: database_connections
      target:
        type: AverageValue
        averageValue: "800"
  
  # è‡ªå®šä¹‰æŒ‡æ ‡ - æŸ¥è¯¢QPS
  - type: Pods
    pods:
      metric:
        name: database_qps
      target:
        type: AverageValue
        averageValue: "3000"
  
  # è‡ªå®šä¹‰æŒ‡æ ‡ - å“åº”æ—¶é—´
  - type: Pods
    pods:
      metric:
        name: database_response_time_ms
      target:
        type: AverageValue
        averageValue: "2000m"  # 2ç§’ï¼Œä½¿ç”¨milliå•ä½
  
  # å¤–éƒ¨æŒ‡æ ‡ - ä¸šåŠ¡æŒ‡æ ‡
  - type: External
    external:
      metric:
        name: business_transaction_rate
      target:
        type: AverageValue
        averageValue: "1000"

# é«˜çº§è¡Œä¸ºé…ç½®
behavior:
  # ç¼©å®¹ç­–ç•¥
  scaleDown:
    # ç¨³å®šæœŸçª—å£
    stabilizationWindowSeconds: 300
    
    # ç¼©å®¹ç­–ç•¥
    policies:
    # æ¯åˆ†é’Ÿæœ€å¤šç¼©å®¹10%
    - type: Percent
      value: 10
      periodSeconds: 60
    
    # æ¯åˆ†é’Ÿæœ€å¤šç¼©å®¹1ä¸ªPod
    - type: Pods
      value: 1
      periodSeconds: 60
    
    # é€‰æ‹©æœ€ä¿å®ˆçš„ç­–ç•¥
    selectPolicy: Min
  
  # æ‰©å®¹ç­–ç•¥
  scaleUp:
    # ç¨³å®šæœŸçª—å£
    stabilizationWindowSeconds: 60
    
    # æ‰©å®¹ç­–ç•¥
    policies:
    # æ¯åˆ†é’Ÿæœ€å¤šæ‰©å®¹50%
    - type: Percent
      value: 50
      periodSeconds: 60
    
    # æ¯åˆ†é’Ÿæœ€å¤šæ‰©å®¹2ä¸ªPod
    - type: Pods
      value: 2
      periodSeconds: 60
    
    # é€‰æ‹©æœ€æ¿€è¿›çš„ç­–ç•¥
    selectPolicy: Max
```

### 2.2 è‡ªå®šä¹‰æŒ‡æ ‡é€‚é…å™¨

#### PrometheusæŒ‡æ ‡é€‚é…å™¨é…ç½®
```yaml
# prometheus-adapter-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: adapter-config
  namespace: custom-metrics
data:
  config.yaml: |
    rules:
    # æ•°æ®åº“è¿æ¥æ•°æŒ‡æ ‡
    - seriesQuery: 'database_connections'
      resources:
        overrides:
          namespace:
            resource: namespace
          pod:
            resource: pod
      name:
        matches: "database_connections"
        as: "database_connections"
      metricsQuery: 'sum(database_connections{<<.LabelMatchers>>}) by (<<.GroupBy>>)'
    
    # æ•°æ®åº“QPSæŒ‡æ ‡
    - seriesQuery: 'database_qps'
      resources:
        overrides:
          namespace:
            resource: namespace
          pod:
            resource: pod
      name:
        matches: "database_qps"
        as: "database_qps"
      metricsQuery: 'sum(rate(database_qps{<<.LabelMatchers>>}[5m])) by (<<.GroupBy>>)'
    
    # æ•°æ®åº“å“åº”æ—¶é—´æŒ‡æ ‡
    - seriesQuery: 'database_response_time_seconds'
      resources:
        overrides:
          namespace:
            resource: namespace
          pod:
            resource: pod
      name:
        matches: "database_response_time_seconds"
        as: "database_response_time_seconds"
      metricsQuery: 'avg(database_response_time_seconds{<<.LabelMatchers>>}) by (<<.GroupBy>>)'
    
    # å­˜å‚¨ä½¿ç”¨ç‡æŒ‡æ ‡
    - seriesQuery: 'database_storage_utilization'
      resources:
        overrides:
          namespace:
            resource: namespace
          pod:
            resource: pod
      name:
        matches: "database_storage_utilization"
        as: "database_storage_utilization"
      metricsQuery: 'avg(database_storage_utilization{<<.LabelMatchers>>}) by (<<.GroupBy>>)'
```

#### è‡ªå®šä¹‰æŒ‡æ ‡é‡‡é›†å™¨
```python
# custom_metrics_collector.py
import time
import threading
from prometheus_client import Gauge, start_http_server
import pymysql
import psutil

class DatabaseMetricsCollector:
    def __init__(self, db_config: dict, port: int = 8000):
        self.db_config = db_config
        self.port = port
        
        # åˆå§‹åŒ–PrometheusæŒ‡æ ‡
        self.connections_gauge = Gauge(
            'database_connections',
            'Current number of database connections',
            ['database']
        )
        
        self.qps_gauge = Gauge(
            'database_qps',
            'Database queries per second',
            ['database']
        )
        
        self.response_time_gauge = Gauge(
            'database_response_time_seconds',
            'Average database response time in seconds',
            ['database']
        )
        
        self.storage_utilization_gauge = Gauge(
            'database_storage_utilization',
            'Database storage utilization ratio',
            ['database']
        )
        
        self.previous_queries = 0
        self.start_time = time.time()
        
    def collect_metrics(self):
        """æ”¶é›†æ•°æ®åº“æŒ‡æ ‡"""
        try:
            # å»ºç«‹æ•°æ®åº“è¿æ¥
            connection = pymysql.connect(**self.db_config)
            cursor = connection.cursor()
            
            # æ”¶é›†è¿æ¥æ•°
            cursor.execute("SHOW STATUS LIKE 'Threads_connected'")
            connections = int(cursor.fetchone()[1])
            self.connections_gauge.labels(database=self.db_config['database']).set(connections)
            
            # æ”¶é›†QPS
            cursor.execute("SHOW STATUS LIKE 'Questions'")
            current_queries = int(cursor.fetchone()[1])
            current_time = time.time()
            
            if hasattr(self, 'previous_queries'):
                time_delta = current_time - self.start_time
                qps = (current_queries - self.previous_queries) / time_delta if time_delta > 0 else 0
                self.qps_gauge.labels(database=self.db_config['database']).set(qps)
            
            self.previous_queries = current_queries
            self.start_time = current_time
            
            # æ”¶é›†å“åº”æ—¶é—´ï¼ˆç®€åŒ–å®ç°ï¼‰
            start_query = time.time()
            cursor.execute("SELECT 1")
            cursor.fetchone()
            response_time = time.time() - start_query
            self.response_time_gauge.labels(database=self.db_config['database']).set(response_time)
            
            # æ”¶é›†å­˜å‚¨ä½¿ç”¨ç‡
            cursor.execute("SELECT table_schema, SUM(data_length + index_length) as size FROM information_schema.tables GROUP BY table_schema")
            schemas = cursor.fetchall()
            
            total_size = sum(size for _, size in schemas)
            # è¿™é‡Œéœ€è¦è·å–æ€»å­˜å‚¨ç©ºé—´ï¼Œç®€åŒ–å¤„ç†
            storage_utilization = min(1.0, total_size / (100 * 1024 * 1024 * 1024))  # å‡è®¾100GBæ€»ç©ºé—´
            self.storage_utilization_gauge.labels(database=self.db_config['database']).set(storage_utilization)
            
            cursor.close()
            connection.close()
            
        except Exception as e:
            print(f"æŒ‡æ ‡æ”¶é›†å¤±è´¥: {str(e)}")
    
    def start_collection(self, interval: int = 30):
        """å¯åŠ¨æŒ‡æ ‡æ”¶é›†"""
        # å¯åŠ¨Prometheus HTTPæœåŠ¡å™¨
        start_http_server(self.port)
        print(f"PrometheusæŒ‡æ ‡æœåŠ¡å™¨å¯åŠ¨åœ¨ç«¯å£ {self.port}")
        
        # å®šæœŸæ”¶é›†æŒ‡æ ‡
        def collect_loop():
            while True:
                self.collect_metrics()
                time.sleep(interval)
        
        collector_thread = threading.Thread(target=collect_loop, daemon=True)
        collector_thread.start()
        
        return collector_thread

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    db_config = {
        'host': 'localhost',
        'port': 3306,
        'user': 'monitor_user',
        'password: "${DB_PASSWORD}",
        'database': 'production_db'
    }
    
    collector = DatabaseMetricsCollector(db_config)
    collector.start_collection(interval=30)
    
    # ä¿æŒç¨‹åºè¿è¡Œ
    try:
        while True:
            time.sleep(1)
    except KeyboardInterrupt:
        print("æŒ‡æ ‡æ”¶é›†å™¨åœæ­¢")
```

## 3. äº‘æœåŠ¡å•†è‡ªåŠ¨æ‰©ç¼©å®¹

### 3.1 AWS RDSè‡ªåŠ¨æ‰©ç¼©å®¹

#### RDSè‡ªåŠ¨æ‰©ç¼©å®¹é…ç½®
```python
# aws_rds_autoscaling.py
import boto3
import json
from typing import Dict, List, Any
import time

class RDSAutoScaler:
    def __init__(self, region: str = 'us-east-1'):
        self.rds_client = boto3.client('rds', region_name=region)
        self.cloudwatch_client = boto3.client('cloudwatch', region_name=region)
        self.application_autoscaling_client = boto3.client('application-autoscaling', region_name=region)
    
    def setup_rds_autoscaling(self, db_instance_identifier: str, 
                            min_capacity: int = 2, max_capacity: int = 16) -> bool:
        """è®¾ç½®RDSè‡ªåŠ¨æ‰©ç¼©å®¹"""
        try:
            # æ³¨å†Œå¯æ‰©å±•ç›®æ ‡
            self._register_scalable_target(db_instance_identifier, min_capacity, max_capacity)
            
            # åˆ›å»ºæ‰©ç¼©å®¹ç­–ç•¥
            self._create_scaling_policies(db_instance_identifier)
            
            # é…ç½®CloudWatchå‘Šè­¦
            self._setup_cloudwatch_alarms(db_instance_identifier)
            
            return True
        except Exception as e:
            print(f"RDSè‡ªåŠ¨æ‰©ç¼©å®¹è®¾ç½®å¤±è´¥: {str(e)}")
            return False
    
    def _register_scalable_target(self, db_instance_identifier: str, 
                                min_capacity: int, max_capacity: int):
        """æ³¨å†Œå¯æ‰©å±•ç›®æ ‡"""
        response = self.application_autoscaling_client.register_scalable_target(
            ServiceNamespace='rds',
            ResourceId=f'db:{db_instance_identifier}',
            ScalableDimension='rds:cluster:ReadReplicaCount',
            MinCapacity=min_capacity,
            MaxCapacity=max_capacity
        )
        print(f"å¯æ‰©å±•ç›®æ ‡æ³¨å†ŒæˆåŠŸ: {response}")
    
    def _create_scaling_policies(self, db_instance_identifier: str):
        """åˆ›å»ºæ‰©ç¼©å®¹ç­–ç•¥"""
        # æ‰©å®¹ç­–ç•¥
        scale_out_policy = {
            'PolicyName': f'{db_instance_identifier}-scale-out',
            'ServiceNamespace': 'rds',
            'ResourceId': f'db:{db_instance_identifier}',
            'ScalableDimension': 'rds:cluster:ReadReplicaCount',
            'PolicyType': 'TargetTrackingScaling',
            'TargetTrackingScalingPolicyConfiguration': {
                'TargetValue': 70.0,  # CPUä½¿ç”¨ç‡ç›®æ ‡70%
                'PredefinedMetricSpecification': {
                    'PredefinedMetricType': 'RDSReaderAverageCPUUtilization'
                },
                'ScaleOutCooldown': 300,  # 5åˆ†é’Ÿæ‰©å®¹å†·å´æœŸ
                'ScaleInCooldown': 300,   # 5åˆ†é’Ÿç¼©å®¹å†·å´æœŸ
                'DisableScaleIn': False
            }
        }
        
        # å­˜å‚¨è‡ªåŠ¨æ‰©å±•ç­–ç•¥
        storage_scaling_policy = {
            'PolicyName': f'{db_instance_identifier}-storage-scaling',
            'ServiceNamespace': 'rds',
            'ResourceId': f'db:{db_instance_identifier}',
            'ScalableDimension': 'rds:cluster:Storage',
            'PolicyType': 'TargetTrackingScaling',
            'TargetTrackingScalingPolicyConfiguration': {
                'TargetValue': 80.0,  # å­˜å‚¨ä½¿ç”¨ç‡ç›®æ ‡80%
                'CustomizedMetricSpecification': {
                    'MetricName': 'FreeStorageSpace',
                    'Namespace': 'AWS/RDS',
                    'Dimensions': [
                        {
                            'Name': 'DBInstanceIdentifier',
                            'Value': db_instance_identifier
                        }
                    ],
                    'Statistic': 'Average',
                    'Unit': 'Bytes'
                },
                'ScaleOutCooldown': 600,  # 10åˆ†é’Ÿæ‰©å®¹å†·å´æœŸ
                'DisableScaleIn': True    # å­˜å‚¨åªèƒ½æ‰©å®¹ä¸èƒ½ç¼©å®¹
            }
        }
        
        # åº”ç”¨æ‰©ç¼©å®¹ç­–ç•¥
        self.application_autoscaling_client.put_scaling_policy(**scale_out_policy)
        self.application_autoscaling_client.put_scaling_policy(**storage_scaling_policy)
        print("æ‰©ç¼©å®¹ç­–ç•¥åˆ›å»ºæˆåŠŸ")
    
    def _setup_cloudwatch_alarms(self, db_instance_identifier: str):
        """è®¾ç½®CloudWatchå‘Šè­¦"""
        alarms_config = [
            {
                'AlarmName': f'{db_instance_identifier}-high-cpu-alarm',
                'AlarmDescription': 'High CPU utilization alarm',
                'MetricName': 'CPUUtilization',
                'Namespace': 'AWS/RDS',
                'Statistic': 'Average',
                'Period': 300,
                'EvaluationPeriods': 2,
                'Threshold': 80.0,
                'ComparisonOperator': 'GreaterThanThreshold',
                'Dimensions': [
                    {
                        'Name': 'DBInstanceIdentifier',
                        'Value': db_instance_identifier
                    }
                ]
            },
            {
                'AlarmName': f'{db_instance_identifier}-low-cpu-alarm',
                'AlarmDescription': 'Low CPU utilization alarm',
                'MetricName': 'CPUUtilization',
                'Namespace': 'AWS/RDS',
                'Statistic': 'Average',
                'Period': 300,
                'EvaluationPeriods': 10,
                'Threshold': 30.0,
                'ComparisonOperator': 'LessThanThreshold',
                'Dimensions': [
                    {
                        'Name': 'DBInstanceIdentifier',
                        'Value': db_instance_identifier
                    }
                ]
            }
        ]
        
        for alarm_config in alarms_config:
            self.cloudwatch_client.put_metric_alarm(**alarm_config)
        
        print("CloudWatchå‘Šè­¦è®¾ç½®æˆåŠŸ")
    
    def get_scaling_activities(self, db_instance_identifier: str, 
                             max_results: int = 50) -> List[Dict[str, Any]]:
        """è·å–æ‰©ç¼©å®¹æ´»åŠ¨å†å²"""
        response = self.application_autoscaling_client.describe_scaling_activities(
            ServiceNamespace='rds',
            ResourceId=f'db:{db_instance_identifier}',
            ScalableDimension='rds:cluster:ReadReplicaCount',
            MaxResults=max_results
        )
        return response.get('ScalingActivities', [])
    
    def adjust_scaling_boundaries(self, db_instance_identifier: str,
                                new_min_capacity: int, new_max_capacity: int) -> bool:
        """è°ƒæ•´æ‰©ç¼©å®¹è¾¹ç•Œ"""
        try:
            self.application_autoscaling_client.register_scalable_target(
                ServiceNamespace='rds',
                ResourceId=f'db:{db_instance_identifier}',
                ScalableDimension='rds:cluster:ReadReplicaCount',
                MinCapacity=new_min_capacity,
                MaxCapacity=new_max_capacity
            )
            print(f"æ‰©ç¼©å®¹è¾¹ç•Œè°ƒæ•´æˆåŠŸ: {new_min_capacity}-{new_max_capacity}")
            return True
        except Exception as e:
            print(f"æ‰©ç¼©å®¹è¾¹ç•Œè°ƒæ•´å¤±è´¥: {str(e)}")
            return False

# ä½¿ç”¨ç¤ºä¾‹
autoscaler = RDSAutoScaler(region='us-east-1')
success = autoscaler.setup_rds_autoscaling(
    db_instance_identifier='production-mysql-cluster',
    min_capacity=2,
    max_capacity=10
)

if success:
    activities = autoscaler.get_scaling_activities('production-mysql-cluster')
    print("æœ€è¿‘çš„æ‰©ç¼©å®¹æ´»åŠ¨:")
    for activity in activities:
        print(f"- {activity['StatusCode']}: {activity['Description']}")

# æ ¹æ®ä¸šåŠ¡éœ€æ±‚è°ƒæ•´æ‰©ç¼©å®¹è¾¹ç•Œ
autoscaler.adjust_scaling_boundaries(
    'production-mysql-cluster',
    new_min_capacity=3,
    new_max_capacity=15
)
```

### 3.2 Azureè‡ªåŠ¨æ‰©ç¼©å®¹é…ç½®

#### Azure Databaseè‡ªåŠ¨æ‰©ç¼©å®¹
```powershell
# azure_database_autoscaling.ps1
param(
    [string]$ResourceGroupName = "Production-RG",
    [string]$ServerName = "mysql-production",
    [string]$DatabaseName = "production-db"
)

# ç™»å½•Azure
Connect-AzAccount

# å¯ç”¨è‡ªåŠ¨è°ƒä¼˜
Set-AzMySqlConfiguration -ResourceGroupName $ResourceGroupName -ServerName $ServerName -Name performance_schema -Value ON

# é…ç½®è‡ªåŠ¨å¢é•¿å­˜å‚¨
Update-AzMySqlConfiguration -ResourceGroupName $ResourceGroupName -ServerName $ServerName -Name storage_autogrow -Value ENABLED

# è®¾ç½®è®¡ç®—å±‚è‡ªåŠ¨æ‰©ç¼©å®¹
$autoGrowConfig = @{
    StorageAutogrow = "Enabled"
    BackupRetentionDays = 7
    GeoRedundantBackup = "Disabled"
}

Set-AzMySqlConfiguration @autoGrowConfig -ResourceGroupName $ResourceGroupName -ServerName $ServerName

# åˆ›å»ºæŒ‡æ ‡å‘Šè­¦è§„åˆ™
$actionGroup = New-AzActionGroup -Name "DatabaseScalingActionGroup" -ShortName "DBScale" -ResourceGroupName $ResourceGroupName

# CPUä½¿ç”¨ç‡é«˜å‘Šè­¦
Add-AzMetricAlertRuleV2 -Name "HighCPUAlert" -ResourceGroupName $ResourceGroupName -TargetResourceId "/subscriptions/$((Get-AzContext).Subscription.Id)/resourceGroups/$ResourceGroupName/providers/Microsoft.DBforMySQL/servers/$ServerName" -Condition (New-AzMetricAlertRuleV2Criteria -MetricName "cpu_percent" -Operator GreaterThan -Threshold 75 -TimeAggregation Average) -WindowSize 00:05:00 -Frequency 00:01:00 -ActionGroup $actionGroup

# CPUä½¿ç”¨ç‡ä½å‘Šè­¦
Add-AzMetricAlertRuleV2 -Name "LowCPUAlert" -ResourceGroupName $ResourceGroupName -TargetResourceId "/subscriptions/$((Get-AzContext).Subscription.Id)/resourceGroups/$ResourceGroupName/providers/Microsoft.DBforMySQL/servers/$ServerName" -Condition (New-AzMetricAlertRuleV2Criteria -MetricName "cpu_percent" -Operator LessThan -Threshold 25 -TimeAggregation Average) -WindowSize 00:30:00 -Frequency 00:05:00 -ActionGroup $actionGroup

Write-Host "Azureæ•°æ®åº“è‡ªåŠ¨æ‰©ç¼©å®¹é…ç½®å®Œæˆ"
```

## 4. æ•°æ®åº“ä¸“ç”¨æ‰©ç¼©å®¹ç­–ç•¥

### 4.1 è¯»å†™åˆ†ç¦»æ‰©ç¼©å®¹

#### è¯»å†™åˆ†ç¦»è‡ªåŠ¨æ‰©ç¼©å®¹æ¶æ„
```mermaid
graph TD
    A[åº”ç”¨å±‚] --> B[è¯»å†™åˆ†ç¦»ä»£ç†]
    B --> C[å†™ä¸»åº“]
    B --> D[è¯»å‰¯æœ¬1]
    B --> E[è¯»å‰¯æœ¬2]
    B --> F[è¯»å‰¯æœ¬3]
    
    G[è‡ªåŠ¨æ‰©ç¼©å®¹æ§åˆ¶å™¨] --> H[ç›‘æ§æŒ‡æ ‡æ”¶é›†]
    H --> I[CPUä½¿ç”¨ç‡]
    H --> J[è¿æ¥æ•°]
    H --> K[æŸ¥è¯¢QPS]
    
    G --> L[æ‰©ç¼©å®¹å†³ç­–å¼•æ“]
    L --> M[æ°´å¹³æ‰©ç¼©å®¹]
    L --> N[å‚ç›´æ‰©ç¼©å®¹]
    
    M --> O[å¢åŠ è¯»å‰¯æœ¬]
    M --> P[å‡å°‘è¯»å‰¯æœ¬]
    N --> Q[å‡çº§å®ä¾‹è§„æ ¼]
    N --> R[é™çº§å®ä¾‹è§„æ ¼]
    
    O --> D
    O --> E
    O --> F
    P --> D
    P --> E
    Q --> C
    R --> C
```

#### è¯»å†™åˆ†ç¦»æ‰©ç¼©å®¹æ§åˆ¶å™¨
```python
# read_write_scaling_controller.py
import asyncio
import aiohttp
from typing import Dict, List, Any
import json

class ReadWriteScalingController:
    def __init__(self, proxy_endpoint: str, db_config: Dict[str, Any]):
        self.proxy_endpoint = proxy_endpoint
        self.db_config = db_config
        self.scaling_state = {
            'master_replicas': 1,
            'read_replicas': 2,
            'current_master_spec': 'medium',
            'scaling_in_progress': False
        }
        
    async def monitor_and_scale(self):
        """ç›‘æ§å¹¶æ‰§è¡Œæ‰©ç¼©å®¹"""
        while True:
            try:
                # æ”¶é›†ç›‘æ§æŒ‡æ ‡
                metrics = await self.collect_metrics()
                
                # åˆ†ææ˜¯å¦éœ€è¦æ‰©ç¼©å®¹
                scaling_decision = self.analyze_scaling_need(metrics)
                
                if scaling_decision['should_scale']:
                    await self.execute_scaling(scaling_decision)
                
                # ç­‰å¾…ä¸‹ä¸€ä¸ªç›‘æ§å‘¨æœŸ
                await asyncio.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
                
            except Exception as e:
                print(f"ç›‘æ§å¾ªç¯å‡ºé”™: {str(e)}")
                await asyncio.sleep(30)
    
    async def collect_metrics(self) -> Dict[str, Any]:
        """æ”¶é›†å„ç§ç›‘æ§æŒ‡æ ‡"""
        metrics = {}
        
        # é€šè¿‡ä»£ç†æ”¶é›†æ•°æ®åº“æŒ‡æ ‡
        async with aiohttp.ClientSession() as session:
            # æ”¶é›†ä¸»åº“æŒ‡æ ‡
            master_metrics = await self._collect_instance_metrics(session, 'master')
            metrics['master'] = master_metrics
            
            # æ”¶é›†è¯»å‰¯æœ¬æŒ‡æ ‡
            read_replica_metrics = []
            for i in range(self.scaling_state['read_replicas']):
                replica_metrics = await self._collect_instance_metrics(session, f'read-{i}')
                read_replica_metrics.append(replica_metrics)
            
            metrics['read_replicas'] = read_replica_metrics
            
            # æ”¶é›†ä»£ç†å±‚æŒ‡æ ‡
            proxy_metrics = await self._collect_proxy_metrics(session)
            metrics['proxy'] = proxy_metrics
        
        return metrics
    
    async def _collect_instance_metrics(self, session: aiohttp.ClientSession, 
                                      instance_type: str) -> Dict[str, float]:
        """æ”¶é›†å•ä¸ªå®ä¾‹çš„æŒ‡æ ‡"""
        try:
            url = f"{self.proxy_endpoint}/metrics/{instance_type}"
            async with session.get(url) as response:
                if response.status == 200:
                    data = await response.json()
                    return {
                        'cpu_utilization': data.get('cpu_percent', 0),
                        'memory_utilization': data.get('memory_percent', 0),
                        'connections': data.get('current_connections', 0),
                        'qps': data.get('queries_per_second', 0),
                        'response_time': data.get('avg_response_time', 0)
                    }
        except Exception:
            pass
        
        return {
            'cpu_utilization': 0,
            'memory_utilization': 0,
            'connections': 0,
            'qps': 0,
            'response_time': 0
        }
    
    async def _collect_proxy_metrics(self, session: aiohttp.ClientSession) -> Dict[str, Any]:
        """æ”¶é›†ä»£ç†å±‚æŒ‡æ ‡"""
        try:
            url = f"{self.proxy_endpoint}/proxy/metrics"
            async with session.get(url) as response:
                if response.status == 200:
                    return await response.json()
        except Exception:
            pass
        
        return {}
    
    def analyze_scaling_need(self, metrics: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†ææ‰©ç¼©å®¹éœ€æ±‚"""
        decision = {
            'should_scale': False,
            'scaling_type': None,
            'target_count': 0,
            'reason': ''
        }
        
        # åˆ†æä¸»åº“å‹åŠ›
        master_metrics = metrics['master']
        master_pressure = self._calculate_instance_pressure(master_metrics)
        
        # åˆ†æè¯»å‰¯æœ¬æ•´ä½“å‹åŠ›
        read_metrics = metrics['read_replicas']
        avg_read_pressure = sum(self._calculate_instance_pressure(m) for m in read_metrics) / len(read_metrics)
        
        # åˆ†æä»£ç†å±‚è´Ÿè½½åˆ†å¸ƒ
        proxy_metrics = metrics['proxy']
        read_write_ratio = proxy_metrics.get('read_write_ratio', 0.8)  # 80%è¯»è¯·æ±‚
        
        # å†³ç­–é€»è¾‘
        if master_pressure > 0.8:
            # ä¸»åº“å‹åŠ›è¿‡å¤§ï¼Œè€ƒè™‘å‚ç›´æ‰©å®¹
            decision['should_scale'] = True
            decision['scaling_type'] = 'vertical_master_up'
            decision['target_spec'] = self._get_next_higher_spec(self.scaling_state['current_master_spec'])
            decision['reason'] = f'ä¸»åº“å‹åŠ›è¿‡é«˜: {master_pressure:.2f}'
        
        elif avg_read_pressure > 0.7 and read_write_ratio > 0.6:
            # è¯»å‹åŠ›å¤§ä¸”è¯»è¯·æ±‚å æ¯”é«˜ï¼Œè€ƒè™‘å¢åŠ è¯»å‰¯æœ¬
            current_replicas = self.scaling_state['read_replicas']
            if current_replicas < 8:  # æœ€å¤§8ä¸ªè¯»å‰¯æœ¬
                decision['should_scale'] = True
                decision['scaling_type'] = 'horizontal_read_up'
                decision['target_count'] = min(current_replicas + 1, 8)
                decision['reason'] = f'è¯»å‰¯æœ¬å‹åŠ›é«˜: {avg_read_pressure:.2f}, è¯»è¯·æ±‚å æ¯”: {read_write_ratio:.2f}'
        
        elif avg_read_pressure < 0.3 and read_write_ratio < 0.4:
            # è¯»å‹åŠ›å°ä¸”è¯»è¯·æ±‚å æ¯”ä½ï¼Œè€ƒè™‘å‡å°‘è¯»å‰¯æœ¬
            current_replicas = self.scaling_state['read_replicas']
            if current_replicas > 1:  # è‡³å°‘ä¿ç•™1ä¸ªè¯»å‰¯æœ¬
                decision['should_scale'] = True
                decision['scaling_type'] = 'horizontal_read_down'
                decision['target_count'] = max(current_replicas - 1, 1)
                decision['reason'] = f'è¯»å‰¯æœ¬å‹åŠ›ä½: {avg_read_pressure:.2f}, è¯»è¯·æ±‚å æ¯”: {read_write_ratio:.2f}'
        
        return decision
    
    def _calculate_instance_pressure(self, metrics: Dict[str, float]) -> float:
        """è®¡ç®—å®ä¾‹å‹åŠ›æŒ‡æ•°"""
        # åŠ æƒè®¡ç®—å„é¡¹æŒ‡æ ‡çš„å‹åŠ›ç¨‹åº¦
        cpu_weight = 0.4
        connections_weight = 0.3
        qps_weight = 0.2
        response_time_weight = 0.1
        
        cpu_pressure = min(1.0, metrics['cpu_utilization'] / 80.0)
        connections_pressure = min(1.0, metrics['connections'] / 1000.0)
        qps_pressure = min(1.0, metrics['qps'] / 5000.0)
        response_time_pressure = min(1.0, metrics['response_time'] / 2.0)
        
        pressure_index = (
            cpu_pressure * cpu_weight +
            connections_pressure * connections_weight +
            qps_pressure * qps_weight +
            response_time_pressure * response_time_weight
        )
        
        return pressure_index
    
    def _get_next_higher_spec(self, current_spec: str) -> str:
        """è·å–æ›´é«˜çš„å®ä¾‹è§„æ ¼"""
        spec_hierarchy = ['small', 'medium', 'large', 'xlarge', '2xlarge']
        try:
            current_index = spec_hierarchy.index(current_spec)
            return spec_hierarchy[min(current_index + 1, len(spec_hierarchy) - 1)]
        except ValueError:
            return 'medium'
    
    async def execute_scaling(self, decision: Dict[str, Any]):
        """æ‰§è¡Œæ‰©ç¼©å®¹æ“ä½œ"""
        if self.scaling_state['scaling_in_progress']:
            print("æ‰©ç¼©å®¹æ­£åœ¨è¿›è¡Œä¸­ï¼Œè·³è¿‡æœ¬æ¬¡æ“ä½œ")
            return
        
        self.scaling_state['scaling_in_progress'] = True
        
        try:
            scaling_type = decision['scaling_type']
            
            if scaling_type == 'horizontal_read_up':
                await self._scale_read_replicas_up(decision['target_count'])
            elif scaling_type == 'horizontal_read_down':
                await self._scale_read_replicas_down(decision['target_count'])
            elif scaling_type == 'vertical_master_up':
                await self._scale_master_up(decision['target_spec'])
            
            print(f"æ‰©ç¼©å®¹æ‰§è¡Œå®Œæˆ: {decision['reason']}")
            
        except Exception as e:
            print(f"æ‰©ç¼©å®¹æ‰§è¡Œå¤±è´¥: {str(e)}")
        finally:
            self.scaling_state['scaling_in_progress'] = False
    
    async def _scale_read_replicas_up(self, target_count: int):
        """å¢åŠ è¯»å‰¯æœ¬"""
        current_count = self.scaling_state['read_replicas']
        for i in range(current_count, target_count):
            # è°ƒç”¨APIåˆ›å»ºæ–°çš„è¯»å‰¯æœ¬
            await self._create_read_replica(f'read-{i}')
        
        self.scaling_state['read_replicas'] = target_count
        print(f"è¯»å‰¯æœ¬æ•°é‡ä» {current_count} å¢åŠ åˆ° {target_count}")
    
    async def _scale_read_replicas_down(self, target_count: int):
        """å‡å°‘è¯»å‰¯æœ¬"""
        current_count = self.scaling_state['read_replicas']
        for i in range(current_count - 1, target_count - 1, -1):
            # è°ƒç”¨APIåˆ é™¤è¯»å‰¯æœ¬
            await self._delete_read_replica(f'read-{i}')
        
        self.scaling_state['read_replicas'] = target_count
        print(f"è¯»å‰¯æœ¬æ•°é‡ä» {current_count} å‡å°‘åˆ° {target_count}")
    
    async def _scale_master_up(self, target_spec: str):
        """å‡çº§ä¸»åº“è§„æ ¼"""
        current_spec = self.scaling_state['current_master_spec']
        # è°ƒç”¨APIå‡çº§ä¸»åº“è§„æ ¼
        await self._upgrade_master_instance(target_spec)
        
        self.scaling_state['current_master_spec'] = target_spec
        print(f"ä¸»åº“è§„æ ¼ä» {current_spec} å‡çº§åˆ° {target_spec}")
    
    async def _create_read_replica(self, replica_name: str):
        """åˆ›å»ºè¯»å‰¯æœ¬"""
        # å®é™…å®ç°ä¸­è°ƒç”¨ç›¸åº”çš„äº‘æœåŠ¡API
        print(f"åˆ›å»ºè¯»å‰¯æœ¬: {replica_name}")
        await asyncio.sleep(2)  # æ¨¡æ‹Ÿåˆ›å»ºæ—¶é—´
    
    async def _delete_read_replica(self, replica_name: str):
        """åˆ é™¤è¯»å‰¯æœ¬"""
        # å®é™…å®ç°ä¸­è°ƒç”¨ç›¸åº”çš„äº‘æœåŠ¡API
        print(f"åˆ é™¤è¯»å‰¯æœ¬: {replica_name}")
        await asyncio.sleep(1)  # æ¨¡æ‹Ÿåˆ é™¤æ—¶é—´
    
    async def _upgrade_master_instance(self, new_spec: str):
        """å‡çº§ä¸»åº“å®ä¾‹"""
        # å®é™…å®ç°ä¸­è°ƒç”¨ç›¸åº”çš„äº‘æœåŠ¡API
        print(f"å‡çº§ä¸»åº“åˆ°è§„æ ¼: {new_spec}")
        await asyncio.sleep(5)  # æ¨¡æ‹Ÿå‡çº§æ—¶é—´

# ä½¿ç”¨ç¤ºä¾‹
async def main():
    controller = ReadWriteScalingController(
        proxy_endpoint="http://proxy-service:8080",
        db_config={
            'master_endpoint': 'master.db.local:3306',
            'read_replica_prefix': 'read-replica',
            'max_read_replicas': 8
        }
    )
    
    # å¯åŠ¨ç›‘æ§å’Œæ‰©ç¼©å®¹
    await controller.monitor_and_scale()

# è¿è¡Œæ§åˆ¶å™¨
if __name__ == "__main__":
    asyncio.run(main())
```

### 4.2 åˆ†ç‰‡æ•°æ®åº“æ‰©ç¼©å®¹

#### åˆ†ç‰‡æ•°æ®åº“æ‰©ç¼©å®¹ç®¡ç†å™¨
```python
# sharding_scaling_manager.py
import hashlib
import bisect
from typing import Dict, List, Any, Optional
import json

class ShardingScalingManager:
    def __init__(self, shard_config: Dict[str, Any]):
        self.shard_config = shard_config
        self.current_shards = shard_config.get('initial_shards', 4)
        self.max_shards = shard_config.get('max_shards', 64)
        self.min_shards = shard_config.get('min_shards', 2)
        self.shard_map = self._initialize_shard_map()
        
    def _initialize_shard_map(self) -> Dict[int, str]:
        """åˆå§‹åŒ–åˆ†ç‰‡æ˜ å°„è¡¨"""
        shard_map = {}
        for i in range(self.current_shards):
            shard_map[i] = f"shard-{i:02d}"
        return shard_map
    
    def get_shard_for_key(self, key: str) -> str:
        """æ ¹æ®é”®è·å–å¯¹åº”çš„åˆ†ç‰‡"""
        hash_value = self._hash_key(key)
        shard_index = hash_value % self.current_shards
        return self.shard_map[shard_index]
    
    def _hash_key(self, key: str) -> int:
        """å“ˆå¸Œé”®å€¼"""
        return int(hashlib.md5(key.encode()).hexdigest(), 16)
    
    def analyze_scaling_need(self, metrics: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æåˆ†ç‰‡æ‰©ç¼©å®¹éœ€æ±‚"""
        decision = {
            'should_scale': False,
            'scaling_type': None,
            'target_shards': 0,
            'reason': ''
        }
        
        # åˆ†æå„åˆ†ç‰‡è´Ÿè½½
        shard_loads = metrics.get('shard_loads', {})
        avg_load = sum(shard_loads.values()) / len(shard_loads) if shard_loads else 0
        max_load = max(shard_loads.values()) if shard_loads else 0
        min_load = min(shard_loads.values()) if shard_loads else 0
        
        load_imbalance = (max_load - min_load) / avg_load if avg_load > 0 else 0
        
        # å†³ç­–é€»è¾‘
        if max_load > 0.8 and self.current_shards < self.max_shards:
            # è´Ÿè½½è¿‡é«˜ä¸”æœªè¾¾åˆ°æœ€å¤§åˆ†ç‰‡æ•°ï¼Œè€ƒè™‘å¢åŠ åˆ†ç‰‡
            decision['should_scale'] = True
            decision['scaling_type'] = 'scale_out'
            decision['target_shards'] = min(self.current_shards * 2, self.max_shards)
            decision['reason'] = f'åˆ†ç‰‡è´Ÿè½½è¿‡é«˜: {max_load:.2f}, è´Ÿè½½ä¸å¹³è¡¡: {load_imbalance:.2f}'
        
        elif avg_load < 0.3 and load_imbalance < 0.2 and self.current_shards > self.min_shards:
            # æ•´ä½“è´Ÿè½½ä½ä¸”è´Ÿè½½å‡è¡¡ï¼Œè€ƒè™‘å‡å°‘åˆ†ç‰‡
            decision['should_scale'] = True
            decision['scaling_type'] = 'scale_in'
            decision['target_shards'] = max(self.current_shards // 2, self.min_shards)
            decision['reason'] = f'åˆ†ç‰‡è´Ÿè½½è¿‡ä½: {avg_load:.2f}, è´Ÿè½½è¾ƒå‡è¡¡: {load_imbalance:.2f}'
        
        return decision
    
    def execute_scaling(self, decision: Dict[str, Any]) -> bool:
        """æ‰§è¡Œåˆ†ç‰‡æ‰©ç¼©å®¹"""
        try:
            if decision['scaling_type'] == 'scale_out':
                return self._scale_out(decision['target_shards'])
            elif decision['scaling_type'] == 'scale_in':
                return self._scale_in(decision['target_shards'])
            return False
        except Exception as e:
            print(f"åˆ†ç‰‡æ‰©ç¼©å®¹æ‰§è¡Œå¤±è´¥: {str(e)}")
            return False
    
    def _scale_out(self, target_shards: int) -> bool:
        """åˆ†ç‰‡æ‰©å®¹"""
        print(f"å¼€å§‹åˆ†ç‰‡æ‰©å®¹: {self.current_shards} -> {target_shards}")
        
        # åˆ›å»ºæ–°åˆ†ç‰‡
        new_shards = []
        for i in range(self.current_shards, target_shards):
            shard_name = f"shard-{i:02d}"
            if self._create_shard(shard_name):
                new_shards.append((i, shard_name))
        
        if len(new_shards) == target_shards - self.current_shards:
            # é‡æ–°åˆ†é…æ•°æ®
            self._redistribute_data(self.current_shards, target_shards)
            
            # æ›´æ–°åˆ†ç‰‡æ˜ å°„
            for shard_index, shard_name in new_shards:
                self.shard_map[shard_index] = shard_name
            
            self.current_shards = target_shards
            print(f"åˆ†ç‰‡æ‰©å®¹å®Œæˆ: {target_shards} ä¸ªåˆ†ç‰‡")
            return True
        
        return False
    
    def _scale_in(self, target_shards: int) -> bool:
        """åˆ†ç‰‡ç¼©å®¹"""
        print(f"å¼€å§‹åˆ†ç‰‡ç¼©å®¹: {self.current_shards} -> {target_shards}")
        
        # æ”¶é›†è¦åˆå¹¶çš„æ•°æ®
        data_to_merge = {}
        for i in range(target_shards, self.current_shards):
            shard_name = self.shard_map[i]
            data_to_merge[shard_name] = self._collect_shard_data(shard_name)
        
        # åˆå¹¶æ•°æ®åˆ°ç°æœ‰åˆ†ç‰‡
        for shard_name, data in data_to_merge.items():
            target_shard_index = self._hash_key(list(data.keys())[0]) % target_shards
            target_shard_name = self.shard_map[target_shard_index]
            self._merge_data_to_shard(data, target_shard_name)
        
        # åˆ é™¤æ—§åˆ†ç‰‡
        for i in range(target_shards, self.current_shards):
            shard_name = self.shard_map.pop(i)
            self._delete_shard(shard_name)
        
        self.current_shards = target_shards
        print(f"åˆ†ç‰‡ç¼©å®¹å®Œæˆ: {target_shards} ä¸ªåˆ†ç‰‡")
        return True
    
    def _create_shard(self, shard_name: str) -> bool:
        """åˆ›å»ºæ–°åˆ†ç‰‡"""
        # å®é™…å®ç°ä¸­è°ƒç”¨æ•°æ®åº“åˆ›å»ºAPI
        print(f"åˆ›å»ºåˆ†ç‰‡: {shard_name}")
        return True
    
    def _delete_shard(self, shard_name: str) -> bool:
        """åˆ é™¤åˆ†ç‰‡"""
        # å®é™…å®ç°ä¸­è°ƒç”¨æ•°æ®åº“åˆ é™¤API
        print(f"åˆ é™¤åˆ†ç‰‡: {shard_name}")
        return True
    
    def _collect_shard_data(self, shard_name: str) -> Dict[str, Any]:
        """æ”¶é›†åˆ†ç‰‡æ•°æ®"""
        # å®é™…å®ç°ä¸­ä»åˆ†ç‰‡æ”¶é›†æ•°æ®
        print(f"æ”¶é›†åˆ†ç‰‡æ•°æ®: {shard_name}")
        return {}
    
    def _merge_data_to_shard(self, data: Dict[str, Any], target_shard: str):
        """åˆå¹¶æ•°æ®åˆ°ç›®æ ‡åˆ†ç‰‡"""
        # å®é™…å®ç°ä¸­å°†æ•°æ®å†™å…¥ç›®æ ‡åˆ†ç‰‡
        print(f"åˆå¹¶æ•°æ®åˆ°åˆ†ç‰‡: {target_shard}")
    
    def _redistribute_data(self, old_shard_count: int, new_shard_count: int):
        """é‡æ–°åˆ†é…æ•°æ®"""
        print(f"é‡æ–°åˆ†é…æ•°æ®: {old_shard_count} -> {new_shard_count}")
        # å®é™…å®ç°ä¸­è¿›è¡Œæ•°æ®é‡åˆ†å¸ƒé€»è¾‘

# ä½¿ç”¨ç¤ºä¾‹
shard_manager = ShardingScalingManager({
    'initial_shards': 4,
    'max_shards': 16,
    'min_shards': 2
})

# æ¨¡æ‹Ÿç›‘æ§æŒ‡æ ‡
metrics = {
    'shard_loads': {
        'shard-00': 0.85,
        'shard-01': 0.92,
        'shard-02': 0.78,
        'shard-03': 0.88
    }
}

# åˆ†ææ‰©ç¼©å®¹éœ€æ±‚
decision = shard_manager.analyze_scaling_need(metrics)
print(f"æ‰©ç¼©å®¹å†³ç­–: {decision}")

# æ‰§è¡Œæ‰©ç¼©å®¹
if decision['should_scale']:
    success = shard_manager.execute_scaling(decision)
    print(f"æ‰©ç¼©å®¹æ‰§è¡Œç»“æœ: {success}")
```

## 5. æ‰©ç¼©å®¹ç›‘æ§ä¸å‘Šè­¦

### 5.1 æ‰©ç¼©å®¹ç›‘æ§é¢æ¿

#### Grafanaç›‘æ§ä»ªè¡¨æ¿é…ç½®
```json
{
  "dashboard": {
    "id": null,
    "title": "æ•°æ®åº“è‡ªåŠ¨æ‰©ç¼©å®¹ç›‘æ§",
    "timezone": "browser",
    "schemaVersion": 16,
    "version": 0,
    "refresh": "30s",
    "panels": [
      {
        "id": 1,
        "type": "graph",
        "title": "å®ä¾‹æ•°é‡å˜åŒ–",
        "gridPos": {
          "h": 8,
          "w": 12,
          "x": 0,
          "y": 0
        },
        "targets": [
          {
            "expr": "database_replicas",
            "legendFormat": "å½“å‰å®ä¾‹æ•°",
            "refId": "A"
          },
          {
            "expr": "database_desired_replicas",
            "legendFormat": "æœŸæœ›å®ä¾‹æ•°",
            "refId": "B"
          }
        ],
        "alert": {
          "conditions": [
            {
              "evaluator": {
                "params": [2, 10],
                "type": "within_range"
              },
              "operator": {
                "type": "and"
              },
              "query": {
                "params": ["A", "5m", "now"]
              },
              "reducer": {
                "params": [],
                "type": "avg"
              },
              "type": "query"
            }
          ],
          "executionErrorState": "alerting",
          "frequency": "1m",
          "handler": 1,
          "name": "å®ä¾‹æ•°é‡å¼‚å¸¸å‘Šè­¦",
          "noDataState": "no_data",
          "notifications": []
        }
      },
      {
        "id": 2,
        "type": "graph",
        "title": "èµ„æºä½¿ç”¨ç‡",
        "gridPos": {
          "h": 8,
          "w": 12,
          "x": 12,
          "y": 0
        },
        "targets": [
          {
            "expr": "avg(database_cpu_utilization)",
            "legendFormat": "å¹³å‡CPUä½¿ç”¨ç‡",
            "refId": "A"
          },
          {
            "expr": "avg(database_memory_utilization)",
            "legendFormat": "å¹³å‡å†…å­˜ä½¿ç”¨ç‡",
            "refId": "B"
          }
        ]
      },
      {
        "id": 3,
        "type": "stat",
        "title": "æœ€è¿‘æ‰©ç¼©å®¹æ´»åŠ¨",
        "gridPos": {
          "h": 6,
          "w": 8,
          "x": 0,
          "y": 8
        },
        "targets": [
          {
            "expr": "increase(database_scaling_events_total[1h])",
            "legendFormat": "æœ€è¿‘1å°æ—¶æ‰©ç¼©å®¹æ¬¡æ•°",
            "refId": "A"
          }
        ]
      },
      {
        "id": 4,
        "type": "table",
        "title": "æ‰©ç¼©å®¹å†å²è®°å½•",
        "gridPos": {
          "h": 6,
          "w": 16,
          "x": 8,
          "y": 8
        },
        "targets": [
          {
            "expr": "database_scaling_events",
            "format": "table",
            "instant": true,
            "refId": "A"
          }
        ],
        "transformations": [
          {
            "id": "organize",
            "options": {
              "excludeByName": {
                "Time": true
              },
              "indexByName": {
                "Value": 4,
                "event_type": 1,
                "instance_count": 2,
                "reason": 3,
                "timestamp": 0
              }
            }
          }
        ]
      }
    ]
  }
}
```

### 5.2 æ‰©ç¼©å®¹å‘Šè­¦è§„åˆ™

#### Prometheuså‘Šè­¦è§„åˆ™é…ç½®
```yaml
# autoscaling-alerts.yaml
groups:
- name: database-autoscaling-alerts
  rules:
  # æ‰©å®¹å‘Šè­¦
  - alert: DatabaseScalingUp
    expr: increase(database_scaling_events_total{event_type="scale_up"}[5m]) > 0
    for: 1m
    labels:
      severity: info
    annotations:
      summary: "æ•°æ®åº“æ­£åœ¨æ‰©å®¹"
      description: "æ£€æµ‹åˆ°æ•°æ®åº“å®ä¾‹æ­£åœ¨æ‰©å®¹ï¼Œå½“å‰å®ä¾‹æ•°: {{ $value }}"

  # ç¼©å®¹å‘Šè­¦
  - alert: DatabaseScalingDown
    expr: increase(database_scaling_events_total{event_type="scale_down"}[5m]) > 0
    for: 1m
    labels:
      severity: info
    annotations:
      summary: "æ•°æ®åº“æ­£åœ¨ç¼©å®¹"
      description: "æ£€æµ‹åˆ°æ•°æ®åº“å®ä¾‹æ­£åœ¨ç¼©å®¹ï¼Œå½“å‰å®ä¾‹æ•°: {{ $value }}"

  # æ‰©å®¹è¿‡äºé¢‘ç¹å‘Šè­¦
  - alert: DatabaseScalingTooFrequent
    expr: increase(database_scaling_events_total[10m]) > 3
    for: 2m
    labels:
      severity: warning
    annotations:
      summary: "æ•°æ®åº“æ‰©ç¼©å®¹è¿‡äºé¢‘ç¹"
      description: "10åˆ†é’Ÿå†…å‘ç”Ÿè¶…è¿‡3æ¬¡æ‰©ç¼©å®¹æ“ä½œï¼Œå¯èƒ½å­˜åœ¨é…ç½®é—®é¢˜"

  # æ‰©å®¹å¤±è´¥å‘Šè­¦
  - alert: DatabaseScalingFailed
    expr: increase(database_scaling_failures_total[5m]) > 0
    for: 1m
    labels:
      severity: critical
    annotations:
      summary: "æ•°æ®åº“æ‰©ç¼©å®¹å¤±è´¥"
      description: "æ£€æµ‹åˆ°æ•°æ®åº“æ‰©ç¼©å®¹æ“ä½œå¤±è´¥ï¼Œè¯·æ£€æŸ¥ç›¸å…³é…ç½®å’Œèµ„æº"

  # èµ„æºä¸è¶³å‘Šè­¦
  - alert: DatabaseResourceExhausted
    expr: database_resource_available < 0.1
    for: 5m
    labels:
      severity: critical
    annotations:
      summary: "æ•°æ®åº“èµ„æºä¸è¶³"
      description: "å¯ç”¨æ•°æ®åº“èµ„æºä½äº10%ï¼Œæ— æ³•æ»¡è¶³æ‰©ç¼©å®¹éœ€æ±‚"

  # æ€§èƒ½ä¸‹é™å‘Šè­¦
  - alert: DatabasePerformanceDegradedAfterScaling
    expr: database_response_time_seconds > 2 and changes(database_replicas[10m]) > 0
    for: 3m
    labels:
      severity: warning
    annotations:
      summary: "æ‰©ç¼©å®¹åæ€§èƒ½ä¸‹é™"
      description: "æ‰©ç¼©å®¹æ“ä½œåæ•°æ®åº“å“åº”æ—¶é—´è¶…è¿‡2ç§’ï¼Œéœ€è¦æ£€æŸ¥æ‰©ç¼©å®¹å½±å“"
```

## 6. æœ€ä½³å®è·µä¸æ•…éšœå¤„ç†

### 6.1 æ‰©ç¼©å®¹æœ€ä½³å®è·µ

#### æ‰©ç¼©å®¹é…ç½®æ£€æŸ¥æ¸…å•
```yaml
# autoscaling-best-practices-checklist.yaml
best_practices:
  pre_scaling_setup:
    - item: "æ˜ç¡®ä¸šåŠ¡SLAè¦æ±‚"
      description: "ç¡®å®šå¯æ¥å—çš„å“åº”æ—¶é—´å’Œå¯ç”¨æ€§æŒ‡æ ‡"
      status: "pending"
    
    - item: "å»ºç«‹å®Œæ•´çš„ç›‘æ§ä½“ç³»"
      description: "é…ç½®CPUã€å†…å­˜ã€è¿æ¥æ•°ã€QPSç­‰å…³é”®æŒ‡æ ‡ç›‘æ§"
      status: "pending"
    
    - item: "è®¾ç½®åˆç†çš„æ‰©ç¼©å®¹é˜ˆå€¼"
      description: "æ ¹æ®å†å²æ•°æ®å’Œä¸šåŠ¡ç‰¹ç‚¹è®¾ç½®é€‚å½“çš„è§¦å‘é˜ˆå€¼"
      status: "pending"
    
    - item: "é…ç½®è¶³å¤Ÿçš„èµ„æºé…é¢"
      description: "ç¡®ä¿äº‘æœåŠ¡å•†æˆ–Kubernetesæœ‰è¶³å¤Ÿçš„èµ„æºé…é¢"
      status: "pending"
  
  scaling_configuration:
    - item: "è®¾ç½®åˆé€‚çš„å†·å´æœŸ"
      description: "é¿å…é¢‘ç¹çš„æ‰©ç¼©å®¹æ“ä½œï¼Œå»ºè®®5-10åˆ†é’Ÿå†·å´æœŸ"
      status: "pending"
    
    - item: "é…ç½®æ¸è¿›å¼æ‰©ç¼©å®¹"
      description: "æ¯æ¬¡æ‰©ç¼©å®¹å¹…åº¦ä¸å®œè¿‡å¤§ï¼Œå»ºè®®æ¯æ¬¡å¢å‡1-2ä¸ªå®ä¾‹"
      status: "pending"
    
    - item: "å¯ç”¨å¤šç»´åº¦æŒ‡æ ‡"
      description: "ç»“åˆCPUã€å†…å­˜ã€è¿æ¥æ•°ç­‰å¤šä¸ªæŒ‡æ ‡è¿›è¡Œç»¼åˆåˆ¤æ–­"
      status: "pending"
    
    - item: "é…ç½®å‘Šè­¦é€šçŸ¥"
      description: "åŠæ—¶é€šçŸ¥ç›¸å…³äººå‘˜æ‰©ç¼©å®¹äº‹ä»¶å’Œå¼‚å¸¸æƒ…å†µ"
      status: "pending"
  
  post_scaling_validation:
    - item: "éªŒè¯æ•°æ®ä¸€è‡´æ€§"
      description: "æ£€æŸ¥æ‰©ç¼©å®¹åæ•°æ®å®Œæ•´æ€§å’Œä¸€è‡´æ€§"
      status: "pending"
    
    - item: "ç›‘æ§æ€§èƒ½æŒ‡æ ‡"
      description: "è§‚å¯Ÿæ‰©ç¼©å®¹åçš„æ€§èƒ½è¡¨ç°æ˜¯å¦ç¬¦åˆé¢„æœŸ"
      status: "pending"
    
    - item: "éªŒè¯åº”ç”¨å…¼å®¹æ€§"
      description: "ç¡®ä¿åº”ç”¨ç¨‹åºèƒ½æ­£ç¡®å¤„ç†è¿æ¥å˜åŒ–"
      status: "pending"
    
    - item: "æ›´æ–°æ–‡æ¡£è®°å½•"
      description: "è®°å½•æ‰©ç¼©å®¹é…ç½®å’Œæ“ä½œå†å²"
      status: "pending"
```

### 6.2 å¸¸è§æ•…éšœå¤„ç†

#### æ‰©ç¼©å®¹æ•…éšœè¯Šæ–­å·¥å…·
```python
# autoscaling_troubleshooter.py
import json
import traceback
from typing import Dict, List, Any, Optional
from datetime import datetime, timedelta

class AutoScalingTroubleshooter:
    def __init__(self):
        self.diagnosis_history = []
    
    def diagnose_scaling_issue(self, error_info: Dict[str, Any]) -> Dict[str, Any]:
        """è¯Šæ–­æ‰©ç¼©å®¹é—®é¢˜"""
        diagnosis = {
            'timestamp': datetime.now().isoformat(),
            'error_type': error_info.get('type', 'unknown'),
            'symptoms': error_info.get('symptoms', []),
            'possible_causes': [],
            'recommended_actions': [],
            'severity': 'unknown'
        }
        
        # æ ¹æ®é”™è¯¯ç±»å‹è¿›è¡Œè¯Šæ–­
        if error_info['type'] == 'scaling_failed':
            self._diagnose_scaling_failure(diagnosis, error_info)
        elif error_info['type'] == 'resource_exhausted':
            self._diagnose_resource_exhaustion(diagnosis, error_info)
        elif error_info['type'] == 'performance_degradation':
            self._diagnose_performance_issue(diagnosis, error_info)
        elif error_info['type'] == 'data_inconsistency':
            self._diagnose_data_issue(diagnosis, error_info)
        
        # è®°å½•è¯Šæ–­å†å²
        self.diagnosis_history.append(diagnosis)
        
        return diagnosis
    
    def _diagnose_scaling_failure(self, diagnosis: Dict[str, Any], error_info: Dict[str, Any]):
        """è¯Šæ–­æ‰©ç¼©å®¹å¤±è´¥é—®é¢˜"""
        symptoms = error_info.get('symptoms', [])
        diagnosis['severity'] = 'critical'
        
        # æ£€æŸ¥å¸¸è§å¤±è´¥åŸå› 
        if 'insufficient_resources' in symptoms:
            diagnosis['possible_causes'].append('èµ„æºé…é¢ä¸è¶³')
            diagnosis['recommended_actions'].extend([
                'æ£€æŸ¥äº‘æœåŠ¡å•†èµ„æºé…é¢',
                'ç”³è¯·å¢åŠ èµ„æºé™åˆ¶',
                'ä¼˜åŒ–èµ„æºé…ç½®å‡å°‘æµªè´¹'
            ])
        
        if 'configuration_error' in symptoms:
            diagnosis['possible_causes'].append('é…ç½®å‚æ•°é”™è¯¯')
            diagnosis['recommended_actions'].extend([
                'æ£€æŸ¥HPAé…ç½®å‚æ•°',
                'éªŒè¯æŒ‡æ ‡åç§°å’Œæ ¼å¼',
                'ç¡®è®¤æƒé™é…ç½®æ­£ç¡®'
            ])
        
        if 'api_call_failed' in symptoms:
            diagnosis['possible_causes'].append('APIè°ƒç”¨å¤±è´¥')
            diagnosis['recommended_actions'].extend([
                'æ£€æŸ¥APIæœåŠ¡çŠ¶æ€',
                'éªŒè¯è®¤è¯å‡­æ®',
                'æŸ¥çœ‹APIè°ƒç”¨æ—¥å¿—'
            ])
    
    def _diagnose_resource_exhaustion(self, diagnosis: Dict[str, Any], error_info: Dict[str, Any]):
        """è¯Šæ–­èµ„æºè€—å°½é—®é¢˜"""
        diagnosis['severity'] = 'critical'
        diagnosis['possible_causes'].append('èµ„æºè§„åˆ’ä¸è¶³')
        diagnosis['recommended_actions'].extend([
            'è¯„ä¼°å®é™…èµ„æºéœ€æ±‚',
            'è°ƒæ•´èµ„æºé…é¢',
            'ä¼˜åŒ–åº”ç”¨èµ„æºä½¿ç”¨',
            'è€ƒè™‘åˆ†ç‰‡æˆ–å…¶ä»–æ‰©å±•æ–¹æ¡ˆ'
        ])
    
    def _diagnose_performance_issue(self, diagnosis: Dict[str, Any], error_info: Dict[str, Any]):
        """è¯Šæ–­æ€§èƒ½é—®é¢˜"""
        diagnosis['severity'] = 'warning'
        
        if error_info.get('performance_drop') > 30:
            diagnosis['severity'] = 'critical'
            diagnosis['possible_causes'].append('æ‰©ç¼©å®¹å¯¼è‡´æ€§èƒ½æ€¥å‰§ä¸‹é™')
            diagnosis['recommended_actions'].extend([
                'ç«‹å³å›æ»šæ‰©ç¼©å®¹æ“ä½œ',
                'åˆ†ææ€§èƒ½ç“¶é¢ˆæ ¹æœ¬åŸå› ',
                'è°ƒæ•´æ‰©ç¼©å®¹ç­–ç•¥å‚æ•°',
                'å¢åŠ æ€§èƒ½æµ‹è¯•éªŒè¯'
            ])
        else:
            diagnosis['possible_causes'].append('æ‰©ç¼©å®¹è¿‡ç¨‹ä¸­çš„æ­£å¸¸æ€§èƒ½æ³¢åŠ¨')
            diagnosis['recommended_actions'].extend([
                'ç›‘æ§æ€§èƒ½æŒ‡æ ‡è¶‹åŠ¿',
                'é€‚å½“å»¶é•¿å†·å´æœŸ',
                'ä¼˜åŒ–åº”ç”¨è¿æ¥æ± é…ç½®'
            ])
    
    def _diagnose_data_issue(self, diagnosis: Dict[str, Any], error_info: Dict[str, Any]):
        """è¯Šæ–­æ•°æ®ä¸€è‡´æ€§é—®é¢˜"""
        diagnosis['severity'] = 'critical'
        diagnosis['possible_causes'].append('æ‰©ç¼©å®¹è¿‡ç¨‹ä¸­çš„æ•°æ®åŒæ­¥é—®é¢˜')
        diagnosis['recommended_actions'].extend([
            'æš‚åœè‡ªåŠ¨æ‰©ç¼©å®¹',
            'æ£€æŸ¥æ•°æ®åŒæ­¥çŠ¶æ€',
            'éªŒè¯ä¸»ä»å¤åˆ¶å¥åº·çŠ¶å†µ',
            'æ‰§è¡Œæ•°æ®ä¸€è‡´æ€§æ£€æŸ¥',
            'å¿…è¦æ—¶æ‰‹åŠ¨ä¿®å¤æ•°æ®'
        ])
    
    def get_troubleshooting_guide(self, issue_type: str) -> Dict[str, Any]:
        """è·å–æ•…éšœæ’é™¤æŒ‡å—"""
        guides = {
            'scaling_failed': {
                'title': 'æ‰©ç¼©å®¹å¤±è´¥æ•…éšœæ’é™¤',
                'steps': [
                    {
                        'step': 1,
                        'action': 'æ£€æŸ¥ç³»ç»Ÿæ—¥å¿—',
                        'details': 'æŸ¥çœ‹æ‰©ç¼©å®¹ç›¸å…³çš„é”™è¯¯æ—¥å¿—å’Œäº‹ä»¶è®°å½•'
                    },
                    {
                        'step': 2,
                        'action': 'éªŒè¯èµ„æºé…ç½®',
                        'details': 'ç¡®è®¤æœ‰è¶³å¤Ÿçš„èµ„æºé…é¢å’Œæ­£ç¡®çš„æƒé™é…ç½®'
                    },
                    {
                        'step': 3,
                        'action': 'æµ‹è¯•æ‰‹åŠ¨æ‰©ç¼©å®¹',
                        'details': 'å°è¯•æ‰‹åŠ¨æ‰§è¡Œæ‰©ç¼©å®¹æ“ä½œéªŒè¯åŸºæœ¬åŠŸèƒ½'
                    },
                    {
                        'step': 4,
                        'action': 'æ£€æŸ¥ä¾èµ–æœåŠ¡',
                        'details': 'ç¡®è®¤ç›¸å…³æœåŠ¡(API Serverã€ç›‘æ§ç³»ç»Ÿç­‰)æ­£å¸¸è¿è¡Œ'
                    }
                ]
            },
            
            'performance_degradation': {
                'title': 'æ€§èƒ½ä¸‹é™æ•…éšœæ’é™¤',
                'steps': [
                    {
                        'step': 1,
                        'action': 'ç«‹å³å›æ»š',
                        'details': 'å¦‚æœæ€§èƒ½ä¸¥é‡å½±å“ä¸šåŠ¡ï¼Œç«‹å³å›æ»šåˆ°ä¹‹å‰çš„é…ç½®'
                    },
                    {
                        'step': 2,
                        'action': 'æ€§èƒ½åˆ†æ',
                        'details': 'ä½¿ç”¨æ€§èƒ½åˆ†æå·¥å…·æ‰¾å‡ºå…·ä½“ç“¶é¢ˆ'
                    },
                    {
                        'step': 3,
                        'action': 'è¿æ¥æ£€æŸ¥',
                        'details': 'æ£€æŸ¥åº”ç”¨è¿æ¥æ± å’Œæ•°æ®åº“è¿æ¥çŠ¶æ€'
                    },
                    {
                        'step': 4,
                        'action': 'å‚æ•°è°ƒä¼˜',
                        'details': 'æ ¹æ®åˆ†æç»“æœè°ƒæ•´ç›¸å…³æ€§èƒ½å‚æ•°'
                    }
                ]
            }
        }
        
        return guides.get(issue_type, {'title': 'é€šç”¨æ•…éšœæ’é™¤', 'steps': []})
    
    def generate_incident_report(self, diagnosis: Dict[str, Any]) -> str:
        """ç”Ÿæˆäº‹æ•…æŠ¥å‘Š"""
        report = f"""
# æ•°æ®åº“è‡ªåŠ¨æ‰©ç¼©å®¹äº‹æ•…æŠ¥å‘Š

## åŸºæœ¬ä¿¡æ¯
- æ—¶é—´: {diagnosis['timestamp']}
- ç±»å‹: {diagnosis['error_type']}
- ä¸¥é‡ç¨‹åº¦: {diagnosis['severity']}

## ç—‡çŠ¶æè¿°
{', '.join(diagnosis['symptoms'])}

## å¯èƒ½åŸå› 
"""
        for cause in diagnosis['possible_causes']:
            report += f"- {cause}\n"
        
        report += "\n## å»ºè®®æªæ–½\n"
        for action in diagnosis['recommended_actions']:
            report += f"- {action}\n"
        
        report += f"\n## è¯Šæ–­è¯¦æƒ…\n```json\n{json.dumps(diagnosis, indent=2, ensure_ascii=False)}\n```"
        
        return report

# ä½¿ç”¨ç¤ºä¾‹
troubleshooter = AutoScalingTroubleshooter()

# æ¨¡æ‹Ÿæ‰©ç¼©å®¹å¤±è´¥æƒ…å†µ
error_info = {
    'type': 'scaling_failed',
    'symptoms': ['insufficient_resources', 'api_call_failed'],
    'error_message': 'failed to create new replica: insufficient quota'
}

# è¿›è¡Œè¯Šæ–­
diagnosis = troubleshooter.diagnose_scaling_issue(error_info)
print("è¯Šæ–­ç»“æœ:")
print(json.dumps(diagnosis, indent=2, ensure_ascii=False))

# è·å–æ•…éšœæ’é™¤æŒ‡å—
guide = troubleshooter.get_troubleshooting_guide('scaling_failed')
print("\næ•…éšœæ’é™¤æŒ‡å—:")
for step in guide['steps']:
    print(f"æ­¥éª¤ {step['step']}: {step['action']}")
    print(f"  è¯¦æƒ…: {step['details']}")

# ç”Ÿæˆäº‹æ•…æŠ¥å‘Š
report = troubleshooter.generate_incident_report(diagnosis)
print("\näº‹æ•…æŠ¥å‘Š:")
print(report)
```

---

## ğŸ” å…³é”®è¦ç‚¹æ€»ç»“

### âœ… è‡ªåŠ¨æ‰©ç¼©å®¹æˆåŠŸè¦ç´ 
- **åˆç†çš„é˜ˆå€¼è®¾ç½®**ï¼šåŸºäºå†å²æ•°æ®å’Œä¸šåŠ¡ç‰¹ç‚¹è®¾ç½®é€‚å½“çš„è§¦å‘é˜ˆå€¼
- **å®Œå–„çš„ç›‘æ§ä½“ç³»**ï¼šå»ºç«‹å¤šç»´åº¦æŒ‡æ ‡ç›‘æ§å’Œå®æ—¶å‘Šè­¦æœºåˆ¶
- **æ¸è¿›å¼æ‰©ç¼©å®¹**ï¼šé¿å…å‰§çƒˆçš„èµ„æºé…ç½®å˜åŒ–ï¼Œç¡®ä¿ä¸šåŠ¡å¹³ç¨³è¿‡æ¸¡
- **æ•…éšœåº”æ€¥æœºåˆ¶**ï¼šå»ºç«‹å¿«é€Ÿå›æ»šå’Œæ•…éšœå¤„ç†æµç¨‹

### âš ï¸ å¸¸è§é£é™©æé†’
- **é…ç½®å¤æ‚æ€§**ï¼šè‡ªåŠ¨æ‰©ç¼©å®¹é…ç½®æ¶‰åŠå¤šä¸ªç³»ç»Ÿç»„ä»¶ï¼Œé…ç½®ä¸å½“å®¹æ˜“å¼•å‘é—®é¢˜
- **èµ„æºç«äº‰**ï¼šæ‰©ç¼©å®¹æ“ä½œå¯èƒ½ä¸å…¶ä»–ç³»ç»Ÿæ“ä½œäº§ç”Ÿèµ„æºç«äº‰
- **æ•°æ®ä¸€è‡´æ€§**ï¼šç‰¹åˆ«æ˜¯åœ¨åˆ†ç‰‡æ•°æ®åº“åœºæ™¯ä¸‹ï¼Œæ‰©ç¼©å®¹å¯èƒ½å½±å“æ•°æ®ä¸€è‡´æ€§
- **æˆæœ¬æ§åˆ¶**ï¼šè‡ªåŠ¨æ‰©å®¹å¯èƒ½å¯¼è‡´æ„å¤–çš„æˆæœ¬å¢åŠ 

### ğŸ¯ æœ€ä½³å®è·µå»ºè®®
1. **å……åˆ†æµ‹è¯•éªŒè¯**ï¼šåœ¨ç”Ÿäº§ç¯å¢ƒå¯ç”¨å‰è¿›è¡Œå……åˆ†çš„æµ‹è¯•éªŒè¯
2. **æ¸è¿›å¼éƒ¨ç½²**ï¼šå…ˆåœ¨éæ ¸å¿ƒä¸šåŠ¡ä¸Šè¯•ç‚¹ï¼Œå†é€æ­¥æ¨å¹¿åˆ°æ ¸å¿ƒç³»ç»Ÿ
3. **å»ºç«‹ç›‘æ§åŸºçº¿**ï¼šæ”¶é›†è¶³å¤Ÿå†å²æ•°æ®å»ºç«‹æ­£å¸¸çš„æ€§èƒ½åŸºçº¿
4. **åˆ¶å®šåº”æ€¥é¢„æ¡ˆ**ï¼šå‡†å¤‡å¥½æ‰‹åŠ¨å¹²é¢„å’Œå¿«é€Ÿå›æ»šçš„åº”æ€¥æ–¹æ¡ˆ
5. **å®šæœŸè¯„å®¡ä¼˜åŒ–**ï¼šå®šæœŸå›é¡¾æ‰©ç¼©å®¹æ•ˆæœï¼ŒæŒç»­ä¼˜åŒ–é…ç½®å‚æ•°

é€šè¿‡ç§‘å­¦çš„è‡ªåŠ¨æ‰©ç¼©å®¹é…ç½®å’Œç®¡ç†ï¼Œå¯ä»¥å®ç°æ•°æ®åº“èµ„æºçš„æ™ºèƒ½è°ƒé…ï¼Œåœ¨ä¿éšœä¸šåŠ¡æ€§èƒ½çš„åŒæ—¶æœ€å¤§åŒ–èµ„æºåˆ©ç”¨æ•ˆç‡ã€‚