# æ•°æ®åº“å¤šåŒºåŸŸéƒ¨ç½²æ–¹æ¡ˆå®Œæ•´æŒ‡å—

## ğŸ¯ æ¦‚è¿°

æ•°æ®åº“å¤šåŒºåŸŸéƒ¨ç½²æ˜¯æ„å»ºå…¨çƒåŒ–åº”ç”¨å’Œå®ç°é«˜å¯ç”¨æ€§çš„å…³é”®æŠ€æœ¯ï¼Œé€šè¿‡åœ¨å¤šä¸ªåœ°ç†åŒºåŸŸéƒ¨ç½²æ•°æ®åº“å®ä¾‹æ¥é™ä½å»¶è¿Ÿã€æé«˜å¯ç”¨æ€§å¹¶æ»¡è¶³æ•°æ®ä¸»æƒè¦æ±‚ã€‚æœ¬æŒ‡å—æä¾›ä»æ¶æ„è®¾è®¡åˆ°å®æ–½è¿ç»´çš„å®Œæ•´å¤šåŒºåŸŸæ•°æ®åº“éƒ¨ç½²è§£å†³æ–¹æ¡ˆã€‚

## ğŸ“‹ ç›®å½•

1. [å¤šåŒºåŸŸéƒ¨ç½²åŸºç¡€ç†è®º](#1-å¤šåŒºåŸŸéƒ¨ç½²åŸºç¡€ç†è®º)
2. [æ¶æ„è®¾è®¡æ¨¡å¼](#2-æ¶æ„è®¾è®¡æ¨¡å¼)
3. [æ•°æ®åŒæ­¥ä¸ä¸€è‡´æ€§](#3-æ•°æ®åŒæ­¥ä¸ä¸€è‡´æ€§)
4. [æ•…éšœåˆ‡æ¢ä¸å®¹ç¾](#4-æ•…éšœåˆ‡æ¢ä¸å®¹ç¾)
5. [æ€§èƒ½ä¼˜åŒ–ç­–ç•¥](#5-æ€§èƒ½ä¼˜åŒ–ç­–ç•¥)
6. [ç›‘æ§ä¸è¿ç»´ç®¡ç†](#6-ç›‘æ§ä¸è¿ç»´ç®¡ç†)

---

## 1. å¤šåŒºåŸŸéƒ¨ç½²åŸºç¡€ç†è®º

### 1.1 å¤šåŒºåŸŸéƒ¨ç½²æ¶æ„æ¨¡å¼

#### å…¨çƒæ•°æ®åº“éƒ¨ç½²æ¶æ„
```mermaid
graph TD
    A[å…¨çƒç”¨æˆ·] --> B[è¾¹ç¼˜æ¥å…¥ç‚¹]
    
    B --> C[åŒºåŸŸè´Ÿè½½å‡è¡¡å™¨]
    C --> D[ä¸»åŒºåŸŸæ•°æ®ä¸­å¿ƒ]
    C --> E[ç¾å¤‡åŒºåŸŸæ•°æ®ä¸­å¿ƒ]
    C --> F[å°±è¿‘åŒºåŸŸæ•°æ®ä¸­å¿ƒ]
    
    subgraph "ä¸»åŒºåŸŸ - us-east-1"
        D --> D1[ä¸»æ•°æ®åº“é›†ç¾¤]
        D --> D2[åªè¯»å‰¯æœ¬é›†ç¾¤]
        D --> D3[ç¼“å­˜å±‚]
    end
    
    subgraph "ç¾å¤‡åŒºåŸŸ - us-west-2"
        E --> E1[ç¾å¤‡æ•°æ®åº“é›†ç¾¤]
        E --> E2[åªè¯»å‰¯æœ¬é›†ç¾¤]
        E --> E3[ç¼“å­˜å±‚]
    end
    
    subgraph "å°±è¿‘åŒºåŸŸ - eu-west-1"
        F --> F1[å°±è¿‘æ•°æ®åº“é›†ç¾¤]
        F --> F2[åªè¯»å‰¯æœ¬é›†ç¾¤]
        F --> F3[ç¼“å­˜å±‚]
    end
    
    D1 --> G[è·¨åŒºåŸŸæ•°æ®åŒæ­¥]
    E1 --> G
    F1 --> G
    
    subgraph "ç›‘æ§æ²»ç†å±‚"
        H[å…¨å±€ç›‘æ§ç³»ç»Ÿ]
        I[ç»Ÿä¸€é…ç½®ç®¡ç†]
        J[è‡ªåŠ¨åŒ–è¿ç»´å¹³å°]
    end
    
    H --> D1
    H --> E1
    H --> F1
    I --> D1
    I --> E1
    I --> F1
    J --> D1
    J --> E1
    J --> F1
```

#### å¤šåŒºåŸŸéƒ¨ç½²æ¨¡å¼å¯¹æ¯”
```python
# multi_region_deployment_patterns.py
from typing import Dict, List, Any
from dataclasses import dataclass
from enum import Enum

class DeploymentPattern(Enum):
    ACTIVE_ACTIVE = "active_active"
    ACTIVE_PASSIVE = "active_passive"
    MULTI_MASTER = "multi_master"
    MASTER_SLAVE_REGIONAL = "master_slave_regional"
    GEO_PARTITIONED = "geo_partitioned"

@dataclass
class RegionConfig:
    region_name: str
    region_code: str
    latency_to_users: float  # ms
    data_sovereignty: bool
    disaster_risk: float  # 0-1
    cost_factor: float    # ç›¸å¯¹äºä¸»åŒºåŸŸçš„æˆæœ¬å€æ•°

@dataclass
class DeploymentPatternAnalysis:
    pattern: DeploymentPattern
    description: str
    pros: List[str]
    cons: List[str]
    use_cases: List[str]
    complexity: int  # 1-5
    consistency_level: str

class MultiRegionDeploymentAnalyzer:
    def __init__(self):
        self.patterns = self._initialize_patterns()
        self.regions = []
    
    def _initialize_patterns(self) -> Dict[DeploymentPattern, DeploymentPatternAnalysis]:
        """åˆå§‹åŒ–éƒ¨ç½²æ¨¡å¼åˆ†æ"""
        return {
            DeploymentPattern.ACTIVE_ACTIVE: DeploymentPatternAnalysis(
                pattern=DeploymentPattern.ACTIVE_ACTIVE,
                description="æ‰€æœ‰åŒºåŸŸéƒ½å¯è¯»å†™ï¼Œè´Ÿè½½å‡è¡¡åˆ†å‘è¯·æ±‚",
                pros=[
                    "æœ€ä½å»¶è¿Ÿä½“éªŒ",
                    "æœ€é«˜å¯ç”¨æ€§",
                    "è´Ÿè½½åˆ†æ•£å‡åŒ€"
                ],
                cons=[
                    "æ•°æ®ä¸€è‡´æ€§å¤æ‚",
                    "å†²çªè§£å†³å›°éš¾",
                    "å®ç°å¤æ‚åº¦é«˜"
                ],
                use_cases=[
                    "å…¨çƒç”µå•†å¹³å°",
                    "å®æ—¶åä½œåº”ç”¨",
                    "æ¸¸æˆæœåŠ¡"
                ],
                complexity=5,
                consistency_level="eventual"
            ),
            
            DeploymentPattern.ACTIVE_PASSIVE: DeploymentPatternAnalysis(
                pattern=DeploymentPattern.ACTIVE_PASSIVE,
                description="ä¸€ä¸ªä¸»åŒºåŸŸè¯»å†™ï¼Œå…¶ä»–åŒºåŸŸåªè¯»å¤‡ä»½",
                pros=[
                    "å¼ºä¸€è‡´æ€§ä¿è¯",
                    "å®ç°ç›¸å¯¹ç®€å•",
                    "æˆæœ¬è¾ƒä½"
                ],
                cons=[
                    "ç¾å¤‡åˆ‡æ¢å»¶è¿Ÿ",
                    "éƒ¨åˆ†åŒºåŸŸé«˜å»¶è¿Ÿ",
                    "èµ„æºåˆ©ç”¨ç‡ä½"
                ],
                use_cases=[
                    "é‡‘èäº¤æ˜“ç³»ç»Ÿ",
                    "ä¼ä¸šERPç³»ç»Ÿ",
                    "åˆè§„è¦æ±‚ä¸¥æ ¼çš„ç³»ç»Ÿ"
                ],
                complexity=3,
                consistency_level="strong"
            ),
            
            DeploymentPattern.MULTI_MASTER: DeploymentPatternAnalysis(
                pattern=DeploymentPattern.MULTI_MASTER,
                description="å¤šä¸ªåŒºåŸŸéƒ½å¯ä»¥ä½œä¸ºä¸»èŠ‚ç‚¹ï¼Œæ”¯æŒåŒå‘åŒæ­¥",
                pros=[
                    "åŒºåŸŸé—´ä½å»¶è¿Ÿ",
                    "æ•…éšœåˆ‡æ¢å¿«é€Ÿ",
                    "èµ„æºåˆ©ç”¨ç‡é«˜"
                ],
                cons=[
                    "å†²çªæ£€æµ‹å¤æ‚",
                    "æ•°æ®åŒæ­¥å¼€é”€å¤§",
                    "è¿ç»´å¤æ‚"
                ],
                use_cases=[
                    "å†…å®¹ç®¡ç†ç³»ç»Ÿ",
                    "ç¤¾äº¤ç½‘ç»œåº”ç”¨",
                    "å¤šåœ°åŒºåŠå…¬ç³»ç»Ÿ"
                ],
                complexity=4,
                consistency_level="causal"
            ),
            
            DeploymentPattern.MASTER_SLAVE_REGIONAL: DeploymentPatternAnalysis(
                pattern=DeploymentPattern.MASTER_SLAVE_REGIONAL,
                description="æ¯ä¸ªåŒºåŸŸå†…éƒ¨ä¸»ä»å¤åˆ¶ï¼ŒåŒºåŸŸé—´å¼‚æ­¥å¤åˆ¶",
                pros=[
                    "åŒºåŸŸå†…å¼ºä¸€è‡´æ€§",
                    "åŒºåŸŸé—´æœ€ç»ˆä¸€è‡´æ€§",
                    "å®ç°å¤æ‚åº¦é€‚ä¸­"
                ],
                cons=[
                    "è·¨åŒºåŸŸå»¶è¿Ÿè¾ƒé«˜",
                    "ç¾å¤‡æ¢å¤æ—¶é—´é•¿",
                    "é…ç½®ç®¡ç†å¤æ‚"
                ],
                use_cases=[
                    "åŒºåŸŸæ€§ä¸šåŠ¡ç³»ç»Ÿ",
                    "æ•°æ®åˆ†æå¹³å°",
                    "æŠ¥è¡¨ç³»ç»Ÿ"
                ],
                complexity=4,
                consistency_level="regional_strong"
            ),
            
            DeploymentPattern.GEO_PARTITIONED: DeploymentPatternAnalysis(
                pattern=DeploymentPattern.GEO_PARTITIONED,
                description="æŒ‰åœ°ç†ä½ç½®åˆ†åŒºå­˜å‚¨æ•°æ®ï¼Œå„åŒºåŸŸç‹¬ç«‹ç®¡ç†",
                pros=[
                    "æ•°æ®ä¸»æƒåˆè§„",
                    "æœ¬åœ°åŒ–æ€§èƒ½æœ€ä¼˜",
                    "æ•…éšœéš”ç¦»æ€§å¥½"
                ],
                cons=[
                    "è·¨åŒºåŸŸæŸ¥è¯¢å›°éš¾",
                    "æ•°æ®æ•´åˆå¤æ‚",
                    "å…¨å±€è§†å›¾ç¼ºå¤±"
                ],
                use_cases=[
                    "æ”¿åºœä¿¡æ¯ç³»ç»Ÿ",
                    "åŒ»ç–—å¥åº·ç³»ç»Ÿ",
                    "æ•°æ®æœ¬åœ°åŒ–è¦æ±‚é«˜çš„åº”ç”¨"
                ],
                complexity=3,
                consistency_level="local_strong"
            )
        }
    
    def analyze_business_requirements(self, requirements: Dict[str, Any]) -> List[Dict[str, Any]]:
        """åŸºäºä¸šåŠ¡éœ€æ±‚åˆ†æé€‚åˆçš„éƒ¨ç½²æ¨¡å¼"""
        analysis_results = []
        
        for pattern, analysis in self.patterns.items():
            score = self._calculate_pattern_score(pattern, requirements, analysis)
            analysis_results.append({
                'pattern': pattern.value,
                'score': score,
                'analysis': analysis.__dict__,
                'recommendation_level': self._get_recommendation_level(score)
            })
        
        # æŒ‰å¾—åˆ†æ’åº
        analysis_results.sort(key=lambda x: x['score'], reverse=True)
        return analysis_results
    
    def _calculate_pattern_score(self, pattern: DeploymentPattern, 
                               requirements: Dict[str, Any], 
                               analysis: DeploymentPatternAnalysis) -> float:
        """è®¡ç®—éƒ¨ç½²æ¨¡å¼å¾—åˆ†"""
        score = 0.0
        
        # å¯ç”¨æ€§è¦æ±‚è¯„åˆ† (æƒé‡: 0.25)
        availability_requirement = requirements.get('availability_target', 99.9)
        if availability_requirement >= 99.99:
            # è¦æ±‚æé«˜å¯ç”¨æ€§
            if pattern in [DeploymentPattern.ACTIVE_ACTIVE, DeploymentPattern.MULTI_MASTER]:
                score += 25
            elif pattern == DeploymentPattern.MASTER_SLAVE_REGIONAL:
                score += 15
        else:
            # ä¸€èˆ¬å¯ç”¨æ€§è¦æ±‚
            if pattern in [DeploymentPattern.ACTIVE_PASSIVE, DeploymentPattern.MASTER_SLAVE_REGIONAL]:
                score += 20
        
        # ä¸€è‡´æ€§è¦æ±‚è¯„åˆ† (æƒé‡: 0.20)
        consistency_requirement = requirements.get('consistency_requirement', 'eventual')
        consistency_mapping = {
            'strong': 1,
            'causal': 2,
            'eventual': 3
        }
        
        pattern_consistency_level = analysis.consistency_level
        pattern_consistency_value = consistency_mapping.get(pattern_consistency_level, 3)
        required_consistency_value = consistency_mapping.get(consistency_requirement, 3)
        
        if pattern_consistency_value <= required_consistency_value:
            score += 20
        elif abs(pattern_consistency_value - required_consistency_value) == 1:
            score += 10
        
        # å»¶è¿Ÿæ•æ„Ÿåº¦è¯„åˆ† (æƒé‡: 0.20)
        latency_sensitivity = requirements.get('latency_sensitivity', 'medium')
        latency_mapping = {'low': 1, 'medium': 2, 'high': 3}
        
        if latency_sensitivity == 'high':
            # å¯¹å»¶è¿Ÿéå¸¸æ•æ„Ÿ
            if pattern in [DeploymentPattern.ACTIVE_ACTIVE, DeploymentPattern.MULTI_MASTER]:
                score += 20
            elif pattern == DeploymentPattern.GEO_PARTITIONED:
                score += 15
        elif latency_sensitivity == 'medium':
            # ä¸­ç­‰å»¶è¿Ÿæ•æ„Ÿåº¦
            if pattern in [DeploymentPattern.MASTER_SLAVE_REGIONAL, DeploymentPattern.MULTI_MASTER]:
                score += 15
        else:
            # å¯¹å»¶è¿Ÿä¸æ•æ„Ÿ
            score += 10
        
        # æˆæœ¬è€ƒè™‘è¯„åˆ† (æƒé‡: 0.15)
        budget_constraint = requirements.get('budget_constraint', 'medium')
        cost_mapping = {'low': 1, 'medium': 2, 'high': 3}
        
        if budget_constraint == 'low':
            # é¢„ç®—æœ‰é™
            if pattern in [DeploymentPattern.ACTIVE_PASSIVE, DeploymentPattern.GEO_PARTITIONED]:
                score += 15
            elif pattern == DeploymentPattern.MASTER_SLAVE_REGIONAL:
                score += 10
        else:
            # é¢„ç®—å……è¶³
            score += 15
        
        # åˆè§„è¦æ±‚è¯„åˆ† (æƒé‡: 0.10)
        compliance_requirement = requirements.get('compliance_requirement', False)
        if compliance_requirement:
            if pattern == DeploymentPattern.GEO_PARTITIONED:
                score += 10
            elif pattern in [DeploymentPattern.ACTIVE_PASSIVE, DeploymentPattern.MASTER_SLAVE_REGIONAL]:
                score += 5
        else:
            score += 5
        
        # æŠ€æœ¯å¤æ‚åº¦è¯„åˆ† (æƒé‡: 0.10)
        team_skill_level = requirements.get('team_skill_level', 'intermediate')
        skill_mapping = {'beginner': 1, 'intermediate': 2, 'advanced': 3, 'expert': 4}
        
        if skill_mapping[team_skill_level] >= analysis.complexity:
            score += 10
        elif skill_mapping[team_skill_level] >= analysis.complexity - 1:
            score += 5
        
        return score
    
    def _get_recommendation_level(self, score: float) -> str:
        """æ ¹æ®å¾—åˆ†ç¡®å®šæ¨èçº§åˆ«"""
        if score >= 85:
            return "å¼ºçƒˆæ¨è"
        elif score >= 70:
            return "æ¨è"
        elif score >= 50:
            return "å¯è€ƒè™‘"
        else:
            return "ä¸æ¨è"
    
    def design_multi_region_architecture(self, business_requirements: Dict[str, Any], 
                                       selected_regions: List[RegionConfig]) -> Dict[str, Any]:
        """è®¾è®¡å¤šåŒºåŸŸæ¶æ„"""
        # åˆ†ææœ€é€‚åˆçš„éƒ¨ç½²æ¨¡å¼
        analysis_results = self.analyze_business_requirements(business_requirements)
        primary_pattern = analysis_results[0]['pattern']
        
        # è®¾è®¡æ¶æ„
        architecture = {
            'primary_pattern': primary_pattern,
            'regions': [region.__dict__ for region in selected_regions],
            'data_flow': self._design_data_flow(primary_pattern, selected_regions),
            'failover_strategy': self._design_failover_strategy(primary_pattern, selected_regions),
            'monitoring_plan': self._design_monitoring_plan(selected_regions),
            'cost_estimation': self._estimate_costs(selected_regions, business_requirements)
        }
        
        return architecture
    
    def _design_data_flow(self, pattern: str, regions: List[RegionConfig]) -> Dict[str, Any]:
        """è®¾è®¡æ•°æ®æµå‘"""
        if pattern == 'active_active':
            return {
                'type': 'bidirectional_sync',
                'sync_mechanism': 'real_time_replication',
                'conflict_resolution': 'timestamp_based',
                'routing_strategy': 'nearest_region_routing'
            }
        elif pattern == 'active_passive':
            return {
                'type': 'unidirectional_sync',
                'sync_mechanism': 'asynchronous_replication',
                'conflict_resolution': 'master_wins',
                'routing_strategy': 'primary_region_routing'
            }
        # å…¶ä»–æ¨¡å¼çš„å®ç°...
        return {}
    
    def _design_failover_strategy(self, pattern: str, regions: List[RegionConfig]) -> Dict[str, Any]:
        """è®¾è®¡æ•…éšœåˆ‡æ¢ç­–ç•¥"""
        return {
            'automatic_failover': pattern in ['active_active', 'multi_master'],
            'failover_time': 'seconds' if pattern in ['active_active', 'multi_master'] else 'minutes',
            'data_loss_tolerance': 'zero' if pattern == 'active_passive' else 'minimal',
            'testing_frequency': 'weekly'
        }
    
    def _design_monitoring_plan(self, regions: List[RegionConfig]) -> Dict[str, Any]:
        """è®¾è®¡ç›‘æ§è®¡åˆ’"""
        return {
            'metrics_to_monitor': [
                'cross_region_latency',
                'replication_lag',
                'region_availability',
                'data_consistency'
            ],
            'alerting_thresholds': {
                'latency_threshold': 100,  # ms
                'replication_lag_threshold': 5,  # seconds
                'availability_threshold': 99.9
            },
            'monitoring_tools': ['prometheus', 'grafana', 'custom_agents']
        }
    
    def _estimate_costs(self, regions: List[RegionConfig], requirements: Dict[str, Any]) -> Dict[str, Any]:
        """ä¼°ç®—æˆæœ¬"""
        base_cost = requirements.get('estimated_monthly_cost', 10000)
        total_cost = sum(region.cost_factor for region in regions) * base_cost / len(regions)
        
        return {
            'monthly_cost_estimate': total_cost,
            'cost_breakdown': {
                'compute': total_cost * 0.6,
                'storage': total_cost * 0.25,
                'network': total_cost * 0.15
            },
            'cost_optimization_suggestions': [
                'ä½¿ç”¨é¢„ç•™å®ä¾‹é™ä½æˆæœ¬',
                'å®æ–½è‡ªåŠ¨æ‰©ç¼©å®¹',
                'ä¼˜åŒ–æ•°æ®ä¼ è¾“'
            ]
        }

# ä½¿ç”¨ç¤ºä¾‹
analyzer = MultiRegionDeploymentAnalyzer()

# å®šä¹‰åŒºåŸŸé…ç½®
regions = [
    RegionConfig("ç¾å›½ä¸œéƒ¨", "us-east-1", 10.0, False, 0.1, 1.0),
    RegionConfig("ç¾å›½è¥¿éƒ¨", "us-west-2", 50.0, False, 0.05, 1.2),
    RegionConfig("æ¬§æ´²è¥¿éƒ¨", "eu-west-1", 80.0, True, 0.08, 1.3),
    RegionConfig("äºšå¤ªä¸œåŒ—", "ap-northeast-1", 120.0, True, 0.15, 1.4)
]

# ä¸šåŠ¡éœ€æ±‚
business_requirements = {
    'availability_target': 99.99,
    'consistency_requirement': 'eventual',
    'latency_sensitivity': 'high',
    'budget_constraint': 'high',
    'compliance_requirement': True,
    'team_skill_level': 'advanced',
    'estimated_monthly_cost': 15000
}

# åˆ†æéƒ¨ç½²æ¨¡å¼
results = analyzer.analyze_business_requirements(business_requirements)

print("éƒ¨ç½²æ¨¡å¼åˆ†æç»“æœ:")
for result in results[:3]:  # æ˜¾ç¤ºå‰3ä¸ªæ¨è
    print(f"\næ¨¡å¼: {result['pattern']}")
    print(f"å¾—åˆ†: {result['score']}")
    print(f"æ¨èçº§åˆ«: {result['recommendation_level']}")
    print(f"ä¼˜ç‚¹: {', '.join(result['analysis']['pros'])}")
    print(f"ç¼ºç‚¹: {', '.join(result['analysis']['cons'][:2])}")  # åªæ˜¾ç¤ºå‰2ä¸ªç¼ºç‚¹

# è®¾è®¡æ¶æ„
architecture = analyzer.design_multi_region_architecture(business_requirements, regions)
print(f"\næ¨èæ¶æ„æ¨¡å¼: {architecture['primary_pattern']}")
print(f"é¢„ä¼°æœˆæˆæœ¬: ${architecture['cost_estimation']['monthly_cost_estimate']:,.2f}")
```

### 1.2 ç½‘ç»œå»¶è¿Ÿä¸æ•°æ®åŒæ­¥

#### è·¨åŒºåŸŸç½‘ç»œå»¶è¿Ÿæ¨¡å‹
```yaml
# cross_region_network_latency.yaml
network_latency_profiles:
  # ç¾å›½ä¸œè¥¿æµ·å²¸ä¹‹é—´
  us_inter_coastal:
    direct_latency: 60-80ms
    via_internet: 80-120ms
    via_private_network: 65-85ms
    packet_loss: 0.01-0.05%
  
  # ç¾å›½åˆ°æ¬§æ´²
  us_to_europe:
    direct_latency: 80-120ms
    via_internet: 120-200ms
    via_private_network: 90-130ms
    packet_loss: 0.02-0.1%
  
  # ç¾å›½åˆ°äºšæ´²
  us_to_asia:
    direct_latency: 150-250ms
    via_internet: 250-400ms
    via_private_network: 180-280ms
    packet_loss: 0.05-0.2%
  
  # æ¬§æ´²åˆ°äºšæ´²
  europe_to_asia:
    direct_latency: 200-300ms
    via_internet: 300-500ms
    via_private_network: 220-320ms
    packet_loss: 0.03-0.15%

data_synchronization_strategies:
  real_time_sync:
    description: "å®æ—¶æ•°æ®åŒæ­¥"
    max_acceptable_latency: 100ms
    suitable_for:
      - active_active_deployment
      - multi_master_deployment
    implementation:
      - database_native_replication
      - message_queue_based_sync
      - change_data_capture
  
  near_real_time_sync:
    description: "è¿‘å®æ—¶æ•°æ®åŒæ­¥"
    max_acceptable_latency: 1000ms
    suitable_for:
      - active_passive_deployment
      - master_slave_regional
    implementation:
      - periodic_batch_sync
      - log_shipping
      - incremental_backup
  
  batch_sync:
    description: "æ‰¹é‡æ•°æ®åŒæ­¥"
    max_acceptable_latency: 3600s
    suitable_for:
      - geo_partitioned_deployment
      - analytics_workloads
    implementation:
      - nightly_batch_processing
      - data_warehouse_sync
      - offline_data_transfer
```

## 2. æ¶æ„è®¾è®¡æ¨¡å¼

### 2.1 Active-ActiveåŒæ´»æ¶æ„

#### åŒæ´»æ¶æ„éƒ¨ç½²é…ç½®
```yaml
# active-active-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: database-active-active
  namespace: multi-region
spec:
  replicas: 6  # æ¯ä¸ªåŒºåŸŸ2ä¸ªå®ä¾‹
  selector:
    matchLabels:
      app: database
      deployment-pattern: active-active
  template:
    metadata:
      labels:
        app: database
        deployment-pattern: active-active
        region: multi  # å¤šåŒºåŸŸæ ‡ç­¾
    spec:
      containers:
      - name: mysql-primary
        image: mysql:8.0.35
        env:
        - name: MYSQL_ROOT_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mysql-root-password
              key: password
        - name: MYSQL_DATABASE
          value: "global_app"
        - name: MYSQL_USER
          value: "appuser"
        - name: MYSQL_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mysql-app-password
              key: password
        ports:
        - containerPort: 3306
        volumeMounts:
        - name: mysql-data
          mountPath: /var/lib/mysql
        - name: mysql-config
          mountPath: /etc/mysql/conf.d
        livenessProbe:
          exec:
            command:
            - mysqladmin
            - ping
            - -h
            - localhost
          initialDelaySeconds: 60
          periodSeconds: 10
        readinessProbe:
          exec:
            command:
            - mysql
            - -h
            - localhost
            - -e
            - SELECT 1
          initialDelaySeconds: 30
          periodSeconds: 5
        resources:
          requests:
            memory: "2Gi"
            cpu: "1"
          limits:
            memory: "4Gi"
            cpu: "2"
      
      volumes:
      - name: mysql-config
        configMap:
          name: mysql-global-config
      
      # èŠ‚ç‚¹äº²å’Œæ€§ - åˆ†å¸ƒåˆ°ä¸åŒåŒºåŸŸ
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: topology.kubernetes.io/region
                operator: In
                values:
                - us-east-1
                - us-west-2
                - eu-west-1
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - database
            topologyKey: topology.kubernetes.io/zone

---
# å¤šåŒºåŸŸæœåŠ¡é…ç½®
apiVersion: v1
kind: Service
metadata:
  name: database-global-service
  namespace: multi-region
spec:
  type: LoadBalancer
  selector:
    app: database
  ports:
  - port: 3306
    targetPort: 3306
  # å¤šåŒºåŸŸè´Ÿè½½å‡è¡¡é…ç½®
  loadBalancerClass: multi-region-lb
  loadBalancerSourceRanges:
  - 0.0.0.0/0

---
# åœ°ç†è·¯ç”±é…ç½®
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: database-geo-routing
  namespace: multi-region
  annotations:
    kubernetes.io/ingress.class: "geo-router"
    geo.routing.algorithm: "latency-based"
spec:
  rules:
  - host: db.global-app.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: database-global-service
            port:
              number: 3306
```

#### åŒæ´»æ•°æ®åŒæ­¥é…ç½®
```python
# active_active_sync_manager.py
import asyncio
import aiomysql
import json
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
import hashlib
import time

@dataclass
class SyncConflict:
    table_name: str
    primary_key: str
    conflicting_fields: List[str]
    timestamps: Dict[str, float]
    resolution_strategy: str

class ActiveActiveSyncManager:
    def __init__(self, regions: List[str]):
        self.regions = regions
        self.region_connections = {}
        self.conflict_resolver = ConflictResolver()
        self.sync_queue = asyncio.Queue()
        
    async def initialize_connections(self, db_configs: Dict[str, Dict[str, Any]]):
        """åˆå§‹åŒ–å„åŒºåŸŸæ•°æ®åº“è¿æ¥"""
        for region in self.regions:
            config = db_configs[region]
            self.region_connections[region] = await aiomysql.connect(
                host=config['host'],
                port=config['port'],
                user=config['user'],
                password=config['password'],
                db=config['database'],
                autocommit=True
            )
        print(f"å·²åˆå§‹åŒ– {len(self.regions)} ä¸ªåŒºåŸŸçš„æ•°æ®åº“è¿æ¥")
    
    async def start_bidirectional_sync(self):
        """å¯åŠ¨åŒå‘åŒæ­¥"""
        tasks = []
        for region in self.regions:
            task = asyncio.create_task(self._sync_from_region(region))
            tasks.append(task)
        
        # å¯åŠ¨å†²çªæ£€æµ‹ä»»åŠ¡
        conflict_task = asyncio.create_task(self._process_conflicts())
        tasks.append(conflict_task)
        
        await asyncio.gather(*tasks)
    
    async def _sync_from_region(self, source_region: str):
        """ä»æŒ‡å®šåŒºåŸŸåŒæ­¥æ•°æ®åˆ°å…¶ä»–åŒºåŸŸ"""
        while True:
            try:
                # è·å–å˜æ›´æ•°æ®
                changes = await self._get_recent_changes(source_region)
                
                # åŒæ­¥åˆ°å…¶ä»–åŒºåŸŸ
                for target_region in self.regions:
                    if target_region != source_region:
                        await self._apply_changes_to_region(changes, source_region, target_region)
                
                # è®°å½•åŒæ­¥æ—¶é—´æˆ³
                await self._update_sync_timestamp(source_region)
                
                await asyncio.sleep(1)  # 1ç§’åŒæ­¥é—´éš”
                
            except Exception as e:
                print(f"åŒºåŸŸ {source_region} åŒæ­¥å‡ºé”™: {str(e)}")
                await asyncio.sleep(10)  # é”™è¯¯åç­‰å¾…10ç§’é‡è¯•
    
    async def _get_recent_changes(self, region: str) -> List[Dict[str, Any]]:
        """è·å–æœ€è¿‘çš„å˜æ›´æ•°æ®"""
        connection = self.region_connections[region]
        cursor = await connection.cursor()
        
        # æŸ¥è¯¢æœ€è¿‘1ç§’å†…çš„å˜æ›´ï¼ˆä½¿ç”¨æ—¶é—´æˆ³æˆ–CDCï¼‰
        query = """
        SELECT table_name, operation, primary_key, changed_data, timestamp
        FROM change_log 
        WHERE timestamp > %s 
        ORDER BY timestamp ASC
        LIMIT 1000
        """
        
        last_sync_time = await self._get_last_sync_time(region)
        await cursor.execute(query, (last_sync_time,))
        changes = await cursor.fetchall()
        
        await cursor.close()
        return [self._format_change(change) for change in changes]
    
    async def _apply_changes_to_region(self, changes: List[Dict[str, Any]], 
                                     source_region: str, target_region: str):
        """å°†å˜æ›´åº”ç”¨åˆ°ç›®æ ‡åŒºåŸŸ"""
        connection = self.region_connections[target_region]
        cursor = await connection.cursor()
        
        for change in changes:
            try:
                if change['operation'] == 'INSERT':
                    await self._apply_insert(cursor, change)
                elif change['operation'] == 'UPDATE':
                    conflict = await self._detect_conflict(cursor, change, source_region, target_region)
                    if conflict:
                        await self.sync_queue.put(conflict)
                    else:
                        await self._apply_update(cursor, change)
                elif change['operation'] == 'DELETE':
                    await self._apply_delete(cursor, change)
                
                await connection.commit()
                
            except Exception as e:
                await connection.rollback()
                print(f"åº”ç”¨å˜æ›´åˆ° {target_region} å¤±è´¥: {str(e)}")
        
        await cursor.close()
    
    async def _detect_conflict(self, cursor, change: Dict[str, Any], 
                             source_region: str, target_region: str) -> Optional[SyncConflict]:
        """æ£€æµ‹å†™å†²çª"""
        # æŸ¥è¯¢ç›®æ ‡åŒºåŸŸçš„å½“å‰æ•°æ®
        query = f"SELECT * FROM {change['table_name']} WHERE {change['primary_key_field']} = %s"
        await cursor.execute(query, (change['primary_key'],))
        target_data = await cursor.fetchone()
        
        if not target_data:
            return None
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å†²çªå­—æ®µè¢«ä¿®æ”¹
        conflicting_fields = []
        for field, new_value in change['changed_data'].items():
            if field in target_data and target_data[field] != new_value:
                conflicting_fields.append(field)
        
        if conflicting_fields:
            return SyncConflict(
                table_name=change['table_name'],
                primary_key=change['primary_key'],
                conflicting_fields=conflicting_fields,
                timestamps={
                    source_region: change['timestamp'],
                    target_region: time.time()  # ç®€åŒ–å¤„ç†
                },
                resolution_strategy='timestamp_wins'
            )
        
        return None
    
    async def _process_conflicts(self):
        """å¤„ç†åŒæ­¥å†²çª"""
        while True:
            try:
                conflict = await self.sync_queue.get()
                resolved_change = await self.conflict_resolver.resolve_conflict(conflict)
                
                # å°†è§£å†³åçš„å˜æ›´åº”ç”¨åˆ°æ‰€æœ‰åŒºåŸŸ
                for region in self.regions:
                    await self._apply_resolved_change(resolved_change, region)
                
                self.sync_queue.task_done()
                
            except Exception as e:
                print(f"å†²çªå¤„ç†å‡ºé”™: {str(e)}")
    
    async def _apply_resolved_change(self, change: Dict[str, Any], region: str):
        """åº”ç”¨å·²è§£å†³å†²çªçš„å˜æ›´"""
        connection = self.region_connections[region]
        cursor = await connection.cursor()
        
        try:
            if change['operation'] == 'UPDATE':
                await self._apply_update(cursor, change)
            await connection.commit()
        except Exception as e:
            await connection.rollback()
            print(f"åº”ç”¨è§£å†³åçš„å˜æ›´åˆ° {region} å¤±è´¥: {str(e)}")
        finally:
            await cursor.close()

class ConflictResolver:
    def __init__(self):
        self.resolution_strategies = {
            'timestamp_wins': self._timestamp_wins,
            'source_wins': self._source_wins,
            'merge_fields': self._merge_fields
        }
    
    async def resolve_conflict(self, conflict: SyncConflict) -> Dict[str, Any]:
        """è§£å†³åŒæ­¥å†²çª"""
        strategy_func = self.resolution_strategies.get(conflict.resolution_strategy)
        if strategy_func:
            return await strategy_func(conflict)
        else:
            # é»˜è®¤ä½¿ç”¨æ—¶é—´æˆ³ç­–ç•¥
            return await self._timestamp_wins(conflict)
    
    async def _timestamp_wins(self, conflict: SyncConflict) -> Dict[str, Any]:
        """æ—¶é—´æˆ³è·èƒœç­–ç•¥"""
        winning_region = max(conflict.timestamps.items(), key=lambda x: x[1])[0]
        # è¿”å›è·èƒœåŒºåŸŸçš„å˜æ›´
        return {
            'operation': 'UPDATE',
            'table_name': conflict.table_name,
            'primary_key': conflict.primary_key,
            'winning_region': winning_region,
            'resolved_data': {}  # å®é™…å®ç°ä¸­éœ€è¦è·å–è·èƒœåŒºåŸŸçš„æ•°æ®
        }
    
    async def _source_wins(self, conflict: SyncConflict) -> Dict[str, Any]:
        """æºåŒºåŸŸè·èƒœç­–ç•¥"""
        # å®ç°æºåŒºåŸŸè·èƒœé€»è¾‘
        return {}
    
    async def _merge_fields(self, conflict: SyncConflict) -> Dict[str, Any]:
        """å­—æ®µåˆå¹¶ç­–ç•¥"""
        # å®ç°å­—æ®µåˆå¹¶é€»è¾‘
        return {}

# ä½¿ç”¨ç¤ºä¾‹
async def main():
    # åŒºåŸŸé…ç½®
    regions = ['us-east-1', 'us-west-2', 'eu-west-1']
    db_configs = {
        'us-east-1': {
            'host': 'db-us-east-1.cluster.example.com',
            'port': 3306,
            'user': 'sync_user',
            'password: "${DB_PASSWORD}",
            'database': 'global_app'
        },
        'us-west-2': {
            'host': 'db-us-west-2.cluster.example.com',
            'port': 3306,
            'user': 'sync_user',
            'password: "${DB_PASSWORD}",
            'database': 'global_app'
        },
        'eu-west-1': {
            'host': 'db-eu-west-1.cluster.example.com',
            'port': 3306,
            'user': 'sync_user',
            'password: "${DB_PASSWORD}",
            'database': 'global_app'
        }
    }
    
    # åˆå§‹åŒ–åŒæ­¥ç®¡ç†å™¨
    sync_manager = ActiveActiveSyncManager(regions)
    await sync_manager.initialize_connections(db_configs)
    
    # å¯åŠ¨åŒå‘åŒæ­¥
    await sync_manager.start_bidirectional_sync()

# è¿è¡ŒåŒæ­¥ç®¡ç†å™¨
if __name__ == "__main__":
    asyncio.run(main())
```

### 2.2 Active-Passiveä¸»å¤‡æ¶æ„

#### ä¸»å¤‡æ¶æ„é…ç½®æ¨¡æ¿
```yaml
# active-passive-deployment.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: database-master
  namespace: multi-region
spec:
  serviceName: database-master
  replicas: 1
  selector:
    matchLabels:
      app: database
      role: master
  template:
    metadata:
      labels:
        app: database
        role: master
        region: primary
    spec:
      containers:
      - name: mysql-master
        image: mysql:8.0.35
        env:
        - name: MYSQL_ROOT_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mysql-root-password
              key: password
        - name: MYSQL_DATABASE
          value: "production_app"
        ports:
        - containerPort: 3306
        volumeMounts:
        - name: mysql-data
          mountPath: /var/lib/mysql
        livenessProbe:
          exec:
            command:
            - mysqladmin
            - ping
            - -h
            - localhost
          initialDelaySeconds: 120
          periodSeconds: 10
        readinessProbe:
          exec:
            command:
            - mysql
            - -h
            - localhost
            - -e
            - SELECT 1
          initialDelaySeconds: 60
          periodSeconds: 5
---
# åªè¯»å‰¯æœ¬é…ç½®
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: database-standby
  namespace: multi-region
spec:
  serviceName: database-standby
  replicas: 2
  selector:
    matchLabels:
      app: database
      role: standby
  template:
    metadata:
      labels:
        app: database
        role: standby
        region: standby
    spec:
      containers:
      - name: mysql-standby
        image: mysql:8.0.35
        env:
        - name: MYSQL_ROOT_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mysql-root-password
              key: password
        ports:
        - containerPort: 3306
        volumeMounts:
        - name: mysql-data
          mountPath: /var/lib/mysql
        livenessProbe:
          exec:
            command:
            - mysqladmin
            - ping
            - -h
            - localhost
          initialDelaySeconds: 120
          periodSeconds: 10
---
# è·¨åŒºåŸŸå¤åˆ¶é…ç½®
apiVersion: batch/v1
kind: CronJob
metadata:
  name: cross-region-replication
  namespace: multi-region
spec:
  schedule: "*/5 * * * *"  # æ¯5åˆ†é’Ÿæ‰§è¡Œä¸€æ¬¡
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: replication-job
            image: mysql:8.0.35
            command:
            - /bin/sh
            - -c
            - |
              # æ‰§è¡Œè·¨åŒºåŸŸå¤åˆ¶é€»è¾‘
              mysqldump -h $MASTER_HOST -u $MYSQL_USER -p$MYSQL_PASSWORD --single-transaction production_app | \
              mysql -h $STANDBY_HOST -u $MYSQL_USER -p$MYSQL_PASSWORD production_app
            env:
            - name: MASTER_HOST
              value: "database-master.multi-region.svc.cluster.local"
            - name: STANDBY_HOST
              value: "database-standby.multi-region.svc.cluster.local"
            - name: MYSQL_USER
              valueFrom:
                secretKeyRef:
                  name: mysql-replication-user
                  key: username
            - name: MYSQL_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: mysql-replication-user
                  key: password
          restartPolicy: OnFailure
```

## 3. æ•°æ®åŒæ­¥ä¸ä¸€è‡´æ€§

### 3.1 è·¨åŒºåŸŸæ•°æ®åŒæ­¥æœºåˆ¶

#### åŸºäºGTIDçš„MySQLè·¨åŒºåŸŸå¤åˆ¶
```python
# cross_region_replication_manager.py
import pymysql
import time
import threading
from typing import Dict, List, Any, Optional
import json
import hashlib

class CrossRegionReplicationManager:
    def __init__(self, master_config: Dict[str, Any], standby_configs: List[Dict[str, Any]]):
        self.master_config = master_config
        self.standby_configs = standby_configs
        self.replication_status = {}
        self.gtid_positions = {}
        
    def setup_cross_region_replication(self) -> bool:
        """è®¾ç½®è·¨åŒºåŸŸå¤åˆ¶"""
        try:
            # åœ¨ä¸»åº“ä¸Šå¯ç”¨GTID
            self._enable_gtid_on_master()
            
            # é…ç½®å„ä¸ªå¤‡åº“
            for standby_config in self.standby_configs:
                self._setup_standby_replication(standby_config)
            
            # å¯åŠ¨å¤åˆ¶ç›‘æ§
            self._start_replication_monitoring()
            
            return True
        except Exception as e:
            print(f"è·¨åŒºåŸŸå¤åˆ¶è®¾ç½®å¤±è´¥: {str(e)}")
            return False
    
    def _enable_gtid_on_master(self):
        """åœ¨ä¸»åº“ä¸Šå¯ç”¨GTID"""
        connection = pymysql.connect(**self.master_config)
        cursor = connection.cursor()
        
        # æ£€æŸ¥GTIDæ˜¯å¦å·²å¯ç”¨
        cursor.execute("SHOW VARIABLES LIKE 'gtid_mode'")
        gtid_mode = cursor.fetchone()[1]
        
        if gtid_mode != 'ON':
            # å¯ç”¨GTIDï¼ˆéœ€è¦é‡å¯MySQLï¼‰
            cursor.execute("SET GLOBAL gtid_mode = ON_PERMISSIVE")
            cursor.execute("SET GLOBAL enforce_gtid_consistency = ON")
            print("GTIDå·²åœ¨ä¸»åº“ä¸Šå¯ç”¨")
        
        cursor.close()
        connection.close()
    
    def _setup_standby_replication(self, standby_config: Dict[str, Any]):
        """è®¾ç½®å¤‡åº“å¤åˆ¶"""
        standby_connection = pymysql.connect(**standby_config)
        standby_cursor = standby_connection.cursor()
        
        # é…ç½®å¤åˆ¶ç”¨æˆ·
        standby_cursor.execute("""
            CREATE USER IF NOT EXISTS 'repl_user'@'%' IDENTIFIED BY 'repl_password'
        """)
        standby_cursor.execute("""
            GRANT REPLICATION SLAVE ON *.* TO 'repl_user'@'%'
        """)
        
        # è·å–ä¸»åº“çš„GTIDä½ç½®
        master_connection = pymysql.connect(**self.master_config)
        master_cursor = master_connection.cursor()
        master_cursor.execute("SELECT @@GLOBAL.gtid_executed")
        gtid_position = master_cursor.fetchone()[0]
        master_cursor.close()
        master_connection.close()
        
        # åœ¨å¤‡åº“ä¸Šé…ç½®å¤åˆ¶
        standby_cursor.execute(f"""
            CHANGE MASTER TO
            MASTER_HOST='{self.master_config['host']}',
            MASTER_PORT={self.master_config['port']},
            MASTER_USER='repl_user',
            MASTER_password: "${DB_PASSWORD}",
            MASTER_AUTO_POSITION=1
        """)
        
        standby_cursor.execute("START SLAVE")
        print(f"å¤‡åº“ {standby_config['host']} å¤åˆ¶å·²å¯åŠ¨")
        
        standby_cursor.close()
        standby_connection.close()
        
        # è®°å½•åˆå§‹GTIDä½ç½®
        self.gtid_positions[standby_config['host']] = gtid_position
    
    def _start_replication_monitoring(self):
        """å¯åŠ¨å¤åˆ¶ç›‘æ§"""
        def monitor_loop():
            while True:
                try:
                    self._check_replication_status()
                    self._detect_replication_lag()
                    time.sleep(30)  # æ¯30ç§’æ£€æŸ¥ä¸€æ¬¡
                except Exception as e:
                    print(f"å¤åˆ¶ç›‘æ§å‡ºé”™: {str(e)}")
                    time.sleep(60)
        
        monitor_thread = threading.Thread(target=monitor_loop, daemon=True)
        monitor_thread.start()
    
    def _check_replication_status(self):
        """æ£€æŸ¥å¤åˆ¶çŠ¶æ€"""
        for standby_config in self.standby_configs:
            try:
                connection = pymysql.connect(**standby_config)
                cursor = connection.cursor()
                
                cursor.execute("SHOW SLAVE STATUS")
                slave_status = cursor.fetchone()
                
                if slave_status:
                    io_running = slave_status[10]  # Slave_IO_Running
                    sql_running = slave_status[11]  # Slave_SQL_Running
                    last_error = slave_status[19]   # Last_Error
                    
                    self.replication_status[standby_config['host']] = {
                        'io_running': io_running == 'Yes',
                        'sql_running': sql_running == 'Yes',
                        'last_error': last_error,
                        'timestamp': time.time()
                    }
                
                cursor.close()
                connection.close()
                
            except Exception as e:
                print(f"æ£€æŸ¥ {standby_config['host']} å¤åˆ¶çŠ¶æ€å¤±è´¥: {str(e)}")
                self.replication_status[standby_config['host']] = {
                    'io_running': False,
                    'sql_running': False,
                    'last_error': str(e),
                    'timestamp': time.time()
                }
    
    def _detect_replication_lag(self):
        """æ£€æµ‹å¤åˆ¶å»¶è¿Ÿ"""
        master_connection = pymysql.connect(**self.master_config)
        master_cursor = master_connection.cursor()
        
        # è·å–ä¸»åº“å½“å‰GTID
        master_cursor.execute("SELECT @@GLOBAL.gtid_executed")
        master_gtid = master_cursor.fetchone()[0]
        
        master_cursor.close()
        master_connection.close()
        
        # æ¯”è¾ƒå„å¤‡åº“çš„GTID
        for standby_config in self.standby_configs:
            try:
                standby_connection = pymysql.connect(**standby_config)
                standby_cursor = standby_connection.cursor()
                
                standby_cursor.execute("SELECT @@GLOBAL.gtid_executed")
                standby_gtid = standby_cursor.fetchone()[0]
                
                # è®¡ç®—GTIDå·®å¼‚ï¼ˆç®€åŒ–å¤„ç†ï¼‰
                lag = self._calculate_gtid_lag(master_gtid, standby_gtid)
                
                if standby_config['host'] in self.replication_status:
                    self.replication_status[standby_config['host']]['lag_seconds'] = lag
                
                standby_cursor.close()
                standby_connection.close()
                
            except Exception as e:
                print(f"æ£€æµ‹ {standby_config['host']} å¤åˆ¶å»¶è¿Ÿå¤±è´¥: {str(e)}")
    
    def _calculate_gtid_lag(self, master_gtid: str, standby_gtid: str) -> int:
        """è®¡ç®—GTIDå»¶è¿Ÿï¼ˆç®€åŒ–å®ç°ï¼‰"""
        # å®é™…å®ç°ä¸­éœ€è¦è§£æGTIDé›†åˆæ¥è®¡ç®—ç²¾ç¡®å»¶è¿Ÿ
        # è¿™é‡Œç®€åŒ–ä¸ºåŸºäºå­—ç¬¦ä¸²é•¿åº¦çš„ç²—ç•¥ä¼°è®¡
        return abs(len(master_gtid) - len(standby_gtid)) // 100
    
    def get_replication_health(self) -> Dict[str, Any]:
        """è·å–å¤åˆ¶å¥åº·çŠ¶å†µ"""
        unhealthy_stanbys = []
        healthy_stanbys = []
        
        for host, status in self.replication_status.items():
            if status['io_running'] and status['sql_running'] and status.get('lag_seconds', 0) < 30:
                healthy_stanbys.append(host)
            else:
                unhealthy_stanbys.append({
                    'host': host,
                    'issues': self._get_replication_issues(status)
                })
        
        return {
            'healthy_standbys': healthy_stanbys,
            'unhealthy_standbys': unhealthy_stanbys,
            'overall_health': len(healthy_stanbys) / len(self.standby_configs) if self.standby_configs else 0
        }
    
    def _get_replication_issues(self, status: Dict[str, Any]) -> List[str]:
        """è·å–å¤åˆ¶é—®é¢˜åˆ—è¡¨"""
        issues = []
        if not status['io_running']:
            issues.append("IOçº¿ç¨‹æœªè¿è¡Œ")
        if not status['sql_running']:
            issues.append("SQLçº¿ç¨‹æœªè¿è¡Œ")
        if status.get('lag_seconds', 0) > 30:
            issues.append(f"å¤åˆ¶å»¶è¿Ÿè¿‡é«˜: {status['lag_seconds']}ç§’")
        if status['last_error']:
            issues.append(f"æœ€åé”™è¯¯: {status['last_error']}")
        return issues
    
    def promote_standby(self, standby_host: str) -> bool:
        """æå‡å¤‡åº“ä¸ºä¸»åº“"""
        try:
            # æ‰¾åˆ°å¯¹åº”çš„é…ç½®
            standby_config = next((config for config in self.standby_configs 
                                 if config['host'] == standby_host), None)
            
            if not standby_config:
                raise ValueError(f"æ‰¾ä¸åˆ°å¤‡åº“é…ç½®: {standby_host}")
            
            # åœæ­¢å¤åˆ¶
            connection = pymysql.connect(**standby_config)
            cursor = connection.cursor()
            cursor.execute("STOP SLAVE")
            
            # é‡ç½®å¤åˆ¶é…ç½®
            cursor.execute("RESET SLAVE ALL")
            
            # æå‡ä¸ºæ–°ä¸»åº“
            cursor.execute("SET GLOBAL read_only = OFF")
            
            cursor.close()
            connection.close()
            
            print(f"å¤‡åº“ {standby_host} å·²æˆåŠŸæå‡ä¸ºä¸»åº“")
            return True
            
        except Exception as e:
            print(f"æå‡å¤‡åº“å¤±è´¥: {str(e)}")
            return False

# ä½¿ç”¨ç¤ºä¾‹
master_config = {
    'host': 'master-db.us-east-1.example.com',
    'port': 3306,
    'user': 'admin',
    'password: "${DB_PASSWORD}",
    'database': 'production_app'
}

standby_configs = [
    {
        'host': 'standby-db.us-west-2.example.com',
        'port': 3306,
        'user': 'admin',
        'password: "${DB_PASSWORD}",
        'database': 'production_app'
    },
    {
        'host': 'standby-db.eu-west-1.example.com',
        'port': 3306,
        'user': 'admin',
        'password: "${DB_PASSWORD}",
        'database': 'production_app'
    }
]

replication_manager = CrossRegionReplicationManager(master_config, standby_configs)
success = replication_manager.setup_cross_region_replication()

if success:
    # ç›‘æ§å¤åˆ¶å¥åº·çŠ¶å†µ
    while True:
        health = replication_manager.get_replication_health()
        print(f"å¤åˆ¶å¥åº·çŠ¶å†µ: {health['overall_health']:.2%}")
        print(f"å¥åº·å¤‡åº“: {health['healthy_standbys']}")
        print(f"é—®é¢˜å¤‡åº“: {health['unhealthy_standbys']}")
        time.sleep(60)
```

### 3.2 ä¸€è‡´æ€§ä¿è¯æœºåˆ¶

#### åˆ†å¸ƒå¼ä¸€è‡´æ€§åè®®å®ç°
```python
# distributed_consistency_protocol.py
import asyncio
import hashlib
import time
from typing import Dict, List, Any, Optional
from enum import Enum
from dataclasses import dataclass

class ConsistencyLevel(Enum):
    STRONG = "strong"
    SEQUENTIAL = "sequential"
    EVENTUAL = "eventual"
    CAUSAL = "causal"

@dataclass
class QuorumConfig:
    read_quorum: int
    write_quorum: int
    total_nodes: int

class DistributedConsistencyManager:
    def __init__(self, regions: List[str], consistency_level: ConsistencyLevel):
        self.regions = regions
        self.consistency_level = consistency_level
        self.node_states = {region: {'status': 'healthy', 'last_heartbeat': time.time()} 
                          for region in regions}
        self.version_vectors = {}  # ç‰ˆæœ¬å‘é‡ç”¨äºå› æœä¸€è‡´æ€§
        
    def get_quorum_config(self) -> QuorumConfig:
        """è·å–ä»²è£é…ç½®"""
        total_nodes = len(self.regions)
        
        if self.consistency_level == ConsistencyLevel.STRONG:
            # å¼ºä¸€è‡´æ€§éœ€è¦æ‰€æœ‰èŠ‚ç‚¹å‚ä¸
            return QuorumConfig(
                read_quorum=total_nodes,
                write_quorum=total_nodes,
                total_nodes=total_nodes
            )
        elif self.consistency_level == ConsistencyLevel.SEQUENTIAL:
            # é¡ºåºä¸€è‡´æ€§åªéœ€è¦å¤šæ•°èŠ‚ç‚¹
            majority = total_nodes // 2 + 1
            return QuorumConfig(
                read_quorum=majority,
                write_quorum=majority,
                total_nodes=total_nodes
            )
        elif self.consistency_level == ConsistencyLevel.CAUSAL:
            # å› æœä¸€è‡´æ€§éœ€è¦è·Ÿè¸ªä¾èµ–å…³ç³»
            return QuorumConfig(
                read_quorum=1,  # å¯ä»¥ä»ä»»ä¸€èŠ‚ç‚¹è¯»å–
                write_quorum=1,  # å†™å…¥åˆ°ä¸€ä¸ªèŠ‚ç‚¹
                total_nodes=total_nodes
            )
        else:  # EVENTUAL
            # æœ€ç»ˆä¸€è‡´æ€§ï¼Œå¯ä»¥å¼‚æ­¥å¤„ç†
            return QuorumConfig(
                read_quorum=1,
                write_quorum=1,
                total_nodes=total_nodes
            )
    
    async def write_with_consistency(self, key: str, value: Any, 
                                   context: Optional[Dict[str, Any]] = None) -> bool:
        """å¸¦ä¸€è‡´æ€§ä¿è¯çš„å†™å…¥æ“ä½œ"""
        quorum_config = self.get_quorum_config()
        
        if self.consistency_level == ConsistencyLevel.STRONG:
            return await self._strong_consistency_write(key, value, quorum_config)
        elif self.consistency_level == ConsistencyLevel.SEQUENTIAL:
            return await self._sequential_consistency_write(key, value, quorum_config)
        elif self.consistency_level == ConsistencyLevel.CAUSAL:
            return await self._causal_consistency_write(key, value, context, quorum_config)
        else:
            return await self._eventual_consistency_write(key, value)
    
    async def read_with_consistency(self, key: str) -> Optional[Any]:
        """å¸¦ä¸€è‡´æ€§ä¿è¯çš„è¯»å–æ“ä½œ"""
        quorum_config = self.get_quorum_config()
        
        if self.consistency_level == ConsistencyLevel.STRONG:
            return await self._strong_consistency_read(key, quorum_config)
        elif self.consistency_level == ConsistencyLevel.SEQUENTIAL:
            return await self._sequential_consistency_read(key, quorum_config)
        elif self.consistency_level == ConsistencyLevel.CAUSAL:
            return await self._causal_consistency_read(key, quorum_config)
        else:
            return await self._eventual_consistency_read(key)
    
    async def _strong_consistency_write(self, key: str, value: Any, 
                                      quorum_config: QuorumConfig) -> bool:
        """å¼ºä¸€è‡´æ€§å†™å…¥"""
        # 1. é¢„å†™é˜¶æ®µï¼šè·å–æ‰€æœ‰èŠ‚ç‚¹çš„å‡†å¤‡ç¡®è®¤
        prepare_tasks = []
        for region in self.regions:
            task = asyncio.create_task(self._prepare_write(region, key, value))
            prepare_tasks.append(task)
        
        prepare_results = await asyncio.gather(*prepare_tasks, return_exceptions=True)
        
        # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°å†™ä»²è£æ•°
        successful_prepares = sum(1 for result in prepare_results if result is True)
        if successful_prepares < quorum_config.write_quorum:
            return False
        
        # 2. æäº¤é˜¶æ®µï¼šåœ¨è¾¾æˆå…±è¯†çš„èŠ‚ç‚¹ä¸Šæäº¤å†™å…¥
        commit_tasks = []
        for i, result in enumerate(prepare_results):
            if result is True:
                region = self.regions[i]
                task = asyncio.create_task(self._commit_write(region, key, value))
                commit_tasks.append(task)
        
        commit_results = await asyncio.gather(*commit_tasks, return_exceptions=True)
        successful_commits = sum(1 for result in commit_results if result is True)
        
        return successful_commits >= quorum_config.write_quorum
    
    async def _strong_consistency_read(self, key: str, 
                                     quorum_config: QuorumConfig) -> Optional[Any]:
        """å¼ºä¸€è‡´æ€§è¯»å–"""
        # ä»æ‰€æœ‰èŠ‚ç‚¹è¯»å–å¹¶è¿”å›æœ€æ–°ç‰ˆæœ¬
        read_tasks = []
        for region in self.regions:
            task = asyncio.create_task(self._read_from_region(region, key))
            read_tasks.append(task)
        
        read_results = await asyncio.gather(*read_tasks, return_exceptions=True)
        
        # æ‰¾åˆ°æœ€æ–°çš„æ•°æ®ç‰ˆæœ¬
        latest_value = None
        latest_version = 0
        
        for result in read_results:
            if isinstance(result, tuple) and len(result) == 2:
                value, version = result
                if version > latest_version:
                    latest_value = value
                    latest_version = version
        
        return latest_value
    
    async def _causal_consistency_write(self, key: str, value: Any, 
                                      context: Optional[Dict[str, Any]], 
                                      quorum_config: QuorumConfig) -> bool:
        """å› æœä¸€è‡´æ€§å†™å…¥"""
        # æ›´æ–°ç‰ˆæœ¬å‘é‡
        if context and 'causal_context' in context:
            causal_vector = context['causal_context']
            self._update_version_vector(causal_vector)
        
        # æ·»åŠ å½“å‰å†™å…¥çš„å› æœå…³ç³»
        current_vector = self.version_vectors.copy()
        current_vector[key] = current_vector.get(key, 0) + 1
        
        # å†™å…¥åˆ°ä¸€ä¸ªåŒºåŸŸï¼ˆå› æœä¸€è‡´æ€§å…è®¸ï¼‰
        target_region = self._select_healthy_region()
        if target_region:
            success = await self._write_to_region(target_region, key, value, current_vector)
            if success:
                # æ›´æ–°æœ¬åœ°ç‰ˆæœ¬å‘é‡
                self.version_vectors = current_vector
                return True
        
        return False
    
    async def _causal_consistency_read(self, key: str, 
                                     quorum_config: QuorumConfig) -> Optional[Any]:
        """å› æœä¸€è‡´æ€§è¯»å–"""
        # æ ¹æ®ç‰ˆæœ¬å‘é‡é€‰æ‹©åˆé€‚çš„åŒºåŸŸè¯»å–
        target_region = self._select_region_by_causal_context(key)
        if target_region:
            return await self._read_from_region(target_region, key)
        return None
    
    def _update_version_vector(self, external_vector: Dict[str, int]):
        """æ›´æ–°ç‰ˆæœ¬å‘é‡"""
        for key, version in external_vector.items():
            if key in self.version_vectors:
                self.version_vectors[key] = max(self.version_vectors[key], version)
            else:
                self.version_vectors[key] = version
    
    def _select_healthy_region(self) -> Optional[str]:
        """é€‰æ‹©å¥åº·çš„åŒºåŸŸ"""
        healthy_regions = [region for region, state in self.node_states.items() 
                          if state['status'] == 'healthy']
        return healthy_regions[0] if healthy_regions else None
    
    def _select_region_by_causal_context(self, key: str) -> Optional[str]:
        """æ ¹æ®å› æœä¸Šä¸‹æ–‡é€‰æ‹©åŒºåŸŸ"""
        # ç®€åŒ–å®ç°ï¼šé€‰æ‹©ç‰ˆæœ¬æœ€é«˜çš„åŒºåŸŸ
        max_version = -1
        selected_region = None
        
        for region in self.regions:
            # å®é™…å®ç°ä¸­éœ€è¦æŸ¥è¯¢å„åŒºåŸŸçš„ç‰ˆæœ¬ä¿¡æ¯
            region_version = self.version_vectors.get(key, 0)
            if region_version > max_version:
                max_version = region_version
                selected_region = region
        
        return selected_region
    
    async def _prepare_write(self, region: str, key: str, value: Any) -> bool:
        """å‡†å¤‡å†™å…¥é˜¶æ®µ"""
        # æ¨¡æ‹Ÿç½‘ç»œè¯·æ±‚
        await asyncio.sleep(0.1)
        return self.node_states[region]['status'] == 'healthy'
    
    async def _commit_write(self, region: str, key: str, value: Any) -> bool:
        """æäº¤å†™å…¥é˜¶æ®µ"""
        await asyncio.sleep(0.05)
        return True
    
    async def _read_from_region(self, region: str, key: str) -> Optional[tuple]:
        """ä»æŒ‡å®šåŒºåŸŸè¯»å–æ•°æ®"""
        await asyncio.sleep(0.05)
        # æ¨¡æ‹Ÿè¿”å›æ•°æ®å’Œç‰ˆæœ¬
        return ("sample_value", 1)
    
    async def _write_to_region(self, region: str, key: str, value: Any, 
                             version_vector: Dict[str, int]) -> bool:
        """å‘æŒ‡å®šåŒºåŸŸå†™å…¥æ•°æ®"""
        await asyncio.sleep(0.1)
        return True

# ä½¿ç”¨ç¤ºä¾‹
async def main():
    regions = ['us-east-1', 'us-west-2', 'eu-west-1']
    
    # å¼ºä¸€è‡´æ€§ç®¡ç†å™¨
    strong_consistency = DistributedConsistencyManager(
        regions, ConsistencyLevel.STRONG
    )
    
    # å› æœä¸€è‡´æ€§ç®¡ç†å™¨
    causal_consistency = DistributedConsistencyManager(
        regions, ConsistencyLevel.CAUSAL
    )
    
    # æµ‹è¯•å†™å…¥æ“ä½œ
    print("æµ‹è¯•å¼ºä¸€è‡´æ€§å†™å…¥...")
    success = await strong_consistency.write_with_consistency("user:123", {"name": "John"})
    print(f"å¼ºä¸€è‡´æ€§å†™å…¥ç»“æœ: {success}")
    
    print("\næµ‹è¯•å› æœä¸€è‡´æ€§å†™å…¥...")
    context = {"causal_context": {"user:123": 1}}
    success = await causal_consistency.write_with_consistency("order:456", {"amount": 100}, context)
    print(f"å› æœä¸€è‡´æ€§å†™å…¥ç»“æœ: {success}")
    
    # æµ‹è¯•è¯»å–æ“ä½œ
    print("\næµ‹è¯•å¼ºä¸€è‡´æ€§è¯»å–...")
    value = await strong_consistency.read_with_consistency("user:123")
    print(f"å¼ºä¸€è‡´æ€§è¯»å–ç»“æœ: {value}")
    
    print("\næµ‹è¯•å› æœä¸€è‡´æ€§è¯»å–...")
    value = await causal_consistency.read_with_consistency("order:456")
    print(f"å› æœä¸€è‡´æ€§è¯»å–ç»“æœ: {value}")

# è¿è¡Œæµ‹è¯•
if __name__ == "__main__":
    asyncio.run(main())
```

## 4. æ•…éšœåˆ‡æ¢ä¸å®¹ç¾

### 4.1 è‡ªåŠ¨æ•…éšœæ£€æµ‹ä¸åˆ‡æ¢

#### æ™ºèƒ½æ•…éšœåˆ‡æ¢ç³»ç»Ÿ
```python
# intelligent_failover_system.py
import asyncio
import time
import json
from typing import Dict, List, Any, Optional
from enum import Enum
from dataclasses import dataclass

class FailoverReason(Enum):
    NETWORK_PARTITION = "network_partition"
    NODE_FAILURE = "node_failure"
    REGION_OUTAGE = "region_outage"
    PERFORMANCE_DEGRADATION = "performance_degradation"

@dataclass
class HealthCheckResult:
    region: str
    status: str  # healthy, degraded, failed
    response_time: float
    error_count: int
    last_check: float

class IntelligentFailoverSystem:
    def __init__(self, regions: List[str], primary_region: str):
        self.regions = regions
        self.primary_region = primary_region
        self.health_status = {region: HealthCheckResult(region, 'unknown', 0, 0, 0) 
                            for region in regions}
        self.failover_history = []
        self.failover_lock = asyncio.Lock()
        
    async def start_health_monitoring(self):
        """å¯åŠ¨å¥åº·ç›‘æ§"""
        while True:
            try:
                await self._perform_health_checks()
                await self._evaluate_failover_conditions()
                await asyncio.sleep(30)  # æ¯30ç§’æ£€æŸ¥ä¸€æ¬¡
            except Exception as e:
                print(f"å¥åº·ç›‘æ§å‡ºé”™: {str(e)}")
                await asyncio.sleep(60)
    
    async def _perform_health_checks(self):
        """æ‰§è¡Œå¥åº·æ£€æŸ¥"""
        check_tasks = []
        for region in self.regions:
            task = asyncio.create_task(self._check_region_health(region))
            check_tasks.append(task)
        
        results = await asyncio.gather(*check_tasks, return_exceptions=True)
        
        for i, result in enumerate(results):
            region = self.regions[i]
            if isinstance(result, HealthCheckResult):
                self.health_status[region] = result
            else:
                # æ£€æŸ¥å¤±è´¥
                self.health_status[region] = HealthCheckResult(
                    region, 'failed', 0, 1, time.time()
                )
    
    async def _check_region_health(self, region: str) -> HealthCheckResult:
        """æ£€æŸ¥å•ä¸ªåŒºåŸŸå¥åº·çŠ¶å†µ"""
        start_time = time.time()
        
        try:
            # æ¨¡æ‹Ÿå¥åº·æ£€æŸ¥ï¼ˆå®é™…å®ç°ä¸­è°ƒç”¨çœŸå®çš„å¥åº·æ£€æŸ¥æ¥å£ï¼‰
            await asyncio.sleep(0.1)  # æ¨¡æ‹Ÿç½‘ç»œå»¶è¿Ÿ
            
            # æ£€æŸ¥æ•°æ®åº“è¿æ¥
            db_healthy = await self._check_database_connectivity(region)
            
            # æ£€æŸ¥åº”ç”¨æœåŠ¡
            app_healthy = await self._check_application_health(region)
            
            response_time = time.time() - start_time
            
            if db_healthy and app_healthy and response_time < 1.0:
                status = 'healthy'
                error_count = 0
            elif db_healthy and response_time < 3.0:
                status = 'degraded'
                error_count = 1
            else:
                status = 'failed'
                error_count = 3
            
        except Exception:
            status = 'failed'
            response_time = 0
            error_count = 3
        
        return HealthCheckResult(
            region=region,
            status=status,
            response_time=response_time,
            error_count=error_count,
            last_check=time.time()
        )
    
    async def _check_database_connectivity(self, region: str) -> bool:
        """æ£€æŸ¥æ•°æ®åº“è¿æ¥æ€§"""
        # å®é™…å®ç°ä¸­è¿æ¥æ•°æ®åº“æ‰§è¡Œç®€å•æŸ¥è¯¢
        await asyncio.sleep(0.05)
        return True  # ç®€åŒ–å¤„ç†
    
    async def _check_application_health(self, region: str) -> bool:
        """æ£€æŸ¥åº”ç”¨å¥åº·çŠ¶å†µ"""
        # å®é™…å®ç°ä¸­è°ƒç”¨å¥åº·æ£€æŸ¥ç«¯ç‚¹
        await asyncio.sleep(0.05)
        return True  # ç®€åŒ–å¤„ç†
    
    async def _evaluate_failover_conditions(self):
        """è¯„ä¼°æ•…éšœåˆ‡æ¢æ¡ä»¶"""
        primary_status = self.health_status[self.primary_region]
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ•…éšœåˆ‡æ¢
        if self._should_failover(primary_status):
            failover_reason = self._determine_failover_reason(primary_status)
            await self._initiate_failover(failover_reason)
    
    def _should_failover(self, primary_status: HealthCheckResult) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥æ•…éšœåˆ‡æ¢"""
        # è¿ç»­3æ¬¡æ£€æŸ¥å¤±è´¥
        if primary_status.error_count >= 3:
            return True
        
        # å“åº”æ—¶é—´è¿‡é•¿ä¸”çŠ¶æ€ä¸ºå¤±è´¥
        if primary_status.response_time > 5.0 and primary_status.status == 'failed':
            return True
        
        # çŠ¶æ€ä¸ºé™çº§ä¸”æŒç»­æ—¶é—´è¶…è¿‡é˜ˆå€¼
        if (primary_status.status == 'degraded' and 
            time.time() - primary_status.last_check > 300):  # 5åˆ†é’Ÿ
            return True
        
        return False
    
    def _determine_failover_reason(self, primary_status: HealthCheckResult) -> FailoverReason:
        """ç¡®å®šæ•…éšœåˆ‡æ¢åŸå› """
        if primary_status.error_count >= 3:
            return FailoverReason.NODE_FAILURE
        elif primary_status.response_time > 5.0:
            return FailoverReason.PERFORMANCE_DEGRADATION
        elif primary_status.status == 'degraded':
            return FailoverReason.NETWORK_PARTITION
        else:
            return FailoverReason.REGION_OUTAGE
    
    async def _initiate_failover(self, reason: FailoverReason):
        """å‘èµ·æ•…éšœåˆ‡æ¢"""
        async with self.failover_lock:
            # æ£€æŸ¥æ˜¯å¦å·²ç»åœ¨è¿›è¡Œæ•…éšœåˆ‡æ¢
            if self._is_failover_in_progress():
                print("æ•…éšœåˆ‡æ¢å·²åœ¨è¿›è¡Œä¸­ï¼Œè·³è¿‡æœ¬æ¬¡åˆ‡æ¢")
                return
            
            print(f"å¼€å§‹æ•…éšœåˆ‡æ¢ï¼ŒåŸå› : {reason.value}")
            
            # é€‰æ‹©æ–°çš„ä¸»åŒºåŸŸ
            new_primary = self._select_new_primary()
            if not new_primary:
                print("æ²¡æœ‰å¯ç”¨çš„å¤‡é€‰åŒºåŸŸï¼Œæ•…éšœåˆ‡æ¢å¤±è´¥")
                return
            
            # æ‰§è¡Œæ•…éšœåˆ‡æ¢
            success = await self._execute_failover(new_primary, reason)
            
            if success:
                # æ›´æ–°é…ç½®
                self.primary_region = new_primary
                print(f"æ•…éšœåˆ‡æ¢æˆåŠŸï¼Œæ–°çš„ä¸»åŒºåŸŸ: {new_primary}")
                
                # è®°å½•æ•…éšœåˆ‡æ¢å†å²
                self._record_failover(reason, new_primary)
            else:
                print("æ•…éšœåˆ‡æ¢æ‰§è¡Œå¤±è´¥")
    
    def _is_failover_in_progress(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦æ­£åœ¨è¿›è¡Œæ•…éšœåˆ‡æ¢"""
        # å®é™…å®ç°ä¸­æ£€æŸ¥åˆ†å¸ƒå¼é”æˆ–çŠ¶æ€æ ‡è®°
        return False
    
    def _select_new_primary(self) -> Optional[str]:
        """é€‰æ‹©æ–°çš„ä¸»åŒºåŸŸ"""
        # ä¼˜å…ˆé€‰æ‹©å¥åº·çš„åŒºåŸŸ
        healthy_regions = [region for region, status in self.health_status.items()
                          if status.status == 'healthy' and region != self.primary_region]
        
        if healthy_regions:
            # é€‰æ‹©å»¶è¿Ÿæœ€ä½çš„å¥åº·åŒºåŸŸ
            return min(healthy_regions, key=lambda r: self.health_status[r].response_time)
        
        # å¦‚æœæ²¡æœ‰å¥åº·åŒºåŸŸï¼Œé€‰æ‹©é™çº§åŒºåŸŸ
        degraded_regions = [region for region, status in self.health_status.items()
                           if status.status == 'degraded' and region != self.primary_region]
        
        return degraded_regions[0] if degraded_regions else None
    
    async def _execute_failover(self, new_primary: str, reason: FailoverReason) -> bool:
        """æ‰§è¡Œæ•…éšœåˆ‡æ¢"""
        try:
            # 1. åœæ­¢å‘æ—§ä¸»åŒºåŸŸçš„å†™å…¥
            await self._stop_writes_to_old_primary()
            
            # 2. ç¡®ä¿æ–°ä¸»åŒºåŸŸæ•°æ®æ˜¯æœ€æ–°çš„
            if reason != FailoverReason.PERFORMANCE_DEGRADATION:
                await self._ensure_data_consistency(new_primary)
            
            # 3. æå‡æ–°ä¸»åŒºåŸŸ
            await self._promote_new_primary(new_primary)
            
            # 4. æ›´æ–°DNSå’ŒæœåŠ¡å‘ç°
            await self._update_service_discovery(new_primary)
            
            # 5. é‡æ–°é…ç½®åº”ç”¨è¿æ¥
            await self._reconfigure_application_connections(new_primary)
            
            # 6. å¯åŠ¨æ–°ä¸»åŒºåŸŸçš„å†™å…¥
            await self._start_writes_to_new_primary(new_primary)
            
            return True
            
        except Exception as e:
            print(f"æ•…éšœåˆ‡æ¢æ‰§è¡Œå¤±è´¥: {str(e)}")
            # å°è¯•å›æ»š
            await self._rollback_failover()
            return False
    
    async def _stop_writes_to_old_primary(self):
        """åœæ­¢å‘æ—§ä¸»åŒºåŸŸçš„å†™å…¥"""
        print("åœæ­¢å‘æ—§ä¸»åŒºåŸŸçš„å†™å…¥")
        await asyncio.sleep(1)  # æ¨¡æ‹Ÿæ“ä½œ
    
    async def _ensure_data_consistency(self, new_primary: str):
        """ç¡®ä¿æ–°ä¸»åŒºåŸŸæ•°æ®ä¸€è‡´æ€§"""
        print(f"ç¡®ä¿ {new_primary} æ•°æ®ä¸€è‡´æ€§")
        await asyncio.sleep(2)  # æ¨¡æ‹Ÿæ•°æ®åŒæ­¥ç­‰å¾…
    
    async def _promote_new_primary(self, new_primary: str):
        """æå‡æ–°ä¸»åŒºåŸŸ"""
        print(f"æå‡ {new_primary} ä¸ºæ–°ä¸»åŒºåŸŸ")
        await asyncio.sleep(1)  # æ¨¡æ‹Ÿæå‡æ“ä½œ
    
    async def _update_service_discovery(self, new_primary: str):
        """æ›´æ–°æœåŠ¡å‘ç°é…ç½®"""
        print(f"æ›´æ–°æœåŠ¡å‘ç°æŒ‡å‘ {new_primary}")
        await asyncio.sleep(1)  # æ¨¡æ‹ŸDNSæ›´æ–°
    
    async def _reconfigure_application_connections(self, new_primary: str):
        """é‡æ–°é…ç½®åº”ç”¨è¿æ¥"""
        print(f"é‡æ–°é…ç½®åº”ç”¨è¿æ¥åˆ° {new_primary}")
        await asyncio.sleep(1)  # æ¨¡æ‹Ÿè¿æ¥é‡é…ç½®
    
    async def _start_writes_to_new_primary(self, new_primary: str):
        """å¼€å§‹å‘æ–°ä¸»åŒºåŸŸå†™å…¥"""
        print(f"å¼€å§‹å‘ {new_primary} å†™å…¥æ•°æ®")
        await asyncio.sleep(1)  # æ¨¡æ‹Ÿå†™å…¥å¯åŠ¨
    
    async def _rollback_failover(self):
        """å›æ»šæ•…éšœåˆ‡æ¢"""
        print("æ‰§è¡Œæ•…éšœåˆ‡æ¢å›æ»š")
        await asyncio.sleep(2)  # æ¨¡æ‹Ÿå›æ»šæ“ä½œ
    
    def _record_failover(self, reason: FailoverReason, new_primary: str):
        """è®°å½•æ•…éšœåˆ‡æ¢å†å²"""
        failover_record = {
            'timestamp': time.time(),
            'reason': reason.value,
            'old_primary': self.primary_region,
            'new_primary': new_primary,
            'duration_seconds': 0  # å®é™…å®ç°ä¸­è®¡ç®—åˆ‡æ¢è€—æ—¶
        }
        self.failover_history.append(failover_record)
    
    def get_failover_status(self) -> Dict[str, Any]:
        """è·å–æ•…éšœåˆ‡æ¢çŠ¶æ€"""
        return {
            'current_primary': self.primary_region,
            'health_status': {region: status.__dict__ for region, status in self.health_status.items()},
            'recent_failovers': self.failover_history[-5:],  # æœ€è¿‘5æ¬¡æ•…éšœåˆ‡æ¢
            'failover_enabled': True
        }

# ä½¿ç”¨ç¤ºä¾‹
async def main():
    regions = ['us-east-1', 'us-west-2', 'eu-west-1']
    failover_system = IntelligentFailoverSystem(regions, 'us-east-1')
    
    # å¯åŠ¨å¥åº·ç›‘æ§
    monitor_task = asyncio.create_task(failover_system.start_health_monitoring())
    
    # æ¨¡æ‹Ÿè¿è¡Œä¸€æ®µæ—¶é—´
    for i in range(10):
        status = failover_system.get_failover_status()
        print(f"ç¬¬ {i+1} åˆ†é’ŸçŠ¶æ€: å½“å‰ä¸»åŒºåŸŸ {status['current_primary']}")
        await asyncio.sleep(60)
    
    # å–æ¶ˆç›‘æ§ä»»åŠ¡
    monitor_task.cancel()

# è¿è¡Œæ•…éšœåˆ‡æ¢ç³»ç»Ÿ
if __name__ == "__main__":
    asyncio.run(main())
```

### 4.2 æ•°æ®ä¿æŠ¤ä¸æ¢å¤

#### è·¨åŒºåŸŸå¤‡ä»½æ¢å¤ç­–ç•¥
```python
# cross_region_backup_restore.py
import asyncio
import boto3
import time
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
import hashlib
import json

@dataclass
class BackupInfo:
    backup_id: str
    region: str
    timestamp: float
    size_bytes: int
    status: str  # completed, failed, in_progress
    checksum: str

class CrossRegionBackupRestore:
    def __init__(self, regions: List[str], backup_bucket: str):
        self.regions = regions
        self.backup_bucket = backup_bucket
        self.s3_clients = {region: boto3.client('s3', region_name=region) 
                          for region in regions}
        self.backup_metadata = {}  # å¤‡ä»½å…ƒæ•°æ®
        
    async def schedule_regular_backups(self, schedule: str = "0 2 * * *"):
        """å®‰æ’å®šæœŸå¤‡ä»½"""
        print(f"å®‰æ’æ¯æ—¥å¤‡ä»½ä»»åŠ¡: {schedule}")
        # å®é™…å®ç°ä¸­ä½¿ç”¨cronæˆ–ç±»ä¼¼è°ƒåº¦å™¨
        
    async def create_cross_region_backup(self, source_region: str, 
                                       backup_name: Optional[str] = None) -> str:
        """åˆ›å»ºè·¨åŒºåŸŸå¤‡ä»½"""
        if not backup_name:
            backup_name = f"backup_{source_region}_{int(time.time())}"
        
        backup_id = f"{source_region}_{backup_name}"
        
        try:
            print(f"å¼€å§‹åˆ›å»ºå¤‡ä»½: {backup_id}")
            
            # 1. åœ¨æºåŒºåŸŸåˆ›å»ºæœ¬åœ°å¤‡ä»½
            local_backup_path = await self._create_local_backup(source_region, backup_name)
            
            # 2. å¤åˆ¶å¤‡ä»½åˆ°å…¶ä»–åŒºåŸŸ
            copy_tasks = []
            for target_region in self.regions:
                if target_region != source_region:
                    task = asyncio.create_task(
                        self._copy_backup_to_region(local_backup_path, target_region, backup_id)
                    )
                    copy_tasks.append(task)
            
            copy_results = await asyncio.gather(*copy_tasks, return_exceptions=True)
            
            # 3. éªŒè¯å¤‡ä»½å®Œæ•´æ€§
            validation_tasks = []
            for target_region in self.regions:
                task = asyncio.create_task(
                    self._validate_backup_integrity(backup_id, target_region)
                )
                validation_tasks.append(task)
            
            validation_results = await asyncio.gather(*validation_tasks, return_exceptions=True)
            
            # 4. è®°å½•å¤‡ä»½å…ƒæ•°æ®
            backup_info = BackupInfo(
                backup_id=backup_id,
                region=source_region,
                timestamp=time.time(),
                size_bytes=await self._get_backup_size(local_backup_path),
                status='completed' if all(isinstance(r, bool) and r for r in validation_results) else 'failed',
                checksum=await self._calculate_checksum(local_backup_path)
            )
            
            self.backup_metadata[backup_id] = backup_info
            
            print(f"å¤‡ä»½åˆ›å»ºå®Œæˆ: {backup_id}")
            return backup_id
            
        except Exception as e:
            print(f"å¤‡ä»½åˆ›å»ºå¤±è´¥: {str(e)}")
            return ""
    
    async def _create_local_backup(self, region: str, backup_name: str) -> str:
        """åœ¨æŒ‡å®šåŒºåŸŸåˆ›å»ºæœ¬åœ°å¤‡ä»½"""
        # å®é™…å®ç°ä¸­æ‰§è¡Œæ•°æ®åº“å¤‡ä»½å‘½ä»¤
        print(f"åœ¨åŒºåŸŸ {region} åˆ›å»ºæœ¬åœ°å¤‡ä»½: {backup_name}")
        await asyncio.sleep(2)  # æ¨¡æ‹Ÿå¤‡ä»½è¿‡ç¨‹
        return f"/backups/{backup_name}.sql"
    
    async def _copy_backup_to_region(self, backup_path: str, target_region: str, backup_id: str):
        """å°†å¤‡ä»½å¤åˆ¶åˆ°ç›®æ ‡åŒºåŸŸ"""
        try:
            s3_client = self.s3_clients[target_region]
            
            # ä¸Šä¼ åˆ°S3
            with open(backup_path, 'rb') as f:
                s3_client.upload_fileobj(
                    f, 
                    self.backup_bucket, 
                    f"{target_region}/{backup_id}.sql"
                )
            
            print(f"å¤‡ä»½å·²å¤åˆ¶åˆ°åŒºåŸŸ: {target_region}")
            return True
            
        except Exception as e:
            print(f"å¤åˆ¶å¤‡ä»½åˆ° {target_region} å¤±è´¥: {str(e)}")
            return False
    
    async def _validate_backup_integrity(self, backup_id: str, region: str) -> bool:
        """éªŒè¯å¤‡ä»½å®Œæ•´æ€§"""
        try:
            s3_client = self.s3_clients[region]
            
            # ä¸‹è½½å¤‡ä»½å¹¶éªŒè¯æ ¡éªŒå’Œ
            response = s3_client.get_object(
                Bucket=self.backup_bucket,
                Key=f"{region}/{backup_id}.sql"
            )
            
            # è®¡ç®—ä¸‹è½½æ–‡ä»¶çš„æ ¡éªŒå’Œ
            downloaded_checksum = hashlib.md5(response['Body'].read()).hexdigest()
            
            # ä¸åŸå§‹æ ¡éªŒå’Œæ¯”è¾ƒ
            original_checksum = self.backup_metadata[backup_id].checksum
            is_valid = downloaded_checksum == original_checksum
            
            print(f"åŒºåŸŸ {region} å¤‡ä»½å®Œæ•´æ€§éªŒè¯: {'é€šè¿‡' if is_valid else 'å¤±è´¥'}")
            return is_valid
            
        except Exception as e:
            print(f"éªŒè¯åŒºåŸŸ {region} å¤‡ä»½å®Œæ•´æ€§å¤±è´¥: {str(e)}")
            return False
    
    async def _get_backup_size(self, backup_path: str) -> int:
        """è·å–å¤‡ä»½æ–‡ä»¶å¤§å°"""
        # å®é™…å®ç°ä¸­è·å–æ–‡ä»¶å¤§å°
        return 1024 * 1024 * 100  # 100MB ç¤ºä¾‹
    
    async def _calculate_checksum(self, backup_path: str) -> str:
        """è®¡ç®—å¤‡ä»½æ–‡ä»¶æ ¡éªŒå’Œ"""
        # å®é™…å®ç°ä¸­è®¡ç®—æ–‡ä»¶MD5
        return hashlib.md5(b"sample backup content").hexdigest()
    
    async def restore_from_backup(self, backup_id: str, target_region: str) -> bool:
        """ä»å¤‡ä»½æ¢å¤åˆ°æŒ‡å®šåŒºåŸŸ"""
        try:
            print(f"å¼€å§‹ä»å¤‡ä»½ {backup_id} æ¢å¤åˆ°åŒºåŸŸ {target_region}")
            
            # 1. éªŒè¯å¤‡ä»½å­˜åœ¨ä¸”å®Œæ•´
            if backup_id not in self.backup_metadata:
                raise ValueError(f"å¤‡ä»½ä¸å­˜åœ¨: {backup_id}")
            
            backup_info = self.backup_metadata[backup_id]
            if backup_info.status != 'completed':
                raise ValueError(f"å¤‡ä»½çŠ¶æ€ä¸æ­£ç¡®: {backup_info.status}")
            
            # 2. ä¸‹è½½å¤‡ä»½æ–‡ä»¶
            backup_file = await self._download_backup(backup_id, target_region)
            
            # 3. æ‰§è¡Œæ¢å¤æ“ä½œ
            await self._execute_restore(backup_file, target_region)
            
            # 4. éªŒè¯æ¢å¤ç»“æœ
            restore_success = await self._verify_restore(target_region)
            
            if restore_success:
                print(f"æ¢å¤æˆåŠŸå®Œæˆåˆ°åŒºåŸŸ: {target_region}")
                return True
            else:
                print(f"æ¢å¤éªŒè¯å¤±è´¥: {target_region}")
                return False
                
        except Exception as e:
            print(f"æ¢å¤æ“ä½œå¤±è´¥: {str(e)}")
            return False
    
    async def _download_backup(self, backup_id: str, target_region: str) -> str:
        """ä¸‹è½½å¤‡ä»½æ–‡ä»¶"""
        s3_client = self.s3_clients[target_region]
        
        # ç¡®å®šæºåŒºåŸŸï¼ˆä¼˜å…ˆä»åŒåŒºåŸŸä¸‹è½½ï¼‰
        source_region = self.backup_metadata[backup_id].region
        if source_region != target_region:
            # æ£€æŸ¥ç›®æ ‡åŒºåŸŸæ˜¯å¦æœ‰å¤‡ä»½å‰¯æœ¬
            try:
                s3_client.head_object(
                    Bucket=self.backup_bucket,
                    Key=f"{target_region}/{backup_id}.sql"
                )
                download_region = target_region
            except:
                download_region = source_region
        else:
            download_region = source_region
        
        # ä¸‹è½½å¤‡ä»½æ–‡ä»¶
        local_path = f"/tmp/{backup_id}.sql"
        s3_client.download_file(
            self.backup_bucket,
            f"{download_region}/{backup_id}.sql",
            local_path
        )
        
        print(f"å¤‡ä»½æ–‡ä»¶å·²ä¸‹è½½åˆ°: {local_path}")
        return local_path
    
    async def _execute_restore(self, backup_file: str, target_region: str):
        """æ‰§è¡Œæ¢å¤æ“ä½œ"""
        print(f"åœ¨åŒºåŸŸ {target_region} æ‰§è¡Œæ¢å¤æ“ä½œ")
        await asyncio.sleep(3)  # æ¨¡æ‹Ÿæ¢å¤è¿‡ç¨‹
    
    async def _verify_restore(self, target_region: str) -> bool:
        """éªŒè¯æ¢å¤ç»“æœ"""
        print(f"éªŒè¯åŒºåŸŸ {target_region} çš„æ¢å¤ç»“æœ")
        await asyncio.sleep(1)  # æ¨¡æ‹ŸéªŒè¯è¿‡ç¨‹
        return True  # ç®€åŒ–å¤„ç†
    
    def list_available_backups(self) -> List[Dict[str, Any]]:
        """åˆ—å‡ºå¯ç”¨çš„å¤‡ä»½"""
        backups = []
        for backup_id, info in self.backup_metadata.items():
            backups.append({
                'backup_id': backup_id,
                'source_region': info.region,
                'timestamp': info.timestamp,
                'size_bytes': info.size_bytes,
                'status': info.status,
                'available_regions': self._get_backup_locations(backup_id)
            })
        return backups
    
    def _get_backup_locations(self, backup_id: str) -> List[str]:
        """è·å–å¤‡ä»½å­˜åœ¨çš„åŒºåŸŸ"""
        locations = []
        for region in self.regions:
            try:
                s3_client = self.s3_clients[region]
                s3_client.head_object(
                    Bucket=self.backup_bucket,
                    Key=f"{region}/{backup_id}.sql"
                )
                locations.append(region)
            except:
                pass
        return locations
    
    async def cleanup_old_backups(self, retention_days: int = 30):
        """æ¸…ç†æ—§å¤‡ä»½"""
        cutoff_time = time.time() - (retention_days * 24 * 3600)
        deleted_count = 0
        
        for backup_id, info in list(self.backup_metadata.items()):
            if info.timestamp < cutoff_time:
                # åˆ é™¤S3ä¸­çš„å¤‡ä»½æ–‡ä»¶
                for region in self.regions:
                    try:
                        s3_client = self.s3_clients[region]
                        s3_client.delete_object(
                            Bucket=self.backup_bucket,
                            Key=f"{region}/{backup_id}.sql"
                        )
                    except:
                        pass
                
                # ä»å…ƒæ•°æ®ä¸­åˆ é™¤
                del self.backup_metadata[backup_id]
                deleted_count += 1
        
        print(f"å·²æ¸…ç† {deleted_count} ä¸ªæ—§å¤‡ä»½")

# ä½¿ç”¨ç¤ºä¾‹
async def main():
    regions = ['us-east-1', 'us-west-2', 'eu-west-1']
    backup_system = CrossRegionBackupRestore(regions, 'global-backups-bucket')
    
    # åˆ›å»ºè·¨åŒºåŸŸå¤‡ä»½
    backup_id = await backup_system.create_cross_region_backup('us-east-1')
    print(f"åˆ›å»ºçš„å¤‡ä»½ID: {backup_id}")
    
    # åˆ—å‡ºå¯ç”¨å¤‡ä»½
    backups = backup_system.list_available_backups()
    print(f"å¯ç”¨å¤‡ä»½æ•°é‡: {len(backups)}")
    
    # æ¨¡æ‹Ÿä»å¤‡ä»½æ¢å¤
    if backup_id:
        success = await backup_system.restore_from_backup(backup_id, 'us-west-2')
        print(f"æ¢å¤æ“ä½œç»“æœ: {'æˆåŠŸ' if success else 'å¤±è´¥'}")
    
    # æ¸…ç†æ—§å¤‡ä»½
    await backup_system.cleanup_old_backups(retention_days=7)

# è¿è¡Œå¤‡ä»½æ¢å¤ç³»ç»Ÿ
if __name__ == "__main__":
    asyncio.run(main())
```

## 5. æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

### 5.1 å»¶è¿Ÿä¼˜åŒ–æŠ€æœ¯

#### æ™ºèƒ½è·¯ç”±ä¸ç¼“å­˜ç­–ç•¥
```python
# latency_optimization_router.py
import asyncio
import time
import json
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
import heapq

@dataclass
class RegionLatency:
    region: str
    latency_ms: float
    last_measured: float
    reliability: float  # 0-1

class LatencyOptimizationRouter:
    def __init__(self, regions: List[str]):
        self.regions = regions
        self.latency_measurements = {region: [] for region in regions}
        self.region_latencies = {region: RegionLatency(region, float('inf'), 0, 1.0) 
                               for region in regions}
        self.cache = {}  # ç®€å•çš„å†…å­˜ç¼“å­˜
        self.cache_ttl = 300  # 5åˆ†é’Ÿç¼“å­˜æ—¶é—´
        
    async def start_continuous_latency_monitoring(self):
        """å¯åŠ¨æŒç»­å»¶è¿Ÿç›‘æ§"""
        while True:
            try:
                await self._measure_all_region_latencies()
                await self._update_routing_decisions()
                await asyncio.sleep(60)  # æ¯åˆ†é’Ÿæµ‹é‡ä¸€æ¬¡
            except Exception as e:
                print(f"å»¶è¿Ÿç›‘æ§å‡ºé”™: {str(e)}")
                await asyncio.sleep(120)
    
    async def _measure_all_region_latencies(self):
        """æµ‹é‡æ‰€æœ‰åŒºåŸŸçš„å»¶è¿Ÿ"""
        measurement_tasks = []
        for region in self.regions:
            task = asyncio.create_task(self._measure_region_latency(region))
            measurement_tasks.append(task)
        
        results = await asyncio.gather(*measurement_tasks, return_exceptions=True)
        
        current_time = time.time()
        for i, result in enumerate(results):
            region = self.regions[i]
            if isinstance(result, float):
                # æ›´æ–°å»¶è¿Ÿæµ‹é‡ç»“æœ
                self.latency_measurements[region].append((result, current_time))
                
                # ä¿ç•™æœ€è¿‘10æ¬¡æµ‹é‡ç»“æœ
                if len(self.latency_measurements[region]) > 10:
                    self.latency_measurements[region] = self.latency_measurements[region][-10:]
                
                # æ›´æ–°åŒºåŸŸå»¶è¿Ÿä¿¡æ¯
                self.region_latencies[region] = RegionLatency(
                    region=region,
                    latency_ms=result,
                    last_measured=current_time,
                    reliability=self._calculate_reliability(region)
                )
            else:
                # æµ‹é‡å¤±è´¥ï¼Œé™ä½å¯é æ€§
                current_latency = self.region_latencies[region]
                self.region_latencies[region] = RegionLatency(
                    region=region,
                    latency_ms=current_latency.latency_ms,
                    last_measured=current_latency.last_measured,
                    reliability=max(0.1, current_latency.reliability * 0.9)
                )
    
    async def _measure_region_latency(self, region: str) -> float:
        """æµ‹é‡å•ä¸ªåŒºåŸŸçš„å»¶è¿Ÿ"""
        start_time = time.time()
        
        try:
            # æ¨¡æ‹Ÿç½‘ç»œè¯·æ±‚åˆ°è¯¥åŒºåŸŸ
            await asyncio.sleep(0.01 + hash(region) % 100 / 1000)  # 10-110msæ¨¡æ‹Ÿå»¶è¿Ÿ
            
            # å®é™…å®ç°ä¸­å‘é€å¿ƒè·³åŒ…æˆ–ç®€å•HTTPè¯·æ±‚
            latency = (time.time() - start_time) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
            return round(latency, 2)
            
        except Exception:
            # è¿”å›è¾ƒå¤§çš„å»¶è¿Ÿå€¼è¡¨ç¤ºè¿æ¥é—®é¢˜
            return 9999.0
    
    def _calculate_reliability(self, region: str) -> float:
        """è®¡ç®—åŒºåŸŸå¯é æ€§"""
        measurements = self.latency_measurements[region]
        if not measurements:
            return 1.0
        
        # è®¡ç®—æœ€è¿‘å‡ æ¬¡æµ‹é‡çš„æˆåŠŸç‡å’Œå»¶è¿Ÿç¨³å®šæ€§
        recent_measurements = measurements[-5:] if len(measurements) > 5 else measurements
        
        success_count = sum(1 for latency, _ in recent_measurements if latency < 5000)
        success_rate = success_count / len(recent_measurements)
        
        # è®¡ç®—å»¶è¿Ÿå˜åŒ–ç³»æ•°
        if len(recent_measurements) > 1:
            latencies = [latency for latency, _ in recent_measurements]
            mean_latency = sum(latencies) / len(latencies)
            variance = sum((l - mean_latency) ** 2 for l in latencies) / len(latencies)
            std_deviation = variance ** 0.5
            stability_factor = 1 / (1 + std_deviation / (mean_latency + 1))
        else:
            stability_factor = 1.0
        
        # ç»¼åˆå¯é æ€§è¯„åˆ†
        reliability = success_rate * 0.7 + stability_factor * 0.3
        return max(0.1, min(1.0, reliability))
    
    async def _update_routing_decisions(self):
        """æ›´æ–°è·¯ç”±å†³ç­–"""
        # æ ¹æ®å»¶è¿Ÿå’Œå¯é æ€§è®¡ç®—ç»¼åˆå¾—åˆ†
        routing_scores = {}
        for region, latency_info in self.region_latencies.items():
            # ç»¼åˆå¾—åˆ† = (1/å»¶è¿Ÿ) * å¯é æ€§
            if latency_info.latency_ms < 5000:  # æ’é™¤ä¸¥é‡å»¶è¿Ÿçš„åŒºåŸŸ
                score = (1.0 / (latency_info.latency_ms + 1)) * latency_info.reliability
                routing_scores[region] = score
        
        # æŒ‰å¾—åˆ†æ’åº
        self.sorted_regions = sorted(routing_scores.items(), key=lambda x: x[1], reverse=True)
        print(f"è·¯ç”±ä¼˜å…ˆçº§æ›´æ–°: {[region for region, _ in self.sorted_regions]}")
    
    def get_optimal_region(self, user_location: Optional[str] = None) -> str:
        """è·å–æœ€ä¼˜åŒºåŸŸ"""
        if not self.sorted_regions:
            return self.regions[0]  # é»˜è®¤è¿”å›ç¬¬ä¸€ä¸ªåŒºåŸŸ
        
        # å¦‚æœæœ‰ç”¨æˆ·ä½ç½®ä¿¡æ¯ï¼Œä¼˜å…ˆé€‰æ‹©åœ°ç†ä½ç½®ç›¸è¿‘çš„åŒºåŸŸ
        if user_location:
            nearby_region = self._get_nearby_region(user_location)
            if nearby_region and nearby_region in [region for region, _ in self.sorted_regions]:
                return nearby_region
        
        # è¿”å›å¾—åˆ†æœ€é«˜çš„åŒºåŸŸ
        return self.sorted_regions[0][0]
    
    def _get_nearby_region(self, user_location: str) -> Optional[str]:
        """æ ¹æ®ç”¨æˆ·ä½ç½®è·å–é™„è¿‘åŒºåŸŸ"""
        # ç®€åŒ–çš„åœ°ç†ä½ç½®æ˜ å°„
        location_mapping = {
            'north_america': ['us-east-1', 'us-west-2'],
            'europe': ['eu-west-1', 'eu-central-1'],
            'asia': ['ap-northeast-1', 'ap-southeast-1'],
            'australia': ['ap-southeast-2']
        }
        
        for region, locations in location_mapping.items():
            if region in user_location.lower():
                return locations[0]  # è¿”å›è¯¥åŒºåŸŸçš„ç¬¬ä¸€ä¸ªåŒºåŸŸ
        
        return None
    
    async def route_request(self, request_data: Dict[str, Any], 
                          user_location: Optional[str] = None) -> Dict[str, Any]:
        """è·¯ç”±è¯·æ±‚åˆ°æœ€ä¼˜åŒºåŸŸ"""
        cache_key = self._generate_cache_key(request_data)
        
        # æ£€æŸ¥ç¼“å­˜
        if cache_key in self.cache:
            cached_entry = self.cache[cache_key]
            if time.time() - cached_entry['timestamp'] < self.cache_ttl:
                return cached_entry['response']
        
        # è·å–æœ€ä¼˜åŒºåŸŸ
        optimal_region = self.get_optimal_region(user_location)
        
        # å‘é€è¯·æ±‚åˆ°æœ€ä¼˜åŒºåŸŸ
        response = await self._send_request_to_region(request_data, optimal_region)
        
        # ç¼“å­˜ç»“æœ
        self.cache[cache_key] = {
            'response': response,
            'timestamp': time.time(),
            'region': optimal_region
        }
        
        return response
    
    def _generate_cache_key(self, request_data: Dict[str, Any]) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        # åŸºäºè¯·æ±‚æ•°æ®ç”Ÿæˆå”¯ä¸€é”®
        request_str = json.dumps(request_data, sort_keys=True)
        return hashlib.md5(request_str.encode()).hexdigest()
    
    async def _send_request_to_region(self, request_data: Dict[str, Any], region: str) -> Dict[str, Any]:
        """å‘é€è¯·æ±‚åˆ°æŒ‡å®šåŒºåŸŸ"""
        print(f"å‘é€è¯·æ±‚åˆ°åŒºåŸŸ: {region}")
        await asyncio.sleep(0.05)  # æ¨¡æ‹Ÿç½‘ç»œå»¶è¿Ÿ
        
        # æ¨¡æ‹Ÿå“åº”
        return {
            'region': region,
            'data': request_data.get('query', 'sample_data'),
            'processing_time_ms': round(self.region_latencies[region].latency_ms * 0.8, 2),
            'timestamp': time.time()
        }
    
    def get_routing_statistics(self) -> Dict[str, Any]:
        """è·å–è·¯ç”±ç»Ÿè®¡ä¿¡æ¯"""
        total_requests = sum(len(measurements) for measurements in self.latency_measurements.values())
        
        return {
            'total_measurements': total_requests,
            'region_performance': {
                region: {
                    'current_latency': info.latency_ms,
                    'reliability': info.reliability,
                    'measurement_count': len(self.latency_measurements[region])
                }
                for region, info in self.region_latencies.items()
            },
            'routing_priority': [region for region, _ in self.sorted_regions],
            'cache_size': len(self.cache),
            'cache_hits': self._get_cache_hit_rate()
        }
    
    def _get_cache_hit_rate(self) -> float:
        """è·å–ç¼“å­˜å‘½ä¸­ç‡"""
        # ç®€åŒ–å®ç°ï¼Œå®é™…éœ€è¦è·Ÿè¸ªç¼“å­˜è®¿é—®ç»Ÿè®¡
        return 0.85  # å‡è®¾85%çš„å‘½ä¸­ç‡

# ä½¿ç”¨ç¤ºä¾‹
async def main():
    regions = ['us-east-1', 'us-west-2', 'eu-west-1', 'ap-northeast-1']
    router = LatencyOptimizationRouter(regions)
    
    # å¯åŠ¨å»¶è¿Ÿç›‘æ§
    monitor_task = asyncio.create_task(router.start_continuous_latency_monitoring())
    
    # æ¨¡æ‹Ÿå¤„ç†è¯·æ±‚
    for i in range(10):
        request = {'query': f'data_request_{i}', 'type': 'read'}
        user_location = ['north_america', 'europe', 'asia'][i % 3]
        
        response = await router.route_request(request, user_location)
        print(f"è¯·æ±‚ {i+1}: è·¯ç”±åˆ° {response['region']}, å¤„ç†æ—¶é—´: {response['processing_time_ms']}ms")
        
        if i % 3 == 0:  # æ¯3æ¬¡è¯·æ±‚æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            stats = router.get_routing_statistics()
            print(f"è·¯ç”±ç»Ÿè®¡: {stats['routing_priority']}")
        
        await asyncio.sleep(10)  # æ¯10ç§’å¤„ç†ä¸€æ¬¡è¯·æ±‚
    
    # å–æ¶ˆç›‘æ§ä»»åŠ¡
    monitor_task.cancel()

# è¿è¡Œæ™ºèƒ½è·¯ç”±ç³»ç»Ÿ
if __name__ == "__main__":
    asyncio.run(main())
```

### 5.2 è´Ÿè½½å‡è¡¡ä¼˜åŒ–

#### è‡ªé€‚åº”è´Ÿè½½å‡è¡¡å™¨
```python
# adaptive_load_balancer.py
import asyncio
import time
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
import random

@dataclass
class ServerMetrics:
    server_id: str
    current_load: float  # 0-1
    response_time_ms: float
    error_rate: float
    health_status: str  # healthy, degraded, failed
    last_update: float

class AdaptiveLoadBalancer:
    def __init__(self, servers: List[str]):
        self.servers = servers
        self.server_metrics = {server: ServerMetrics(server, 0, 0, 0, 'unknown', 0) 
                             for server in servers}
        self.request_counts = {server: 0 for server in servers}
        self.algorithm = 'weighted_round_robin'  # é»˜è®¤ç®—æ³•
        self.weights = {server: 1.0 for server in servers}
        self.last_rebalancing = time.time()
        
    async def start_health_monitoring(self):
        """å¯åŠ¨å¥åº·ç›‘æ§"""
        while True:
            try:
                await self._update_server_metrics()
                await self._adjust_weights()
                await asyncio.sleep(30)  # æ¯30ç§’æ›´æ–°ä¸€æ¬¡
            except Exception as e:
                print(f"å¥åº·ç›‘æ§å‡ºé”™: {str(e)}")
                await asyncio.sleep(60)
    
    async def _update_server_metrics(self):
        """æ›´æ–°æœåŠ¡å™¨æŒ‡æ ‡"""
        for server in self.servers:
            metrics = await self._collect_server_metrics(server)
            self.server_metrics[server] = metrics
    
    async def _collect_server_metrics(self, server: str) -> ServerMetrics:
        """æ”¶é›†å•ä¸ªæœåŠ¡å™¨æŒ‡æ ‡"""
        # æ¨¡æ‹ŸæŒ‡æ ‡æ”¶é›†
        await asyncio.sleep(0.1)
        
        # ç”Ÿæˆæ¨¡æ‹ŸæŒ‡æ ‡
        base_load = 0.3 + random.random() * 0.5  # 30-80% è´Ÿè½½
        response_time = 50 + random.random() * 150  # 50-200ms å“åº”æ—¶é—´
        error_rate = random.random() * 0.05  # 0-5% é”™è¯¯ç‡
        
        health_status = 'healthy'
        if base_load > 0.8:
            health_status = 'degraded'
        elif base_load > 0.95:
            health_status = 'failed'
        
        return ServerMetrics(
            server_id=server,
            current_load=base_load,
            response_time_ms=response_time,
            error_rate=error_rate,
            health_status=health_status,
            last_update=time.time()
        )
    
    async def _adjust_weights(self):
        """è°ƒæ•´æœåŠ¡å™¨æƒé‡"""
        current_time = time.time()
        
        # æ¯5åˆ†é’Ÿé‡æ–°å¹³è¡¡ä¸€æ¬¡æƒé‡
        if current_time - self.last_rebalancing > 300:
            self._rebalance_weights()
            self.last_rebalancing = current_time
            return
        
        # æ ¹æ®å®æ—¶æŒ‡æ ‡åŠ¨æ€è°ƒæ•´æƒé‡
        for server, metrics in self.server_metrics.items():
            if metrics.health_status == 'failed':
                self.weights[server] = 0
            elif metrics.health_status == 'degraded':
                self.weights[server] *= 0.5  # é™æƒ50%
            else:
                # åŸºäºè´Ÿè½½å’Œå“åº”æ—¶é—´è°ƒæ•´æƒé‡
                load_factor = 1 - metrics.current_load
                response_factor = max(0.1, 1 - (metrics.response_time_ms / 500))
                error_factor = 1 - metrics.error_rate
                
                self.weights[server] = load_factor * response_factor * error_factor
    
    def _rebalance_weights(self):
        """é‡æ–°å¹³è¡¡æƒé‡"""
        healthy_servers = [s for s, m in self.server_metrics.items() 
                          if m.health_status != 'failed']
        
        if healthy_servers:
            # å¹³å‡åˆ†é…æƒé‡
            equal_weight = 1.0 / len(healthy_servers)
            for server in self.servers:
                if server in healthy_servers:
                    self.weights[server] = equal_weight
                else:
                    self.weights[server] = 0
    
    def select_server(self, request_context: Optional[Dict[str, Any]] = None) -> str:
        """é€‰æ‹©æœåŠ¡å™¨"""
        healthy_servers = [s for s in self.servers 
                          if self.server_metrics[s].health_status != 'failed' 
                          and self.weights[s] > 0]
        
        if not healthy_servers:
            # æ²¡æœ‰å¥åº·æœåŠ¡å™¨ï¼Œè¿”å›ç¬¬ä¸€ä¸ªæœåŠ¡å™¨
            return self.servers[0]
        
        if self.algorithm == 'weighted_round_robin':
            return self._weighted_round_robin(healthy_servers)
        elif self.algorithm == 'least_connections':
            return self._least_connections(healthy_servers)
        elif self.algorithm == 'random':
            return self._weighted_random(healthy_servers)
        else:
            return healthy_servers[0]
    
    def _weighted_round_robin(self, servers: List[str]) -> str:
        """åŠ æƒè½®è¯¢ç®—æ³•"""
        # ç®€åŒ–å®ç°ï¼šåŸºäºæƒé‡é€‰æ‹©
        total_weight = sum(self.weights[s] for s in servers)
        if total_weight == 0:
            return servers[0]
        
        random_point = random.random() * total_weight
        current_weight = 0
        
        for server in servers:
            current_weight += self.weights[server]
            if current_weight >= random_point:
                self.request_counts[server] += 1
                return server
        
        return servers[0]
    
    def _least_connections(self, servers: List[str]) -> str:
        """æœ€å°‘è¿æ¥ç®—æ³•"""
        return min(servers, key=lambda s: self.request_counts[s])
    
    def _weighted_random(self, servers: List[str]) -> str:
        """åŠ æƒéšæœºç®—æ³•"""
        return random.choices(servers, weights=[self.weights[s] for s in servers])[0]
    
    async def handle_request(self, request_data: Dict[str, Any]) -> Dict[str, Any]:
        """å¤„ç†è¯·æ±‚"""
        # é€‰æ‹©æœ€ä¼˜æœåŠ¡å™¨
        selected_server = self.select_server(request_data)
        
        # å‘é€è¯·æ±‚åˆ°é€‰å®šæœåŠ¡å™¨
        response = await self._send_request_to_server(request_data, selected_server)
        
        # æ›´æ–°è¯·æ±‚è®¡æ•°
        self.request_counts[selected_server] += 1
        
        return response
    
    async def _send_request_to_server(self, request_data: Dict[str, Any], server: str) -> Dict[str, Any]:
        """å‘é€è¯·æ±‚åˆ°æœåŠ¡å™¨"""
        print(f"è¯·æ±‚è·¯ç”±åˆ°æœåŠ¡å™¨: {server}")
        await asyncio.sleep(0.05 + self.server_metrics[server].response_time_ms / 1000)
        
        # æ¨¡æ‹ŸæœåŠ¡å™¨å“åº”
        return {
            'server': server,
            'response': f"å¤„ç†å®Œæˆ: {request_data.get('data', 'default')}",
            'processing_time_ms': self.server_metrics[server].response_time_ms,
            'timestamp': time.time()
        }
    
    def get_load_balancer_stats(self) -> Dict[str, Any]:
        """è·å–è´Ÿè½½å‡è¡¡å™¨ç»Ÿè®¡ä¿¡æ¯"""
        total_requests = sum(self.request_counts.values())
        
        return {
            'algorithm': self.algorithm,
            'server_stats': {
                server: {
                    'weight': self.weights[server],
                    'request_count': self.request_counts[server],
                    'metrics': self.server_metrics[server].__dict__,
                    'load_percentage': (self.request_counts[server] / total_requests * 100) 
                                     if total_requests > 0 else 0
                }
                for server in self.servers
            },
            'total_requests': total_requests,
            'healthy_servers': len([s for s in self.servers 
                                  if self.server_metrics[s].health_status == 'healthy'])
        }
    
    def change_algorithm(self, new_algorithm: str):
        """æ›´æ”¹è´Ÿè½½å‡è¡¡ç®—æ³•"""
        valid_algorithms = ['weighted_round_robin', 'least_connections', 'random']
        if new_algorithm in valid_algorithms:
            self.algorithm = new_algorithm
            print(f"è´Ÿè½½å‡è¡¡ç®—æ³•å·²æ›´æ”¹ä¸º: {new_algorithm}")
        else:
            print(f"æ— æ•ˆçš„ç®—æ³•: {new_algorithm}")

# ä½¿ç”¨ç¤ºä¾‹
async def main():
    servers = ['server-1', 'server-2', 'server-3', 'server-4']
    lb = AdaptiveLoadBalancer(servers)
    
    # å¯åŠ¨å¥åº·ç›‘æ§
    monitor_task = asyncio.create_task(lb.start_health_monitoring())
    
    # æ¨¡æ‹Ÿå¤„ç†è¯·æ±‚
    for i in range(20):
        request = {'data': f'request_{i}', 'priority': 'normal'}
        response = await lb.handle_request(request)
        print(f"è¯·æ±‚ {i+1}: {response['server']} - {response['processing_time_ms']:.2f}ms")
        
        if i % 5 == 0:  # æ¯5æ¬¡è¯·æ±‚æ˜¾ç¤ºç»Ÿè®¡
            stats = lb.get_load_balancer_stats()
            print(f"è´Ÿè½½åˆ†å¸ƒ: {[f'{s}:{w:.2f}' for s, w in lb.weights.items() if w > 0]}")
        
        await asyncio.sleep(1)
    
    # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
    final_stats = lb.get_load_balancer_stats()
    print("\næœ€ç»ˆè´Ÿè½½å‡è¡¡ç»Ÿè®¡:")
    for server, stats in final_stats['server_stats'].items():
        print(f"{server}: æƒé‡={stats['weight']:.2f}, "
              f"è¯·æ±‚æ•°={stats['request_count']}, "
              f"è´Ÿè½½={stats['load_percentage']:.1f}%")
    
    # å–æ¶ˆç›‘æ§ä»»åŠ¡
    monitor_task.cancel()

# è¿è¡Œè‡ªé€‚åº”è´Ÿè½½å‡è¡¡å™¨
if __name__ == "__main__":
    asyncio.run(main())
```

## 6. ç›‘æ§ä¸è¿ç»´ç®¡ç†

### 6.1 å…¨å±€ç›‘æ§ä»ªè¡¨æ¿

#### å¤šåŒºåŸŸç›‘æ§ç³»ç»Ÿ
```json
{
  "dashboard": {
    "id": null,
    "title": "å¤šåŒºåŸŸæ•°æ®åº“ç›‘æ§æ€»è§ˆ",
    "timezone": "browser",
    "schemaVersion": 16,
    "version": 0,
    "refresh": "30s",
    "panels": [
      {
        "id": 1,
        "type": "graph",
        "title": "è·¨åŒºåŸŸå»¶è¿Ÿç›‘æ§",
        "gridPos": {
          "h": 8,
          "w": 12,
          "x": 0,
          "y": 0
        },
        "targets": [
          {
            "expr": "region_to_region_latency_seconds{source_region=\"us-east-1\", target_region=\"us-west-2\"}",
            "legendFormat": "US East to US West",
            "refId": "A"
          },
          {
            "expr": "region_to_region_latency_seconds{source_region=\"us-east-1\", target_region=\"eu-west-1\"}",
            "legendFormat": "US East to EU West",
            "refId": "B"
          },
          {
            "expr": "region_to_region_latency_seconds{source_region=\"us-east-1\", target_region=\"ap-northeast-1\"}",
            "legendFormat": "US East to Asia Pacific",
            "refId": "C"
          }
        ],
        "alert": {
          "conditions": [
            {
              "evaluator": {
                "params": [0.5],
                "type": "gt"
              },
              "operator": {
                "type": "and"
              },
              "query": {
                "params": ["A", "5m", "now"]
              },
              "reducer": {
                "params": [],
                "type": "avg"
              },
              "type": "query"
            }
          ],
          "executionErrorState": "alerting",
          "frequency": "1m",
          "handler": 1,
          "name": "è·¨åŒºåŸŸé«˜å»¶è¿Ÿå‘Šè­¦",
          "noDataState": "no_data",
          "notifications": []
        }
      },
      {
        "id": 2,
        "type": "stat",
        "title": "å„åŒºåŸŸå¥åº·çŠ¶æ€",
        "gridPos": {
          "h": 8,
          "w": 12,
          "x": 12,
          "y": 0
        },
        "targets": [
          {
            "expr": "region_health_status{region=\"us-east-1\"}",
            "legendFormat": "US East",
            "refId": "A"
          },
          {
            "expr": "region_health_status{region=\"us-west-2\"}",
            "legendFormat": "US West",
            "refId": "B"
          },
          {
            "expr": "region_health_status{region=\"eu-west-1\"}",
            "legendFormat": "EU West",
            "refId": "C"
          },
          {
            "expr": "region_health_status{region=\"ap-northeast-1\"}",
            "legendFormat": "Asia Pacific",
            "refId": "D"
          }
        ]
      },
      {
        "id": 3,
        "type": "graph",
        "title": "æ•°æ®åŒæ­¥å»¶è¿Ÿ",
        "gridPos": {
          "h": 8,
          "w": 12,
          "x": 0,
          "y": 8
        },
        "targets": [
          {
            "expr": "data_replication_lag_seconds{source_region=\"us-east-1\"}",
            "legendFormat": "å¤åˆ¶å»¶è¿Ÿ (seconds)",
            "refId": "A"
          }
        ]
      },
      {
        "id": 4,
        "type": "table",
        "title": "æ•…éšœåˆ‡æ¢å†å²",
        "gridPos": {
          "h": 8,
          "w": 12,
          "x": 12,
          "y": 8
        },
        "targets": [
          {
            "expr": "failover_events",
            "format": "table",
            "instant": true,
            "refId": "A"
          }
        ]
      }
    ]
  }
}
```

### 6.2 è‡ªåŠ¨åŒ–è¿ç»´è„šæœ¬

#### å¤šåŒºåŸŸè¿ç»´ç®¡ç†å·¥å…·
```bash
#!/bin/bash
# multi_region_ops.sh - å¤šåŒºåŸŸæ•°æ®åº“è¿ç»´ç®¡ç†å·¥å…·

# é¢œè‰²å®šä¹‰
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# å…¨å±€å˜é‡
REGIONS=("us-east-1" "us-west-2" "eu-west-1" "ap-northeast-1")
PRIMARY_REGION="us-east-1"
BACKUP_BUCKET="multi-region-backups"

# æ—¥å¿—å‡½æ•°
log_info() {
    echo -e "${BLUE}[INFO]$(date '+%Y-%m-%d %H:%M:%S') $1${NC}"
}

log_warn() {
    echo -e "${YELLOW}[WARN]$(date '+%Y-%m-%d %H:%M:%S') $1${NC}"
}

log_error() {
    echo -e "${RED}[ERROR]$(date '+%Y-%m-%d %H:%M:%S') $1${NC}"
}

log_success() {
    echo -e "${GREEN}[SUCCESS]$(date '+%Y-%m-%d %H:%M:%S') $1${NC}"
}

# å¥åº·æ£€æŸ¥å‡½æ•°
check_region_health() {
    local region=$1
    log_info "æ£€æŸ¥åŒºåŸŸ $region å¥åº·çŠ¶å†µ"
    
    # æ£€æŸ¥æ•°æ®åº“è¿æ¥
    if ! aws rds describe-db-instances --region $region >/dev/null 2>&1; then
        log_error "æ— æ³•è¿æ¥åˆ°åŒºåŸŸ $region çš„RDSæœåŠ¡"
        return 1
    fi
    
    # æ£€æŸ¥å®ä¾‹çŠ¶æ€
    local instances=$(aws rds describe-db-instances --region $region --query 'DBInstances[*].DBInstanceStatus' --output text)
    if [[ $instances != *"available"* ]]; then
        log_error "åŒºåŸŸ $region æœ‰æ•°æ®åº“å®ä¾‹ä¸å¯ç”¨"
        return 1
    fi
    
    # æ£€æŸ¥ç½‘ç»œè¿æ¥
    local ping_result=$(ping -c 3 $(aws rds describe-db-instances --region $region --query 'DBInstances[0].Endpoint.Address' --output text) 2>/dev/null)
    if [[ $? -ne 0 ]]; then
        log_warn "åŒºåŸŸ $region ç½‘ç»œè¿æ¥å¯èƒ½å­˜åœ¨é—®é¢˜"
    fi
    
    log_success "åŒºåŸŸ $region å¥åº·æ£€æŸ¥é€šè¿‡"
    return 0
}

# å¤‡ä»½ç®¡ç†å‡½æ•°
create_cross_region_backup() {
    local source_region=$1
    local backup_name="backup_$(date +%Y%m%d_%H%M%S)"
    
    log_info "åˆ›å»ºè·¨åŒºåŸŸå¤‡ä»½: $backup_name ä» $source_region"
    
    # åœ¨æºåŒºåŸŸåˆ›å»ºå¤‡ä»½
    local db_instance=$(aws rds describe-db-instances --region $source_region --query 'DBInstances[0].DBInstanceIdentifier' --output text)
    local backup_id=$(aws rds create-db-snapshot \
        --region $source_region \
        --db-instance-identifier $db_instance \
        --db-snapshot-identifier $backup_name \
        --query 'DBSnapshot.DBSnapshotIdentifier' \
        --output text)
    
    if [[ -z "$backup_id" ]]; then
        log_error "åœ¨ $source_region åˆ›å»ºå¤‡ä»½å¤±è´¥"
        return 1
    fi
    
    log_info "ç­‰å¾…å¤‡ä»½å®Œæˆ..."
    aws rds wait db-snapshot-completed --region $source_region --db-snapshot-identifier $backup_id
    
    # å¤åˆ¶å¤‡ä»½åˆ°å…¶ä»–åŒºåŸŸ
    for target_region in "${REGIONS[@]}"; do
        if [[ "$target_region" != "$source_region" ]]; then
            log_info "å¤åˆ¶å¤‡ä»½åˆ° $target_region"
            aws rds copy-db-snapshot \
                --region $target_region \
                --source-db-snapshot-identifier "arn:aws:rds:$source_region:$(aws sts get-caller-identity --query Account --output text):snapshot:$backup_id" \
                --target-db-snapshot-identifier "${backup_name}_${target_region}" \
                --copy-tags \
                --output text >/dev/null &
        fi
    done
    
    # ç­‰å¾…æ‰€æœ‰å¤åˆ¶å®Œæˆ
    wait
    
    log_success "è·¨åŒºåŸŸå¤‡ä»½åˆ›å»ºå®Œæˆ: $backup_name"
    echo $backup_name
}

# æ•…éšœåˆ‡æ¢å‡½æ•°
initiate_failover() {
    local failed_region=$1
    local new_primary=$2
    
    log_warn "å¼€å§‹æ•…éšœåˆ‡æ¢: $failed_region -> $new_primary"
    
    # éªŒè¯æ–°ä¸»åŒºåŸŸå¥åº·çŠ¶å†µ
    if ! check_region_health $new_primary; then
        log_error "æ–°ä¸»åŒºåŸŸ $new_primary ä¸å¥åº·ï¼Œæ— æ³•è¿›è¡Œæ•…éšœåˆ‡æ¢"
        return 1
    fi
    
    # åœæ­¢å†™å…¥åˆ°æ•…éšœåŒºåŸŸ
    log_info "åœæ­¢å‘æ•…éšœåŒºåŸŸ $failed_region çš„å†™å…¥"
    # è¿™é‡Œåº”è¯¥å®ç°å…·ä½“çš„å†™å…¥åœæ­¢é€»è¾‘
    
    # æå‡æ–°ä¸»åŒºåŸŸ
    log_info "æå‡ $new_primary ä¸ºæ–°ä¸»åŒºåŸŸ"
    # è¿™é‡Œåº”è¯¥å®ç°å…·ä½“çš„æå‡é€»è¾‘
    
    # æ›´æ–°DNSè®°å½•
    log_info "æ›´æ–°DNSæŒ‡å‘æ–°ä¸»åŒºåŸŸ"
    # è¿™é‡Œåº”è¯¥å®ç°DNSæ›´æ–°é€»è¾‘
    
    # é‡æ–°é…ç½®åº”ç”¨è¿æ¥
    log_info "é‡æ–°é…ç½®åº”ç”¨è¿æ¥åˆ°æ–°ä¸»åŒºåŸŸ"
    # è¿™é‡Œåº”è¯¥å®ç°åº”ç”¨é‡é…ç½®é€»è¾‘
    
    log_success "æ•…éšœåˆ‡æ¢å®Œæˆ: $new_primary ç°åœ¨æ˜¯ä¸»åŒºåŸŸ"
    PRIMARY_REGION=$new_primary
}

# ç›‘æ§å‡½æ•°
monitor_all_regions() {
    log_info "å¼€å§‹ç›‘æ§æ‰€æœ‰åŒºåŸŸ"
    
    while true; do
        for region in "${REGIONS[@]}"; do
            if ! check_region_health $region; then
                log_error "åŒºåŸŸ $region å¥åº·æ£€æŸ¥å¤±è´¥"
                
                # å¦‚æœæ˜¯ä¸»åŒºåŸŸæ•…éšœï¼Œè§¦å‘æ•…éšœåˆ‡æ¢
                if [[ "$region" == "$PRIMARY_REGION" ]]; then
                    # é€‰æ‹©æ–°çš„ä¸»åŒºåŸŸ
                    local new_primary=""
                    for candidate in "${REGIONS[@]}"; do
                        if [[ "$candidate" != "$region" ]] && check_region_health $candidate; then
                            new_primary=$candidate
                            break
                        fi
                    done
                    
                    if [[ -n "$new_primary" ]]; then
                        initiate_failover $region $new_primary
                    else
                        log_error "æ²¡æœ‰å¯ç”¨çš„å¤‡é€‰ä¸»åŒºåŸŸ"
                    fi
                fi
            fi
        done
        
        sleep 300  # æ¯5åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
    done
}

# ä¸»èœå•
show_menu() {
    echo "==================================="
    echo "    å¤šåŒºåŸŸæ•°æ®åº“è¿ç»´ç®¡ç†å·¥å…·"
    echo "==================================="
    echo "1. æ£€æŸ¥æ‰€æœ‰åŒºåŸŸå¥åº·çŠ¶å†µ"
    echo "2. åˆ›å»ºè·¨åŒºåŸŸå¤‡ä»½"
    echo "3. æ‰‹åŠ¨è§¦å‘æ•…éšœåˆ‡æ¢"
    echo "4. å¯åŠ¨è‡ªåŠ¨ç›‘æ§"
    echo "5. æŸ¥çœ‹å½“å‰é…ç½®"
    echo "6. é€€å‡º"
    echo "==================================="
}

# ä¸»ç¨‹åºå¾ªç¯
while true; do
    show_menu
    read -p "è¯·é€‰æ‹©æ“ä½œ (1-6): " choice
    
    case $choice in
        1)
            log_info "æ£€æŸ¥æ‰€æœ‰åŒºåŸŸå¥åº·çŠ¶å†µ"
            for region in "${REGIONS[@]}"; do
                check_region_health $region
            done
            ;;
        2)
            read -p "è¯·è¾“å…¥æºåŒºåŸŸ (é»˜è®¤: $PRIMARY_REGION): " source_region
            source_region=${source_region:-$PRIMARY_REGION}
            create_cross_region_backup $source_region
            ;;
        3)
            read -p "è¯·è¾“å…¥æ•…éšœåŒºåŸŸ: " failed_region
            read -p "è¯·è¾“å…¥æ–°ä¸»åŒºåŸŸ: " new_primary
            initiate_failover $failed_region $new_primary
            ;;
        4)
            log_info "å¯åŠ¨è‡ªåŠ¨ç›‘æ§ (æŒ‰ Ctrl+C åœæ­¢)"
            monitor_all_regions
            ;;
        5)
            echo "å½“å‰é…ç½®:"
            echo "ä¸»åŒºåŸŸ: $PRIMARY_REGION"
            echo "æ‰€æœ‰åŒºåŸŸ: ${REGIONS[*]}"
            echo "å¤‡ä»½å­˜å‚¨æ¡¶: $BACKUP_BUCKET"
            ;;
        6)
            log_info "é€€å‡ºç¨‹åº"
            exit 0
            ;;
        *)
            log_error "æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥"
            ;;
    esac
    
    echo
done
```

---

## ğŸ” å…³é”®è¦ç‚¹æ€»ç»“

### âœ… å¤šåŒºåŸŸéƒ¨ç½²æˆåŠŸè¦ç´ 
- **åˆç†çš„æ¶æ„è®¾è®¡**ï¼šæ ¹æ®ä¸šåŠ¡éœ€æ±‚é€‰æ‹©åˆé€‚çš„éƒ¨ç½²æ¨¡å¼ï¼ˆActive-Activeã€Active-Passiveç­‰ï¼‰
- **å®Œå–„çš„ä¸€è‡´æ€§æœºåˆ¶**ï¼šå®ç°è·¨åŒºåŸŸæ•°æ®åŒæ­¥å’Œå†²çªè§£å†³ç­–ç•¥
- **æ™ºèƒ½çš„æ•…éšœåˆ‡æ¢**ï¼šå»ºç«‹è‡ªåŠ¨åŒ–çš„å¥åº·æ£€æµ‹å’Œæ•…éšœåˆ‡æ¢æœºåˆ¶
- **å…¨é¢çš„ç›‘æ§ä½“ç³»**ï¼šå®æ—¶ç›‘æ§å„åŒºåŸŸçŠ¶æ€ã€å»¶è¿Ÿå’Œæ€§èƒ½æŒ‡æ ‡

### âš ï¸ å¸¸è§é£é™©æé†’
- **ç½‘ç»œåˆ†åŒºé£é™©**ï¼šè·¨åŒºåŸŸç½‘ç»œä¸ç¨³å®šå¯èƒ½å¯¼è‡´æ•°æ®ä¸ä¸€è‡´
- **å»¶è¿Ÿå½±å“**ï¼šåœ°ç†è·ç¦»å¸¦æ¥çš„ç½‘ç»œå»¶è¿Ÿä¼šå½±å“ç”¨æˆ·ä½“éªŒ
- **æˆæœ¬æ§åˆ¶**ï¼šå¤šåŒºåŸŸéƒ¨ç½²æ˜¾è‘—å¢åŠ åŸºç¡€è®¾æ–½å’Œè¿ç»´æˆæœ¬
- **å¤æ‚æ€§ç®¡ç†**ï¼šåˆ†å¸ƒå¼ç³»ç»Ÿçš„å¤æ‚æ€§è¿œè¶…å•åŒºåŸŸéƒ¨ç½²

### ğŸ¯ æœ€ä½³å®è·µå»ºè®®
1. **æ¸è¿›å¼éƒ¨ç½²**ï¼šä»æ ¸å¿ƒåŒºåŸŸå¼€å§‹ï¼Œé€æ­¥æ‰©å±•åˆ°å…¶ä»–åŒºåŸŸ
2. **å……åˆ†çš„æµ‹è¯•éªŒè¯**ï¼šåœ¨ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²å‰è¿›è¡Œå®Œæ•´çš„åŠŸèƒ½å’Œæ€§èƒ½æµ‹è¯•
3. **å»ºç«‹åº”æ€¥å“åº”æœºåˆ¶**ï¼šåˆ¶å®šè¯¦ç»†çš„æ•…éšœå¤„ç†å’Œå›æ»šé¢„æ¡ˆ
4. **æŒç»­ä¼˜åŒ–è°ƒæ•´**ï¼šæ ¹æ®å®é™…è¿è¡Œæƒ…å†µä¸æ–­ä¼˜åŒ–é…ç½®å’Œç­–ç•¥
5. **æˆæœ¬æ•ˆç›Šåˆ†æ**ï¼šå®šæœŸè¯„ä¼°å¤šåŒºåŸŸéƒ¨ç½²çš„æŠ•èµ„å›æŠ¥ç‡

é€šè¿‡ç§‘å­¦çš„å¤šåŒºåŸŸæ•°æ®åº“éƒ¨ç½²å’Œç®¡ç†ï¼Œå¯ä»¥æ˜¾è‘—æå‡åº”ç”¨çš„å…¨çƒå¯ç”¨æ€§ã€é™ä½ç”¨æˆ·è®¿é—®å»¶è¿Ÿï¼Œå¹¶æ»¡è¶³æ•°æ®æœ¬åœ°åŒ–ç­‰åˆè§„è¦æ±‚ã€‚