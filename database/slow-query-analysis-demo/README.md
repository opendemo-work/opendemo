# ğŸ” æ•°æ®åº“æ…¢æŸ¥è¯¢åˆ†ææŒ‡å—

> ä¼ä¸šçº§æ•°æ®åº“æ€§èƒ½è¯Šæ–­ä½“ç³»ï¼Œæ¶µç›–æ…¢æŸ¥è¯¢è¯†åˆ«ã€åˆ†æå·¥å…·ä½¿ç”¨ã€ä¼˜åŒ–å»ºè®®ç”Ÿæˆç­‰å®Œæ•´çš„æ€§èƒ½é—®é¢˜è¯Šæ–­å’Œè§£å†³æ–¹æ¡ˆ

## ğŸ“‹ æ¡ˆä¾‹æ¦‚è¿°

æœ¬æ¡ˆä¾‹è¯¦ç»†ä»‹ç»æ•°æ®åº“æ…¢æŸ¥è¯¢çš„è¯Šæ–­æ–¹æ³•å’Œä¼˜åŒ–æŠ€æœ¯ï¼Œé€šè¿‡ç³»ç»Ÿæ€§çš„åˆ†æå·¥å…·å’Œä¼˜åŒ–ç­–ç•¥ï¼Œå¸®åŠ©DBAå¿«é€Ÿå®šä½å’Œè§£å†³æ•°æ®åº“æ€§èƒ½ç“¶é¢ˆé—®é¢˜ã€‚

### ğŸ¯ å­¦ä¹ ç›®æ ‡

- æŒæ¡æ…¢æŸ¥è¯¢è¯†åˆ«å’Œç›‘æ§æ–¹æ³•
- ç†Ÿç»ƒä½¿ç”¨å„ç§æ€§èƒ½åˆ†æå·¥å…·
- ç†è§£æ‰§è¡Œè®¡åˆ’çš„è§£è¯»å’Œä¼˜åŒ–
- å®æ–½ç³»ç»Ÿæ€§çš„æ€§èƒ½ä¼˜åŒ–ç­–ç•¥
- å»ºç«‹æŒç»­æ€§èƒ½ç›‘æ§æœºåˆ¶

### â±ï¸ å­¦ä¹ æ—¶é•¿

- **ç†è®ºå­¦ä¹ **: 3å°æ—¶
- **å®è·µæ“ä½œ**: 4å°æ—¶
- **æ€»è®¡**: 7å°æ—¶

---

## ğŸ“Š æ…¢æŸ¥è¯¢è¯Šæ–­æ¡†æ¶

### æ€§èƒ½é—®é¢˜åˆ†ç±»

```
æ•°æ®åº“æ€§èƒ½é—®é¢˜
â”œâ”€â”€ æŸ¥è¯¢æ€§èƒ½é—®é¢˜
â”‚   â”œâ”€â”€ æ…¢æŸ¥è¯¢
â”‚   â”œâ”€â”€ å…¨è¡¨æ‰«æ
â”‚   â”œâ”€â”€ ç´¢å¼•å¤±æ•ˆ
â”‚   â””â”€â”€ JOINæ•ˆç‡ä½ä¸‹
â”œâ”€â”€ ç³»ç»Ÿèµ„æºé—®é¢˜
â”‚   â”œâ”€â”€ CPUä½¿ç”¨ç‡é«˜
â”‚   â”œâ”€â”€ å†…å­˜ä¸è¶³
â”‚   â”œâ”€â”€ ç£ç›˜I/Oç“¶é¢ˆ
â”‚   â””â”€â”€ ç½‘ç»œå»¶è¿Ÿ
â”œâ”€â”€ æ¶æ„è®¾è®¡é—®é¢˜
â”‚   â”œâ”€â”€ è¡¨ç»“æ„ä¸åˆç†
â”‚   â”œâ”€â”€ ç´¢å¼•è®¾è®¡ç¼ºé™·
â”‚   â”œâ”€â”€ åˆ†åŒºç­–ç•¥ä¸å½“
â”‚   â””â”€â”€ ç¼“å­˜é…ç½®é”™è¯¯
â””â”€â”€ å¹¶å‘æ§åˆ¶é—®é¢˜
    â”œâ”€â”€ é”ç­‰å¾…
    â”œâ”€â”€ æ­»é”
    â”œâ”€â”€ è¿æ¥æ± è€—å°½
    â””â”€â”€ äº‹åŠ¡éš”ç¦»çº§åˆ«è¿‡é«˜
```

### è¯Šæ–­æµç¨‹

```
é—®é¢˜å‘ç°
    â†“
æ•°æ®æ”¶é›†
    â†“
æ ¹æœ¬åŸå› åˆ†æ
    â†“
è§£å†³æ–¹æ¡ˆåˆ¶å®š
    â†“
å®æ–½ä¼˜åŒ–
    â†“
æ•ˆæœéªŒè¯
    â†“
æŒç»­ç›‘æ§
```

---

## ğŸ¬ MySQLæ…¢æŸ¥è¯¢åˆ†æå®è·µ

### 1. æ…¢æŸ¥è¯¢ç›‘æ§é…ç½®

#### å¯ç”¨æ…¢æŸ¥è¯¢æ—¥å¿—
```sql
-- æŸ¥çœ‹å½“å‰æ…¢æŸ¥è¯¢é…ç½®
SHOW VARIABLES LIKE 'slow_query_log%';
SHOW VARIABLES LIKE 'long_query_time';
SHOW VARIABLES LIKE 'log_queries_not_using_indexes';

-- å¯ç”¨æ…¢æŸ¥è¯¢æ—¥å¿—
SET GLOBAL slow_query_log = 'ON';
SET GLOBAL slow_query_log_file = '/var/log/mysql/slow.log';
SET GLOBAL long_query_time = 1.0; -- è¶…è¿‡1ç§’çš„æŸ¥è¯¢è®°å½•
SET GLOBAL log_queries_not_using_indexes = 'ON'; -- è®°å½•æœªä½¿ç”¨ç´¢å¼•çš„æŸ¥è¯¢
SET GLOBAL log_throttle_queries_not_using_indexes = 10; -- é™åˆ¶æœªä½¿ç”¨ç´¢å¼•æŸ¥è¯¢çš„æ—¥å¿—é¢‘ç‡

-- éªŒè¯é…ç½®
SELECT @@slow_query_log, @@long_query_time, @@log_queries_not_using_indexes;
```

#### æ…¢æŸ¥è¯¢æ—¥å¿—é…ç½®æ–‡ä»¶
```ini
# /etc/my.cnf æ…¢æŸ¥è¯¢é…ç½®
[mysqld]
# æ…¢æŸ¥è¯¢æ—¥å¿—åŸºç¡€é…ç½®
slow_query_log = 1
slow_query_log_file = /var/log/mysql/slow.log
long_query_time = 1.0
log_queries_not_using_indexes = 1
log_throttle_queries_not_using_indexes = 10

# æ—¥å¿—æ ¼å¼ä¼˜åŒ–
log_slow_admin_statements = 1
log_slow_slave_statements = 1
log_output = FILE,TABLE  # åŒæ—¶è¾“å‡ºåˆ°æ–‡ä»¶å’Œè¡¨

# æ€§èƒ½æ¨¡å¼é…ç½®
performance_schema = ON
```

### 2. æ…¢æŸ¥è¯¢åˆ†æå·¥å…·

#### ä½¿ç”¨mysqldumpslowåˆ†æ
```bash
#!/bin/bash
# mysql_slow_query_analyzer.sh

LOG_FILE="/var/log/mysql/slow.log"
ANALYSIS_DIR="/var/reports/slow_queries"

# åˆ›å»ºåˆ†æç›®å½•
mkdir -p $ANALYSIS_DIR

# åŸºæœ¬ç»Ÿè®¡åˆ†æ
echo "=== æ…¢æŸ¥è¯¢åŸºæœ¬ç»Ÿè®¡ ===" > $ANALYSIS_DIR/analysis_$(date +%Y%m%d).txt
mysqldumpslow -s t -t 10 $LOG_FILE >> $ANALYSIS_DIR/analysis_$(date +%Y%m%d).txt

# æŒ‰å¹³å‡æ‰§è¡Œæ—¶é—´æ’åº
echo -e "\n=== æŒ‰å¹³å‡æ‰§è¡Œæ—¶é—´æ’åº ===" >> $ANALYSIS_DIR/analysis_$(date +%Y%m%d).txt
mysqldumpslow -s at -t 10 $LOG_FILE >> $ANALYSIS_DIR/analysis_$(date +%Y%m%d).txt

# æŒ‰æ‰§è¡Œæ¬¡æ•°æ’åº
echo -e "\n=== æŒ‰æ‰§è¡Œæ¬¡æ•°æ’åº ===" >> $ANALYSIS_DIR/analysis_$(date +%Y%m%d).txt
mysqldumpslow -s c -t 10 $LOG_FILE >> $ANALYSIS_DIR/analysis_$(date +%Y%m%d).txt

# åˆ†æç‰¹å®šæ¨¡å¼çš„æŸ¥è¯¢
echo -e "\n=== SELECTæŸ¥è¯¢åˆ†æ ===" >> $ANALYSIS_DIR/analysis_$(date +%Y%m%d).txt
mysqldumpslow -g "SELECT" $LOG_FILE >> $ANALYSIS_DIR/analysis_$(date +%Y%m%d).txt

# ç”ŸæˆHTMLæŠ¥å‘Š
cat > $ANALYSIS_DIR/report_$(date +%Y%m%d).html << 'EOF'
<!DOCTYPE html>
<html>
<head>
    <title>MySQLæ…¢æŸ¥è¯¢åˆ†ææŠ¥å‘Š</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 20px; }
        .section { margin: 20px 0; }
        .query { background: #f5f5f5; padding: 10px; margin: 5px 0; border-left: 4px solid #007cba; }
        .stats { display: grid; grid-template-columns: repeat(3, 1fr); gap: 10px; }
        .stat-card { background: #e9f7fe; padding: 15px; border-radius: 5px; }
    </style>
</head>
<body>
    <h1>MySQLæ…¢æŸ¥è¯¢åˆ†ææŠ¥å‘Š</h1>
    <div class="stats">
        <div class="stat-card">
            <h3>æ€»æ…¢æŸ¥è¯¢æ•°</h3>
            <p id="total_queries">-</p>
        </div>
        <div class="stat-card">
            <h3>å¹³å‡æ‰§è¡Œæ—¶é—´</h3>
            <p id="avg_time">-</p>
        </div>
        <div class="stat-card">
            <h3>æœ€æ…¢æŸ¥è¯¢</h3>
            <p id="slowest_query">-</p>
        </div>
    </div>
    <div class="section">
        <h2>Top 10 æ…¢æŸ¥è¯¢</h2>
        <div id="top_queries"></div>
    </div>
</body>
<script>
// JavaScriptä»£ç ç”¨äºåŠ¨æ€å¡«å……æ•°æ®
</script>
</html>
EOF
```

#### Percona Toolkitä½¿ç”¨
```bash
#!/bin/bash
# percona_toolkit_analyzer.sh

# å®‰è£…Percona Toolkit
# yum install percona-toolkit æˆ– apt-get install percona-toolkit

# ä½¿ç”¨pt-query-digeståˆ†ææ…¢æŸ¥è¯¢æ—¥å¿—
pt-query-digest \
    --limit 10 \
    --order-by Query_time:sum \
    --filter '($event->{Bytes} || 0) > 1024' \
    /var/log/mysql/slow.log > /var/reports/pt-query-digest-report.txt

# è¯¦ç»†åˆ†æç‰¹å®šæŸ¥è¯¢
pt-query-digest \
    --filter '$event->{arg} =~ m/SELECT.*users/i' \
    /var/log/mysql/slow.log > /var/reports/users-query-analysis.txt

# å®æ—¶ç›‘æ§æ¨¡å¼
pt-query-digest \
    --processlist h=localhost,u=root,p=password \
    --run-time 60 \
    --interval 5 \
    --review D=test,t=query_review \
    --history D=test,t=query_history
```

### 3. æ‰§è¡Œè®¡åˆ’æ·±åº¦åˆ†æ

#### EXPLAINè¯¦ç»†è§£è¯»
```sql
-- åŸºç¡€æ‰§è¡Œè®¡åˆ’åˆ†æ
EXPLAIN FORMAT=JSON 
SELECT u.username, o.order_date, o.total_amount
FROM users u 
JOIN orders o ON u.id = o.user_id 
WHERE u.status = 'active' 
AND o.order_date >= '2024-01-01'
ORDER BY o.order_date DESC 
LIMIT 10;

-- æ‰©å±•æ‰§è¡Œè®¡åˆ’
EXPLAIN FORMAT=TREE
SELECT u.username, COUNT(o.id) as order_count
FROM users u 
LEFT JOIN orders o ON u.id = o.user_id 
GROUP BY u.id, u.username
HAVING order_count > 5;

-- åˆ†æè¡¨ç»Ÿè®¡ä¿¡æ¯
ANALYZE TABLE users, orders;
SHOW TABLE STATUS LIKE 'users';
SHOW INDEX FROM users;

-- ä¼˜åŒ–å™¨è·Ÿè¸ª
SET optimizer_trace="enabled=on";
SELECT u.username, o.order_date
FROM users u 
JOIN orders o ON u.id = o.user_id 
WHERE u.email = 'user@example.com';
SELECT * FROM information_schema.OPTIMIZER_TRACE;
SET optimizer_trace="enabled=off";
```

#### æ€§èƒ½æ¨¡å¼åˆ†æ
```sql
-- å¯ç”¨æ€§èƒ½æ¨¡å¼ç›‘æ§
UPDATE performance_schema.setup_instruments 
SET ENABLED = 'YES', TIMED = 'YES' 
WHERE NAME LIKE '%statement%';

UPDATE performance_schema.setup_consumers 
SET ENABLED = 'YES' 
WHERE NAME LIKE '%statements%';

-- æŸ¥è¯¢æ‰§è¡Œç»Ÿè®¡
SELECT 
    DIGEST_TEXT,
    COUNT_STAR as execution_count,
    AVG_TIMER_WAIT/1000000000 as avg_time_sec,
    MAX_TIMER_WAIT/1000000000 as max_time_sec,
    SUM_ROWS_EXAMINED as total_rows_examined,
    SUM_CREATED_TMP_TABLES as tmp_tables_created,
    SUM_SELECT_FULL_JOIN as full_joins
FROM performance_schema.events_statements_summary_by_digest
WHERE SCHEMA_NAME = 'myapp'
AND AVG_TIMER_WAIT > 1000000000  -- è¶…è¿‡1ç§’çš„æŸ¥è¯¢
ORDER BY AVG_TIMER_WAIT DESC
LIMIT 10;

-- ç­‰å¾…äº‹ä»¶åˆ†æ
SELECT 
    EVENT_NAME,
    COUNT_STAR,
    SUM_TIMER_WAIT/1000000000 as total_wait_time_sec,
    AVG_TIMER_WAIT/1000000000 as avg_wait_time_sec
FROM performance_schema.events_waits_summary_global_by_event_name
WHERE COUNT_STAR > 0
ORDER BY SUM_TIMER_WAIT DESC
LIMIT 10;
```

---

## ğŸ˜ PostgreSQLæ…¢æŸ¥è¯¢åˆ†æå®è·µ

### 1. æ…¢æŸ¥è¯¢æ—¥å¿—é…ç½®

#### åŸºç¡€é…ç½®
```sql
-- æŸ¥çœ‹å½“å‰é…ç½®
SHOW log_min_duration_statement;
SHOW log_statement;
SHOW log_line_prefix;

-- é…ç½®æ…¢æŸ¥è¯¢æ—¥å¿—
ALTER SYSTEM SET log_min_duration_statement = 1000; -- 1ç§’
ALTER SYSTEM SET log_statement = 'none'; -- ä¸è®°å½•æ‰€æœ‰è¯­å¥
ALTER SYSTEM SET log_line_prefix = '%t [%p]: [%l-1] user=%u,db=%d,app=%a,client=%h ';
ALTER SYSTEM SET log_duration = on;
ALTER SYSTEM SET log_lock_waits = on;

-- é‡æ–°åŠ è½½é…ç½®
SELECT pg_reload_conf();
```

#### æ—¥å¿—æ ¼å¼ä¼˜åŒ–
```conf
# postgresql.conf è¯¦ç»†é…ç½®
logging_collector = on
log_directory = 'pg_log'
log_filename = 'postgresql-%Y-%m-%d_%H%M%S.log'
log_file_mode = 0600
log_truncate_on_rotation = off
log_rotation_age = 1d
log_rotation_size = 100MB

# æ…¢æŸ¥è¯¢ç›¸å…³é…ç½®
log_min_duration_statement = 1000
log_statement = 'none'
log_duration = on
log_line_prefix = '%t [%p]: [%l-1] user=%u,db=%d,app=%a,client=%h '
log_lock_waits = on
log_temp_files = 0
log_checkpoints = on
log_connections = on
log_disconnections = on
```

### 2. æŸ¥è¯¢æ€§èƒ½åˆ†æå·¥å…·

#### pgBadgeræ—¥å¿—åˆ†æ
```bash
#!/bin/bash
# pgbadger_analyzer.sh

# å®‰è£…pgBadger
# cpan install pgBadger æˆ– yum install pgbadger

# ç”ŸæˆHTMLæŠ¥å‘Š
pgbadger \
    -I \
    -O /var/reports/pgbadger \
    -o report_$(date +%Y%m%d).html \
    /var/lib/pgsql/data/pg_log/postgresql-*.log

# è¯¦ç»†åˆ†æç‰¹å®šæ—¶é—´æ®µ
pgbadger \
    --begin 2024-01-01 00:00:00 \
    --end 2024-01-01 23:59:59 \
    -O /var/reports/pgbadger_daily \
    -o daily_report_20240101.html \
    /var/lib/pgsql/data/pg_log/postgresql-2024-01-01*.log

# æŒ‰æŸ¥è¯¢ç±»å‹åˆ†ç±»åˆ†æ
pgbadger \
    --exclude-query "autovacuum.*" \
    --include-query-time 5 \
    -O /var/reports/pgbadger_filtered \
    -o filtered_report.html \
    /var/lib/pgsql/data/pg_log/postgresql-*.log
```

#### è‡ªå®šä¹‰åˆ†æè„šæœ¬
```python
#!/usr/bin/env python3
# postgres_slow_query_analyzer.py

import psycopg2
import re
from datetime import datetime, timedelta
from collections import defaultdict, Counter
import json

class PostgresSlowQueryAnalyzer:
    def __init__(self, connection_params):
        self.conn_params = connection_params
        self.queries = []
        
    def connect_and_extract(self):
        """è¿æ¥æ•°æ®åº“å¹¶æå–æ…¢æŸ¥è¯¢ä¿¡æ¯"""
        try:
            conn = psycopg2.connect(**self.conn_params)
            cur = conn.cursor()
            
            # ä»pg_stat_statementsè·å–æ…¢æŸ¥è¯¢
            cur.execute("""
                SELECT 
                    query,
                    calls,
                    total_time,
                    mean_time,
                    rows,
                    100.0 * shared_blks_hit / nullif(shared_blks_hit + shared_blks_read, 0) as hit_percent,
                    temp_blks_written,
                    blk_read_time,
                    blk_write_time
                FROM pg_stat_statements 
                WHERE userid = (SELECT usesysid FROM pg_user WHERE usename = current_user)
                AND mean_time > 1000  -- è¶…è¿‡1ç§’çš„æŸ¥è¯¢
                ORDER BY mean_time DESC
                LIMIT 50
            """)
            
            columns = [desc[0] for desc in cur.description]
            for row in cur.fetchall():
                query_info = dict(zip(columns, row))
                self.queries.append(query_info)
            
            cur.close()
            conn.close()
            
        except Exception as e:
            print(f"æ•°æ®åº“è¿æ¥é”™è¯¯: {e}")
            return False
        return True
    
    def analyze_query_patterns(self):
        """åˆ†ææŸ¥è¯¢æ¨¡å¼"""
        pattern_stats = defaultdict(lambda: {'count': 0, 'total_time': 0, 'queries': []})
        
        for query_info in self.queries:
            query = query_info['query'].strip()
            
            # è¯†åˆ«æŸ¥è¯¢ç±»å‹
            if query.upper().startswith('SELECT'):
                pattern = 'SELECT'
            elif query.upper().startswith('INSERT'):
                pattern = 'INSERT'
            elif query.upper().startswith('UPDATE'):
                pattern = 'UPDATE'
            elif query.upper().startswith('DELETE'):
                pattern = 'DELETE'
            else:
                pattern = 'OTHER'
            
            pattern_stats[pattern]['count'] += 1
            pattern_stats[pattern]['total_time'] += query_info['total_time']
            pattern_stats[pattern]['queries'].append({
                'query': query[:100] + '...' if len(query) > 100 else query,
                'mean_time': query_info['mean_time'],
                'calls': query_info['calls']
            })
        
        return dict(pattern_stats)
    
    def identify_performance_issues(self):
        """è¯†åˆ«æ€§èƒ½é—®é¢˜"""
        issues = []
        
        for query_info in self.queries:
            query = query_info['query']
            mean_time = query_info['mean_time']
            hit_percent = query_info['hit_percent'] or 0
            
            # ç¼“å†²åŒºå‘½ä¸­ç‡ä½
            if hit_percent < 80:
                issues.append({
                    'type': 'LOW_BUFFER_HIT_RATE',
                    'query': query[:200],
                    'hit_percent': hit_percent,
                    'severity': 'HIGH' if hit_percent < 50 else 'MEDIUM'
                })
            
            # äº§ç”Ÿå¤§é‡ä¸´æ—¶æ–‡ä»¶
            if query_info['temp_blks_written'] > 1000:
                issues.append({
                    'type': 'HIGH_TEMP_BLOCKS',
                    'query': query[:200],
                    'temp_blocks': query_info['temp_blks_written'],
                    'severity': 'HIGH'
                })
            
            # I/Oç­‰å¾…æ—¶é—´é•¿
            io_time = query_info['blk_read_time'] + query_info['blk_write_time']
            if io_time > 1000:
                issues.append({
                    'type': 'HIGH_IO_WAIT',
                    'query': query[:200],
                    'io_time': io_time,
                    'severity': 'HIGH' if io_time > 5000 else 'MEDIUM'
                })
        
        return issues
    
    def generate_recommendations(self):
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []
        
        pattern_stats = self.analyze_query_patterns()
        
        # åŸºäºæŸ¥è¯¢æ¨¡å¼çš„å»ºè®®
        if pattern_stats.get('SELECT', {}).get('count', 0) > len(self.queries) * 0.7:
            recommendations.append({
                'category': 'QUERY_PATTERN',
                'issue': 'å¤§é‡SELECTæŸ¥è¯¢',
                'recommendation': 'è€ƒè™‘æ·»åŠ é€‚å½“çš„ç´¢å¼•æˆ–ä¼˜åŒ–æŸ¥è¯¢æ¡ä»¶'
            })
        
        # åŸºäºæ€§èƒ½é—®é¢˜çš„å»ºè®®
        issues = self.identify_performance_issues()
        high_severity_issues = [issue for issue in issues if issue['severity'] == 'HIGH']
        
        if high_severity_issues:
            recommendations.append({
                'category': 'PERFORMANCE',
                'issue': f'å‘ç°{len(high_severity_issues)}ä¸ªé«˜ä¸¥é‡æ€§æ€§èƒ½é—®é¢˜',
                'recommendation': 'ç«‹å³ä¼˜åŒ–ç›¸å…³æŸ¥è¯¢ï¼Œé‡ç‚¹å…³æ³¨ç¼“å†²åŒºå‘½ä¸­ç‡å’ŒI/Oæ€§èƒ½'
            })
        
        return recommendations
    
    def export_report(self, filename):
        """å¯¼å‡ºåˆ†ææŠ¥å‘Š"""
        report = {
            'generated_at': datetime.now().isoformat(),
            'total_slow_queries': len(self.queries),
            'pattern_analysis': self.analyze_query_patterns(),
            'performance_issues': self.identify_performance_issues(),
            'recommendations': self.generate_recommendations(),
            'top_queries': self.queries[:10]  # Top 10æ…¢æŸ¥è¯¢
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    analyzer = PostgresSlowQueryAnalyzer({
        'host': 'localhost',
        'database': 'myapp',
        'user': 'postgres',
        'password: "${DB_PASSWORD}"
    })
    
    if analyzer.connect_and_extract():
        analyzer.export_report('/var/reports/postgres_slow_query_report.json')
        print("åˆ†ææŠ¥å‘Šå·²ç”Ÿæˆ")
```

### 3. æ‰§è¡Œè®¡åˆ’è¯¦è§£

#### EXPLAIN ANALYZEæ·±åº¦åˆ†æ
```sql
-- è¯¦ç»†æ‰§è¡Œè®¡åˆ’åˆ†æ
EXPLAIN (ANALYZE, BUFFERS, VERBOSE, FORMAT JSON)
SELECT 
    u.username,
    COUNT(o.id) as order_count,
    AVG(o.total_amount) as avg_order_value
FROM users u 
JOIN orders o ON u.id = o.user_id 
WHERE u.created_at >= '2024-01-01'
AND u.status = 'active'
GROUP BY u.id, u.username
HAVING COUNT(o.id) > 5
ORDER BY avg_order_value DESC
LIMIT 20;

-- å¹¶è¡ŒæŸ¥è¯¢åˆ†æ
EXPLAIN (ANALYZE, BUFFERS, VERBOSE)
SELECT 
    p.category,
    COUNT(*) as product_count,
    AVG(p.price) as avg_price
FROM products p 
WHERE p.price > 100
GROUP BY p.category
ORDER BY avg_price DESC;

-- ç´¢å¼•ä½¿ç”¨æƒ…å†µåˆ†æ
SELECT 
    schemaname,
    tablename,
    indexname,
    idx_tup_read,
    idx_tup_fetch,
    idx_scan,
    100.0 * idx_tup_fetch / nullif(idx_tup_read, 0) as fetch_ratio
FROM pg_stat_user_indexes 
WHERE schemaname = 'public'
ORDER BY idx_scan DESC;
```

---

## ğŸƒ MongoDBæ…¢æŸ¥è¯¢åˆ†æå®è·µ

### 1. æ…¢æŸ¥è¯¢é…ç½®å’Œç›‘æ§

#### å¯ç”¨æ…¢æŸ¥è¯¢æ—¥å¿—
```javascript
// MongoDBæ…¢æŸ¥è¯¢é…ç½®
use admin

// è®¾ç½®æ…¢æŸ¥è¯¢é˜ˆå€¼
db.setProfilingLevel(1, { slowms: 1000 })  // è®°å½•è¶…è¿‡1ç§’çš„æŸ¥è¯¢

// æˆ–è€…è®¾ç½®ä¸ºè®°å½•æ‰€æœ‰æ“ä½œ
// db.setProfilingLevel(2)

// æŸ¥çœ‹å½“å‰é…ç½®
db.getProfilingStatus()

// æŸ¥çœ‹æ…¢æŸ¥è¯¢æ—¥å¿—
db.system.profile.find().sort({ ts: -1 }).limit(10)

// åˆ†æç‰¹å®šæ—¶é—´èŒƒå›´çš„æ…¢æŸ¥è¯¢
db.system.profile.find({
    ts: {
        $gte: new Date(Date.now() - 24*60*60*1000),  // æœ€è¿‘24å°æ—¶
        $lte: new Date()
    },
    millis: { $gte: 1000 }  // è¶…è¿‡1ç§’çš„æ“ä½œ
}).sort({ millis: -1 })
```

#### æ…¢æŸ¥è¯¢èšåˆåˆ†æ
```javascript
// æ…¢æŸ¥è¯¢ç»Ÿè®¡åˆ†æ
use admin

// æŒ‰æ“ä½œç±»å‹ç»Ÿè®¡
db.system.profile.aggregate([
    { $match: { millis: { $gte: 1000 } } },
    { $group: {
        _id: "$op",
        count: { $sum: 1 },
        avgDuration: { $avg: "$millis" },
        maxDuration: { $max: "$millis" },
        minDuration: { $min: "$millis" }
    }},
    { $sort: { avgDuration: -1 } }
])

// æŒ‰å‘½åç©ºé—´ç»Ÿè®¡
db.system.profile.aggregate([
    { $match: { millis: { $gte: 1000 } } },
    { $group: {
        _id: "$ns",
        count: { $sum: 1 },
        totalDuration: { $sum: "$millis" },
        avgDuration: { $avg: "$millis" },
        operations: { $addToSet: { op: "$op", query: "$query" } }
    }},
    { $sort: { totalDuration: -1 } },
    { $limit: 10 }
])

// è¯†åˆ«æœ€è€—æ—¶çš„æŸ¥è¯¢æ¨¡å¼
db.system.profile.aggregate([
    { $match: { 
        millis: { $gte: 1000 },
        op: "query"
    }},
    { $project: {
        queryPattern: { $substr: ["$query", 0, 200] },
        duration: "$millis",
        timestamp: "$ts"
    }},
    { $group: {
        _id: "$queryPattern",
        count: { $sum: 1 },
        avgDuration: { $avg: "$duration" },
        maxDuration: { $max: "$duration" },
        firstSeen: { $min: "$timestamp" },
        lastSeen: { $max: "$timestamp" }
    }},
    { $sort: { avgDuration: -1 } },
    { $limit: 20 }
])
```

### 2. æ€§èƒ½åˆ†æå·¥å…·

#### MongoDB Compassæ€§èƒ½åˆ†æå™¨
```javascript
// ä½¿ç”¨MongoDB Compassçš„æ€§èƒ½åˆ†æåŠŸèƒ½
// 1. è¿æ¥åˆ°æ•°æ®åº“
// 2. æ‰“å¼€Performance Tab
// 3. æŸ¥çœ‹å®æ—¶æ€§èƒ½æŒ‡æ ‡
// 4. åˆ†ææ…¢æŸ¥è¯¢å’Œç´¢å¼•ä½¿ç”¨æƒ…å†µ

// é€šè¿‡shellæ¨¡æ‹Ÿåˆ†æ
function analyzeMongoDBPerformance() {
    const analysis = {
        timestamp: new Date(),
        slowQueries: [],
        indexUsage: {},
        connectionStats: {}
    };
    
    // åˆ†ææ…¢æŸ¥è¯¢
    const slowQueries = db.system.profile.find({
        millis: { $gte: 1000 }
    }).sort({ millis: -1 }).limit(50);
    
    slowQueries.forEach(query => {
        analysis.slowQueries.push({
            operation: query.op,
            namespace: query.ns,
            duration: query.millis,
            query: JSON.stringify(query.query),
            timestamp: query.ts
        });
    });
    
    // åˆ†æç´¢å¼•ä½¿ç”¨æƒ…å†µ
    db.getSiblingDB("myapp").getCollectionInfos().forEach(collection => {
        if (collection.name !== "system.profile") {
            const stats = db[collection.name].stats();
            analysis.indexUsage[collection.name] = {
                size: stats.size,
                count: stats.count,
                avgObjSize: stats.avgObjSize,
                indexes: stats.nindexes
            };
        }
    });
    
    return analysis;
}

// æ‰§è¡Œåˆ†æ
const perfAnalysis = analyzeMongoDBPerformance();
printjson(perfAnalysis);
```

#### è‡ªå®šä¹‰ç›‘æ§è„šæœ¬
```python
#!/usr/bin/env python3
# mongodb_performance_analyzer.py

from pymongo import MongoClient
from datetime import datetime, timedelta
import json
from collections import defaultdict, Counter

class MongoDBPerformanceAnalyzer:
    def __init__(self, connection_string="mongodb://localhost:27017/"):
        self.client = MongoClient(connection_string)
        self.db = self.client.admin
        
    def get_slow_queries(self, hours_back=24, min_duration_ms=1000):
        """è·å–æ…¢æŸ¥è¯¢"""
        cutoff_time = datetime.utcnow() - timedelta(hours=hours_back)
        
        slow_queries = self.db.system.profile.find({
            "ts": {"$gte": cutoff_time},
            "millis": {"$gte": min_duration_ms}
        }).sort("millis", -1)
        
        return list(slow_queries)
    
    def analyze_query_patterns(self, slow_queries):
        """åˆ†ææŸ¥è¯¢æ¨¡å¼"""
        patterns = defaultdict(lambda: {
            'count': 0,
            'total_duration': 0,
            'avg_duration': 0,
            'max_duration': 0,
            'sample_queries': []
        })
        
        for query in slow_queries:
            # æå–æŸ¥è¯¢ç‰¹å¾
            op_type = query.get('op', 'unknown')
            namespace = query.get('ns', 'unknown')
            duration = query.get('millis', 0)
            
            pattern_key = f"{op_type}:{namespace}"
            
            patterns[pattern_key]['count'] += 1
            patterns[pattern_key]['total_duration'] += duration
            patterns[pattern_key]['max_duration'] = max(
                patterns[pattern_key]['max_duration'], 
                duration
            )
            
            # ä¿å­˜æ ·æœ¬æŸ¥è¯¢ï¼ˆæœ€å¤š3ä¸ªï¼‰
            if len(patterns[pattern_key]['sample_queries']) < 3:
                patterns[pattern_key]['sample_queries'].append({
                    'query': str(query.get('query', ''))[:200],
                    'duration': duration,
                    'timestamp': query.get('ts')
                })
        
        # è®¡ç®—å¹³å‡æŒç»­æ—¶é—´
        for pattern in patterns.values():
            if pattern['count'] > 0:
                pattern['avg_duration'] = pattern['total_duration'] / pattern['count']
        
        return dict(patterns)
    
    def analyze_index_usage(self):
        """åˆ†æç´¢å¼•ä½¿ç”¨æƒ…å†µ"""
        index_analysis = {}
        
        # è·å–æ‰€æœ‰æ•°æ®åº“
        databases = self.client.list_database_names()
        
        for db_name in databases:
            if db_name in ['admin', 'config', 'local']:
                continue
                
            db = self.client[db_name]
            index_analysis[db_name] = {}
            
            # è·å–é›†åˆä¿¡æ¯
            collections = db.list_collection_names()
            
            for collection_name in collections:
                try:
                    # è·å–é›†åˆç»Ÿè®¡
                    stats = db.command("collStats", collection_name)
                    
                    # è·å–ç´¢å¼•ä¿¡æ¯
                    indexes = list(db[collection_name].list_indexes())
                    
                    index_analysis[db_name][collection_name] = {
                        'document_count': stats.get('count', 0),
                        'size_bytes': stats.get('size', 0),
                        'avg_obj_size': stats.get('avgObjSize', 0),
                        'index_count': len(indexes),
                        'indexes': [idx.get('name') for idx in indexes]
                    }
                except Exception as e:
                    print(f"Error analyzing {db_name}.{collection_name}: {e}")
        
        return index_analysis
    
    def identify_performance_bottlenecks(self, slow_queries):
        """è¯†åˆ«æ€§èƒ½ç“¶é¢ˆ"""
        bottlenecks = []
        
        # æŒ‰æ“ä½œç±»å‹åˆ†ç±»
        op_counts = Counter(query.get('op', 'unknown') for query in slow_queries)
        
        for op_type, count in op_counts.most_common():
            if count > len(slow_queries) * 0.3:  # è¶…è¿‡30%çš„æ…¢æŸ¥è¯¢
                bottlenecks.append({
                    'type': 'OPERATION_TYPE_BOTTLENECK',
                    'operation': op_type,
                    'count': count,
                    'percentage': round(count / len(slow_queries) * 100, 2),
                    'severity': 'HIGH' if count > len(slow_queries) * 0.5 else 'MEDIUM'
                })
        
        # è¯†åˆ«é•¿æ—¶é—´è¿è¡Œçš„æŸ¥è¯¢
        long_running = [q for q in slow_queries if q.get('millis', 0) > 10000]  # è¶…è¿‡10ç§’
        if long_running:
            bottlenecks.append({
                'type': 'LONG_RUNNING_QUERIES',
                'count': len(long_running),
                'max_duration': max(q.get('millis', 0) for q in long_running),
                'severity': 'HIGH'
            })
        
        return bottlenecks
    
    def generate_optimization_recommendations(self, slow_queries, index_analysis):
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []
        
        # åŸºäºæŸ¥è¯¢åˆ†æçš„å»ºè®®
        patterns = self.analyze_query_patterns(slow_queries)
        
        for pattern, stats in patterns.items():
            if stats['avg_duration'] > 5000:  # å¹³å‡è¶…è¿‡5ç§’
                recommendations.append({
                    'category': 'QUERY_OPTIMIZATION',
                    'issue': f'æ…¢æŸ¥è¯¢æ¨¡å¼: {pattern}',
                    'impact': f'å¹³å‡æ‰§è¡Œæ—¶é—´ {stats["avg_duration"]:.0f}ms',
                    'recommendation': 'åˆ†ææŸ¥è¯¢æ¡ä»¶ï¼Œè€ƒè™‘æ·»åŠ é€‚å½“ç´¢å¼•'
                })
        
        # åŸºäºç´¢å¼•åˆ†æçš„å»ºè®®
        for db_name, collections in index_analysis.items():
            for collection_name, stats in collections.items():
                if stats['document_count'] > 100000 and stats['index_count'] < 3:
                    recommendations.append({
                        'category': 'INDEX_DESIGN',
                        'issue': f'{db_name}.{collection_name} ç´¢å¼•ä¸è¶³',
                        'impact': f'{stats["document_count"]} æ–‡æ¡£ä»…æœ‰ {stats["index_count"]} ä¸ªç´¢å¼•',
                        'recommendation': 'åˆ†ææŸ¥è¯¢æ¨¡å¼ï¼Œæ·»åŠ å¿…è¦çš„å¤åˆç´¢å¼•'
                    })
        
        return recommendations
    
    def export_comprehensive_report(self, output_file, hours_back=24):
        """å¯¼å‡ºç»¼åˆæ€§èƒ½æŠ¥å‘Š"""
        slow_queries = self.get_slow_queries(hours_back)
        index_analysis = self.analyze_index_usage()
        bottlenecks = self.identify_performance_bottlenecks(slow_queries)
        recommendations = self.generate_optimization_recommendations(slow_queries, index_analysis)
        
        report = {
            'generated_at': datetime.utcnow().isoformat(),
            'analysis_period_hours': hours_back,
            'total_slow_queries': len(slow_queries),
            'query_patterns': self.analyze_query_patterns(slow_queries),
            'index_analysis': index_analysis,
            'performance_bottlenecks': bottlenecks,
            'optimization_recommendations': recommendations,
            'top_slow_queries': [
                {
                    'operation': q.get('op'),
                    'namespace': q.get('ns'),
                    'duration_ms': q.get('millis'),
                    'timestamp': q.get('ts').isoformat() if q.get('ts') else None,
                    'query_sample': str(q.get('query', ''))[:300]
                }
                for q in slow_queries[:20]  # Top 20æ…¢æŸ¥è¯¢
            ]
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False, default=str)
        
        return report

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    analyzer = MongoDBPerformanceAnalyzer()
    report = analyzer.export_comprehensive_report(
        '/var/reports/mongodb_performance_report.json',
        hours_back=48
    )
    print(f"æ€§èƒ½æŠ¥å‘Šå·²ç”Ÿæˆï¼ŒåŒ…å« {report['total_slow_queries']} ä¸ªæ…¢æŸ¥è¯¢")
```

---

## ğŸ” æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

### é€šç”¨ä¼˜åŒ–åŸåˆ™

#### 1. ç´¢å¼•ä¼˜åŒ–ç­–ç•¥
```sql
-- ç´¢å¼•ä½¿ç”¨åˆ†æ
SELECT 
    table_name,
    index_name,
    seq_in_index,
    column_name,
    cardinality,
    100.0 * cardinality / table_rows as selectivity
FROM information_schema.statistics s
JOIN information_schema.tables t ON s.table_schema = t.table_schema AND s.table_name = t.table_name
WHERE s.table_schema = 'myapp'
AND t.table_type = 'BASE TABLE'
ORDER BY table_name, index_name, seq_in_index;

-- ç´¢å¼•æ•ˆæœè¯„ä¼°
EXPLAIN FORMAT=JSON
SELECT u.username, o.order_date
FROM users u 
JOIN orders o ON u.id = o.user_id 
WHERE u.status = 'active' 
AND o.total_amount > 1000;

-- ç´¢å¼•å»ºè®®ç”Ÿæˆ
SELECT 
    t.table_schema,
    t.table_name,
    ROUND((t.data_length + t.index_length) / 1024 / 1024, 2) AS total_size_mb,
    t.table_rows,
    COUNT(k.column_name) AS indexed_columns,
    GROUP_CONCAT(k.column_name ORDER BY k.seq_in_index) AS indexed_columns_list
FROM information_schema.tables t
LEFT JOIN information_schema.key_column_usage k 
    ON t.table_schema = k.table_schema 
    AND t.table_name = k.table_name
    AND k.constraint_name = 'PRIMARY'
WHERE t.table_schema = 'myapp'
GROUP BY t.table_schema, t.table_name
HAVING indexed_columns = 0  -- æ— ä¸»é”®çš„è¡¨
ORDER BY total_size_mb DESC;
```

#### 2. æŸ¥è¯¢é‡æ„ä¼˜åŒ–
```sql
-- å­æŸ¥è¯¢ä¼˜åŒ–ä¸ºJOIN
-- ä¼˜åŒ–å‰
SELECT u.username 
FROM users u 
WHERE u.id IN (
    SELECT user_id FROM orders WHERE total_amount > 1000
);

-- ä¼˜åŒ–å
SELECT DISTINCT u.username
FROM users u 
JOIN orders o ON u.id = o.user_id 
WHERE o.total_amount > 1000;

-- EXISTSä¼˜åŒ–
-- ä¼˜åŒ–å‰
SELECT u.username 
FROM users u 
WHERE (
    SELECT COUNT(*) FROM orders o WHERE o.user_id = u.id
) > 5;

-- ä¼˜åŒ–å
SELECT u.username
FROM users u 
WHERE EXISTS (
    SELECT 1 FROM orders o 
    WHERE o.user_id = u.id 
    LIMIT 5
);

-- LIMITä¼˜åŒ–
-- ä¼˜åŒ–å‰
SELECT * FROM users ORDER BY created_at DESC;

-- ä¼˜åŒ–å
SELECT id, username, email, created_at 
FROM users 
ORDER BY created_at DESC 
LIMIT 100;
```

#### 3. é…ç½®å‚æ•°ä¼˜åŒ–
```sql
-- MySQLé…ç½®ä¼˜åŒ–
SET GLOBAL innodb_buffer_pool_size = 2147483648;  -- 2GB
SET GLOBAL query_cache_size = 134217728;          -- 128MB
SET GLOBAL tmp_table_size = 67108864;             -- 64MB
SET GLOBAL max_heap_table_size = 67108864;        -- 64MB
SET GLOBAL sort_buffer_size = 2097152;            -- 2MB
SET GLOBAL read_buffer_size = 1048576;            -- 1MB
SET GLOBAL join_buffer_size = 1048576;            -- 1MB

-- PostgreSQLé…ç½®ä¼˜åŒ–
ALTER SYSTEM SET shared_buffers = '2GB';
ALTER SYSTEM SET effective_cache_size = '6GB';
ALTER SYSTEM SET work_mem = '64MB';
ALTER SYSTEM SET maintenance_work_mem = '512MB';
ALTER SYSTEM SET checkpoint_completion_target = 0.9;
ALTER SYSTEM SET wal_buffers = '16MB';
ALTER SYSTEM SET default_statistics_target = 100;

-- é‡æ–°åŠ è½½é…ç½®
SELECT pg_reload_conf();
```

---

## ğŸ“Š æ€§èƒ½ç›‘æ§å’Œå‘Šè­¦

### å®æ—¶ç›‘æ§è„šæœ¬

#### ç»¼åˆæ€§èƒ½ç›‘æ§
```bash
#!/bin/bash
# database_performance_monitor.sh

ALERT_EMAIL="dba@company.com"
LOG_DIR="/var/log/performance"
REPORT_DIR="/var/reports/performance"

# åˆ›å»ºç›®å½•
mkdir -p $LOG_DIR $REPORT_DIR

# MySQLæ€§èƒ½ç›‘æ§
monitor_mysql_performance() {
    local timestamp=$(date '+%Y-%m-%d %H:%M:%S')
    
    # æ…¢æŸ¥è¯¢æ•°é‡
    local slow_queries=$(mysql -e "SHOW GLOBAL STATUS LIKE 'Slow_queries';" | awk 'NR==2 {print $2}')
    
    # è¿æ¥æ•°
    local connections=$(mysql -e "SHOW STATUS LIKE 'Threads_connected';" | awk 'NR==2 {print $2}')
    local max_connections=$(mysql -e "SHOW VARIABLES LIKE 'max_connections';" | awk 'NR==2 {print $2}')
    
    # QPSå’ŒTPS
    local questions=$(mysql -e "SHOW GLOBAL STATUS LIKE 'Questions';" | awk 'NR==2 {print $2}')
    local commits=$(mysql -e "SHOW GLOBAL STATUS LIKE 'Com_commit';" | awk 'NR==2 {print $2}')
    
    echo "[$timestamp] MySQL - Slow Queries: $slow_queries, Connections: $connections/$max_connections, QPS: $questions, TPS: $commits" >> $LOG_DIR/mysql_performance.log
    
    # å‘Šè­¦æ£€æŸ¥
    if [ $slow_queries -gt 100 ] || [ $connections -gt $((max_connections * 80 / 100)) ]; then
        echo "MySQL Performance Alert - Slow Queries: $slow_queries, Connections: $connections/$max_connections" | \
        mail -s "MySQL Performance Alert" $ALERT_EMAIL
    fi
}

# PostgreSQLæ€§èƒ½ç›‘æ§
monitor_postgresql_performance() {
    local timestamp=$(date '+%Y-%m-%d %H:%M:%S')
    
    # æ´»è·ƒè¿æ¥æ•°
    local active_connections=$(psql -t -c "SELECT count(*) FROM pg_stat_activity WHERE state = 'active';" 2>/dev/null || echo "0")
    
    # æ…¢æŸ¥è¯¢ç»Ÿè®¡
    local slow_queries=$(psql -t -c "
        SELECT count(*) FROM pg_stat_statements 
        WHERE mean_time > 1000;
    " 2>/dev/null || echo "0")
    
    # ç¼“å†²åŒºå‘½ä¸­ç‡
    local buffer_hit_ratio=$(psql -t -c "
        SELECT round(blks_hit::numeric / (blks_hit + blks_read + 1) * 100, 2) 
        FROM pg_stat_database 
        WHERE datname = current_database();
    " 2>/dev/null || echo "0")
    
    echo "[$timestamp] PostgreSQL - Active Connections: $active_connections, Slow Queries: $slow_queries, Buffer Hit Ratio: ${buffer_hit_ratio}%" >> $LOG_DIR/postgresql_performance.log
    
    # å‘Šè­¦æ£€æŸ¥
    if [ $active_connections -gt 50 ] || [ $(echo "$buffer_hit_ratio < 85" | bc) -eq 1 ]; then
        echo "PostgreSQL Performance Alert - Active Connections: $active_connections, Buffer Hit Ratio: ${buffer_hit_ratio}%" | \
        mail -s "PostgreSQL Performance Alert" $ALERT_EMAIL
    fi
}

# MongoDBæ€§èƒ½ç›‘æ§
monitor_mongodb_performance() {
    local timestamp=$(date '+%Y-%m-%d %H:%M:%S')
    
    # è¿æ¥æ•°
    local connections=$(mongo --quiet --eval "db.serverStatus().connections" 2>/dev/null | grep "current" | awk '{print $2}' | tr -d ',')
    
    # æ“ä½œè®¡æ•°
    local opcounters=$(mongo --quiet --eval "db.serverStatus().opcounters" 2>/dev/null)
    
    echo "[$timestamp] MongoDB - Connections: $connections" >> $LOG_DIR/mongodb_performance.log
    echo "$opcounters" >> $LOG_DIR/mongodb_performance.log
    
    # å‘Šè­¦æ£€æŸ¥
    if [ $connections -gt 1000 ]; then
        echo "MongoDB Connection Alert - Current Connections: $connections" | \
        mail -s "MongoDB Connection Alert" $ALERT_EMAIL
    fi
}

# æ‰§è¡Œç›‘æ§
while true; do
    monitor_mysql_performance
    monitor_postgresql_performance
    monitor_mongodb_performance
    
    # æ¯5åˆ†é’Ÿæ‰§è¡Œä¸€æ¬¡
    sleep 300
done
```

### æ€§èƒ½è¶‹åŠ¿åˆ†æ
```sql
-- åˆ›å»ºæ€§èƒ½ç›‘æ§è¡¨
CREATE TABLE performance_metrics (
    id BIGINT AUTO_INCREMENT PRIMARY KEY,
    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    database_type ENUM('MYSQL', 'POSTGRESQL', 'MONGODB') NOT NULL,
    metric_name VARCHAR(100) NOT NULL,
    metric_value DECIMAL(15,4) NOT NULL,
    threshold_value DECIMAL(15,4),
    alert_triggered BOOLEAN DEFAULT FALSE
);

-- æ€§èƒ½è¶‹åŠ¿åˆ†æè§†å›¾
CREATE VIEW performance_trends AS
SELECT 
    database_type,
    metric_name,
    DATE(timestamp) as metric_date,
    AVG(metric_value) as avg_value,
    MAX(metric_value) as max_value,
    MIN(metric_value) as min_value,
    COUNT(*) as sample_count
FROM performance_metrics
WHERE timestamp >= DATE_SUB(NOW(), INTERVAL 30 DAY)
GROUP BY database_type, metric_name, DATE(timestamp)
ORDER BY database_type, metric_name, metric_date;

-- å¼‚å¸¸æ£€æµ‹æŸ¥è¯¢
SELECT 
    database_type,
    metric_name,
    timestamp,
    metric_value,
    threshold_value,
    ((metric_value - threshold_value) / threshold_value * 100) as deviation_percent
FROM performance_metrics
WHERE alert_triggered = TRUE
AND timestamp >= DATE_SUB(NOW(), INTERVAL 7 DAY)
ORDER BY timestamp DESC;
```

---

## ğŸ“‹ æ€§èƒ½ä¼˜åŒ–æ£€æŸ¥æ¸…å•

### ç³»ç»Ÿæ€§ä¼˜åŒ–æµç¨‹

```markdown
# æ•°æ®åº“æ€§èƒ½ä¼˜åŒ–æ£€æŸ¥æ¸…å•

## 1. é—®é¢˜è¯†åˆ«é˜¶æ®µ
- [ ] ç¡®è®¤æ€§èƒ½é—®é¢˜çš„å…·ä½“è¡¨ç°
- [ ] æ”¶é›†ç›¸å…³æ€§èƒ½æŒ‡æ ‡æ•°æ®
- [ ] ç¡®å®šé—®é¢˜çš„å½±å“èŒƒå›´å’Œä¸¥é‡ç¨‹åº¦
- [ ] å»ºç«‹æ€§èƒ½åŸºçº¿ç”¨äºå¯¹æ¯”

## 2. æ ¹å› åˆ†æé˜¶æ®µ
- [ ] ä½¿ç”¨EXPLAINåˆ†ææ‰§è¡Œè®¡åˆ’
- [ ] æ£€æŸ¥ç´¢å¼•ä½¿ç”¨æƒ…å†µ
- [ ] åˆ†ææŸ¥è¯¢æ¨¡å¼å’Œé¢‘ç‡
- [ ] è¯„ä¼°ç³»ç»Ÿèµ„æºé…ç½®
- [ ] è¯†åˆ«é”ç­‰å¾…å’Œé˜»å¡æƒ…å†µ

## 3. ä¼˜åŒ–æ–¹æ¡ˆåˆ¶å®š
- [ ] ç¡®å®šä¼˜åŒ–ä¼˜å…ˆçº§
- [ ] åˆ¶å®šå…·ä½“çš„ä¼˜åŒ–æªæ–½
- [ ] è¯„ä¼°ä¼˜åŒ–æ–¹æ¡ˆçš„é£é™©
- [ ] åˆ¶å®šå›æ»šè®¡åˆ’
- [ ] å‡†å¤‡æµ‹è¯•ç¯å¢ƒéªŒè¯

## 4. å®æ–½ä¼˜åŒ–é˜¶æ®µ
- [ ] åœ¨æµ‹è¯•ç¯å¢ƒéªŒè¯ä¼˜åŒ–æ•ˆæœ
- [ ] åˆ¶å®šç”Ÿäº§ç¯å¢ƒå®æ–½è®¡åˆ’
- [ ] é€‰æ‹©åˆé€‚çš„æ—¶é—´çª—å£
- [ ] åˆ†æ­¥éª¤å®æ–½ä¼˜åŒ–æªæ–½
- [ ] å®æ—¶ç›‘æ§å®æ–½è¿‡ç¨‹

## 5. æ•ˆæœéªŒè¯é˜¶æ®µ
- [ ] å¯¹æ¯”ä¼˜åŒ–å‰åçš„æ€§èƒ½æŒ‡æ ‡
- [ ] éªŒè¯ä¸šåŠ¡åŠŸèƒ½æ­£å¸¸æ€§
- [ ] ç›‘æ§ç³»ç»Ÿç¨³å®šæ€§å’Œèµ„æºä½¿ç”¨
- [ ] æ”¶é›†ç”¨æˆ·åé¦ˆ
- [ ] è¯„ä¼°ROIå’Œä¼˜åŒ–æ”¶ç›Š

## 6. æŒç»­æ”¹è¿›é˜¶æ®µ
- [ ] å»ºç«‹å¸¸æ€åŒ–ç›‘æ§æœºåˆ¶
- [ ] å®šæœŸæ€§èƒ½è¯„ä¼°å’Œä¼˜åŒ–
- [ ] å®Œå–„æ€§èƒ½ä¼˜åŒ–æ–‡æ¡£
- [ ] åŸ¹è®­å›¢é˜Ÿæˆå‘˜
- [ ] å»ºç«‹æ€§èƒ½ä¼˜åŒ–çŸ¥è¯†åº“
```

---