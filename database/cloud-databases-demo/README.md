# äº‘æ•°æ®åº“æœåŠ¡ä½¿ç”¨å®Œæ•´æŒ‡å—

## ğŸ¯ æ¦‚è¿°

äº‘æ•°æ®åº“æœåŠ¡ä½œä¸ºç°ä»£åº”ç”¨æ¶æ„çš„æ ¸å¿ƒç»„ä»¶ï¼Œæä¾›äº†æ‰˜ç®¡çš„ã€å¯æ‰©å±•çš„æ•°æ®åº“è§£å†³æ–¹æ¡ˆã€‚æœ¬æŒ‡å—æ·±å…¥è§£æAWS RDSã€Azure Databaseå’ŒGoogle Cloud SQLç­‰ä¸»æµäº‘æ•°æ®åº“æœåŠ¡ï¼Œå¸®åŠ©ä¼ä¸šé€‰æ‹©æœ€é€‚åˆçš„äº‘æ•°æ®åº“æ–¹æ¡ˆå¹¶å®ç°é«˜æ•ˆè¿ç»´ã€‚

## ğŸ“‹ ç›®å½•

1. [äº‘æ•°æ®åº“æœåŠ¡æ¦‚è§ˆ](#1-äº‘æ•°æ®åº“æœåŠ¡æ¦‚è§ˆ)
2. [AWS RDSæ·±åº¦å®è·µ](#2-aws-rdsæ·±åº¦å®è·µ)
3. [Azure DatabaseæœåŠ¡](#3-azure-databaseæœåŠ¡)
4. [Google Cloud SQLè¯¦è§£](#4-google-cloud-sqlè¯¦è§£)
5. [å¤šäº‘æ•°æ®åº“ç­–ç•¥](#5-å¤šäº‘æ•°æ®åº“ç­–ç•¥)
6. [æˆæœ¬ä¼˜åŒ–ä¸æ€§èƒ½è°ƒä¼˜](#6-æˆæœ¬ä¼˜åŒ–ä¸æ€§èƒ½è°ƒä¼˜)

---

## 1. äº‘æ•°æ®åº“æœåŠ¡æ¦‚è§ˆ

### 1.1 ä¸»æµäº‘æ•°æ®åº“æœåŠ¡å¯¹æ¯”

#### äº‘æ•°æ®åº“æœåŠ¡å…¨æ™¯å›¾
```mermaid
graph TD
    A[äº‘æ•°æ®åº“æœåŠ¡] --> B[AWSç”Ÿæ€]
    A --> C[Azureç”Ÿæ€]
    A --> D[GCPç”Ÿæ€]
    
    B --> B1[RDS for MySQL]
    B --> B2[RDS for PostgreSQL]
    B --> B3[RDS for MariaDB]
    B --> B4[Aurora MySQL]
    B --> B5[Aurora PostgreSQL]
    B --> B6[DynamoDB]
    B --> B7[DocumentDB]
    B --> B8[ElastiCache]
    B --> B9[Timestream]
    
    C --> C1[Azure Database for MySQL]
    C --> C2[Azure Database for PostgreSQL]
    C --> C3[Azure SQL Database]
    C --> C4[Cosmos DB]
    C --> C5[Azure Cache for Redis]
    C --> C6[Azure Database for MariaDB]
    C --> C7[Synapse Analytics]
    
    D --> D1[Cloud SQL for MySQL]
    D --> D2[Cloud SQL for PostgreSQL]
    D --> D3[Cloud SQL for SQL Server]
    D --> D4[Cloud Spanner]
    D --> D5[Firestore]
    D --> D6[Bigtable]
    D --> D7[Memorystore]
    D --> D8[AlloyDB]
    
    subgraph "å…³ç³»å‹æ•°æ®åº“"
        B1 --> R1[MySQLå…¼å®¹]
        B2 --> R1
        B3 --> R1
        C1 --> R1
        C2 --> R1
        D1 --> R1
        D2 --> R1
    end
    
    subgraph "NoSQLæ•°æ®åº“"
        B6 --> N1[æ–‡æ¡£æ•°æ®åº“]
        B7 --> N1
        C4 --> N1
        D5 --> N1
        B9 --> N2[æ—¶åºæ•°æ®åº“]
        D6 --> N3[å®½åˆ—æ•°æ®åº“]
    end
    
    subgraph "ç¼“å­˜æœåŠ¡"
        B8 --> CA1[Rediså…¼å®¹]
        C5 --> CA1
        D7 --> CA1
    end
```

#### äº‘æ•°æ®åº“æœåŠ¡ç‰¹æ€§å¯¹æ¯”çŸ©é˜µ
```python
# äº‘æ•°æ®åº“æœåŠ¡å¯¹æ¯”åˆ†æç³»ç»Ÿ
class CloudDatabaseComparison:
    def __init__(self):
        self.services = {
            'aws_rds_mysql': {
                'provider': 'AWS',
                'service_name': 'RDS for MySQL',
                'type': 'å…³ç³»å‹',
                'engine_version': '8.0.35',
                'pricing_model': ['æŒ‰éœ€', 'é¢„ç•™å®ä¾‹', 'èŠ‚çœè®¡åˆ’'],
                'features': {
                    'automatic_backup': True,
                    'point_in_time_recovery': True,
                    'multi_az': True,
                    'read_replicas': 5,
                    'storage_auto_scaling': True,
                    'encryption_at_rest': True,
                    'iam_integration': True
                },
                'performance': {
                    'max_connections': 65535,
                    'max_storage': '64TB',
                    'max_iops': 16000,
                    'latency': '< 10ms'
                },
                'pricing': {
                    'small_instance': '$0.11/hour',
                    'medium_instance': '$0.44/hour',
                    'large_instance': '$1.76/hour',
                    'storage_per_gb': '$0.10/month'
                }
            },
            
            'azure_mysql': {
                'provider': 'Azure',
                'service_name': 'Azure Database for MySQL',
                'type': 'å…³ç³»å‹',
                'engine_version': '8.0.21',
                'pricing_model': ['æŒ‰éœ€', 'é¢„ç•™å®ä¾‹'],
                'features': {
                    'automatic_backup': True,
                    'point_in_time_recovery': True,
                    'geo_restore': True,
                    'read_replicas': 5,
                    'storage_auto_scaling': True,
                    'encryption_at_rest': True,
                    'aad_integration': True
                },
                'performance': {
                    'max_connections': 8000,
                    'max_storage': '16TB',
                    'max_iops': 20000,
                    'latency': '< 5ms'
                },
                'pricing': {
                    'small_instance': '$0.12/hour',
                    'medium_instance': '$0.48/hour',
                    'large_instance': '$1.92/hour',
                    'storage_per_gb': '$0.115/month'
                }
            },
            
            'gcp_cloudsql_mysql': {
                'provider': 'GCP',
                'service_name': 'Cloud SQL for MySQL',
                'type': 'å…³ç³»å‹',
                'engine_version': '8.0.26',
                'pricing_model': ['æŒ‰éœ€', 'æ‰¿è¯ºä½¿ç”¨æŠ˜æ‰£'],
                'features': {
                    'automatic_backup': True,
                    'point_in_time_recovery': True,
                    'cross_region_replication': True,
                    'read_replicas': 10,
                    'storage_auto_scaling': True,
                    'encryption_at_rest': True,
                    'iam_integration': True
                },
                'performance': {
                    'max_connections': 4000,
                    'max_storage': '30TB',
                    'max_iops': 25000,
                    'latency': '< 3ms'
                },
                'pricing': {
                    'small_instance': '$0.10/hour',
                    'medium_instance': '$0.40/hour',
                    'large_instance': '$1.60/hour',
                    'storage_per_gb': '$0.17/month'
                }
            },
            
            'aws_aurora': {
                'provider': 'AWS',
                'service_name': 'Amazon Aurora',
                'type': 'å…³ç³»å‹',
                'engine_version': 'MySQL 8.0å…¼å®¹',
                'pricing_model': ['æŒ‰éœ€', 'é¢„ç•™å®ä¾‹'],
                'features': {
                    'automatic_backup': True,
                    'point_in_time_recovery': True,
                    'multi_master': True,
                    'global_database': True,
                    'serverless': True,
                    'parallel_query': True
                },
                'performance': {
                    'max_connections': 300000,
                    'max_storage': '128TB',
                    'max_throughput': '15GB/s',
                    'latency': '< 5ms'
                },
                'pricing': {
                    'small_instance': '$0.20/hour',
                    'medium_instance': '$0.80/hour',
                    'large_instance': '$3.20/hour',
                    'io_per_request': '$0.20/million'
                }
            }
        }
    
    def compare_services(self, criteria: list, providers: list = None):
        """æŒ‰æŒ‡å®šæ ‡å‡†æ¯”è¾ƒäº‘æ•°æ®åº“æœåŠ¡"""
        if providers is None:
            providers = list(self.services.keys())
        
        comparison_results = {}
        
        for provider in providers:
            if provider in self.services:
                service = self.services[provider]
                scores = {}
                
                for criterion in criteria:
                    if criterion == 'cost_effectiveness':
                        # æˆæœ¬æ•ˆç›Šè¯„åˆ† (ä»·æ ¼è¶Šä½è¶Šå¥½)
                        base_price = float(service['pricing']['medium_instance'].replace('$', '').replace('/hour', ''))
                        scores[criterion] = max(0, 100 - (base_price * 10))  # æ ‡å‡†åŒ–åˆ°0-100
                    
                    elif criterion == 'performance':
                        # æ€§èƒ½è¯„åˆ†
                        perf_score = (
                            service['performance']['max_connections'] / 1000 +
                            service['performance']['max_iops'] / 1000 +
                            (30 - float(service['performance']['latency'].replace('< ', '').replace('ms', ''))) * 2
                        )
                        scores[criterion] = min(100, perf_score)
                    
                    elif criterion == 'feature_completeness':
                        # åŠŸèƒ½å®Œæ•´æ€§è¯„åˆ†
                        features = service['features']
                        feature_score = sum([
                            features['automatic_backup'],
                            features['point_in_time_recovery'],
                            features['multi_az'] if 'multi_az' in features else 0,
                            features['read_replicas'] / 5,
                            features['storage_auto_scaling'],
                            features['encryption_at_rest'],
                            features['iam_integration'] if 'iam_integration' in features else 0
                        ]) * 15
                        scores[criterion] = min(100, feature_score)
                    
                    elif criterion == 'scalability':
                        # å¯æ‰©å±•æ€§è¯„åˆ†
                        max_storage = service['performance']['max_storage']
                        storage_tb = float(max_storage.replace('TB', '').replace('GB', '')) 
                        if 'GB' in max_storage:
                            storage_tb /= 1024
                        
                        scalability_score = (
                            storage_tb / 10 +  # å­˜å‚¨æ‰©å±•èƒ½åŠ›
                            service['features']['read_replicas'] +  # è¯»å‰¯æœ¬æ•°é‡
                            (10 if service['features'].get('storage_auto_scaling', False) else 0)  # è‡ªåŠ¨æ‰©å±•
                        )
                        scores[criterion] = min(100, scalability_score * 3)
                
                comparison_results[provider] = {
                    'service_info': {
                        'provider': service['provider'],
                        'service_name': service['service_name'],
                        'type': service['type']
                    },
                    'scores': scores,
                    'total_score': sum(scores.values()) / len(criteria)
                }
        
        return comparison_results
    
    def recommend_service(self, workload_profile: dict, budget_limit: float = None):
        """åŸºäºå·¥ä½œè´Ÿè½½ç‰¹å¾æ¨èæœåŠ¡"""
        # å·¥ä½œè´Ÿè½½æƒé‡å®šä¹‰
        workload_weights = {
            'oltp_transactional': {
                'performance': 0.35,
                'feature_completeness': 0.25,
                'scalability': 0.25,
                'cost_effectiveness': 0.15
            },
            'olap_analytical': {
                'performance': 0.30,
                'scalability': 0.30,
                'feature_completeness': 0.25,
                'cost_effectiveness': 0.15
            },
            'mixed_workload': {
                'performance': 0.30,
                'scalability': 0.25,
                'feature_completeness': 0.25,
                'cost_effectiveness': 0.20
            },
            'development_testing': {
                'cost_effectiveness': 0.40,
                'feature_completeness': 0.30,
                'performance': 0.20,
                'scalability': 0.10
            }
        }
        
        # ç¡®å®šå·¥ä½œè´Ÿè½½ç±»å‹
        workload_type = workload_profile.get('type', 'mixed_workload')
        weights = workload_weights.get(workload_type, workload_weights['mixed_workload'])
        
        # è·å–æ¯”è¾ƒç»“æœ
        criteria = list(weights.keys())
        comparison_results = self.compare_services(criteria)
        
        # åº”ç”¨æƒé‡è®¡ç®—åŠ æƒåˆ†æ•°
        weighted_results = {}
        for provider, result in comparison_results.items():
            weighted_score = 0
            for criterion, weight in weights.items():
                if criterion in result['scores']:
                    weighted_score += result['scores'][criterion] * weight
            
            # å¦‚æœæœ‰é¢„ç®—é™åˆ¶ï¼Œè¿‡æ»¤è¶…å‡ºé¢„ç®—çš„æœåŠ¡
            if budget_limit:
                medium_price = float(self.services[provider]['pricing']['medium_instance'].replace('$', '').replace('/hour', ''))
                if medium_price * 24 * 30 > budget_limit:  # æœˆè´¹ç”¨ä¼°ç®—
                    continue
            
            weighted_results[provider] = {
                'service_info': result['service_info'],
                'weighted_score': weighted_score,
                'detailed_scores': result['scores']
            }
        
        # æ’åºå¹¶è¿”å›æ¨èç»“æœ
        recommendations = sorted(weighted_results.items(), key=lambda x: x[1]['weighted_score'], reverse=True)
        
        return {
            'primary_recommendation': recommendations[0] if recommendations else None,
            'alternatives': recommendations[1:3] if len(recommendations) > 1 else [],
            'detailed_analysis': weighted_results
        }

# ä½¿ç”¨ç¤ºä¾‹
comparison_system = CloudDatabaseComparison()

# OLTPäº‹åŠ¡å‹å·¥ä½œè´Ÿè½½æ¨è
oltp_workload = {
    'type': 'oltp_transactional',
    'connections': 1000,
    'transactions_per_second': 5000,
    'data_size_gb': 500
}

recommendation = comparison_system.recommend_service(oltp_workload, budget_limit=2000)
print("ä¸»è¦æ¨è:", recommendation['primary_recommendation'])
print("å¤‡é€‰æ–¹æ¡ˆ:", recommendation['alternatives'])
```

### 1.2 äº‘æ•°æ®åº“é€‰å‹å†³ç­–æ¡†æ¶

#### é€‰å‹å†³ç­–çŸ©é˜µ
```yaml
# cloud-database-selection-framework.yaml
selection_framework:
  business_requirements:
    - requirement: æ•°æ®ä¸€è‡´æ€§è¦æ±‚
      importance: critical
      evaluation_criteria:
        - strong_consistency: éœ€è¦å¼ºä¸€è‡´æ€§ä¿è¯
        - eventual_consistency: å¯æ¥å—æœ€ç»ˆä¸€è‡´æ€§
        - mixed_consistency: æ··åˆä¸€è‡´æ€§éœ€æ±‚
    
    - requirement: æ€§èƒ½è¦æ±‚
      importance: high
      evaluation_criteria:
        - latency_sensitive: ä½å»¶è¿Ÿæ•æ„Ÿåº”ç”¨
        - throughput_focused: é«˜ååé‡éœ€æ±‚
        - balanced: å¹³è¡¡å»¶è¿Ÿå’Œååé‡
    
    - requirement: æ‰©å±•æ€§éœ€æ±‚
      importance: high
      evaluation_criteria:
        - vertical_scaling: å‚ç›´æ‰©å±•ä¸ºä¸»
        - horizontal_scaling: æ°´å¹³æ‰©å±•éœ€æ±‚
        - auto_scaling: è‡ªåŠ¨æ‰©å±•è¦æ±‚
    
    - requirement: å¯ç”¨æ€§è¦æ±‚
      importance: critical
      evaluation_criteria:
        - high_availability: 99.9%ä»¥ä¸Šå¯ç”¨æ€§
        - disaster_recovery: å®Œå–„çš„ç¾å¤‡æ–¹æ¡ˆ
        - multi_region: å¤šåŒºåŸŸéƒ¨ç½²éœ€æ±‚
  
  technical_requirements:
    - requirement: æ•°æ®åº“å¼•æ“å…¼å®¹æ€§
      importance: high
      evaluation_criteria:
        - mysql_compatible: MySQLå…¼å®¹æ€§
        - postgresql_compatible: PostgreSQLå…¼å®¹æ€§
        - sql_server_compatible: SQL Serverå…¼å®¹æ€§
        - nosql_support: NoSQLæ”¯æŒ
    
    - requirement: å¼€å‘è¿ç»´å‹å¥½æ€§
      importance: medium
      evaluation_criteria:
        - managed_service: å®Œå…¨æ‰˜ç®¡æœåŠ¡
        - self_managed: éœ€è¦è‡ªç®¡ç†èƒ½åŠ›
        - migration_tools: è¿ç§»å·¥å…·æ”¯æŒ
        - monitoring_integration: ç›‘æ§é›†æˆèƒ½åŠ›
    
    - requirement: å®‰å…¨åˆè§„è¦æ±‚
      importance: critical
      evaluation_criteria:
        - encryption_at_rest: é™æ€æ•°æ®åŠ å¯†
        - encryption_in_transit: ä¼ è¾“åŠ å¯†
        - iam_integration: èº«ä»½è®¤è¯é›†æˆ
        - compliance_certifications: åˆè§„è®¤è¯æ”¯æŒ
  
  financial_requirements:
    - requirement: æˆæœ¬æ§åˆ¶
      importance: high
      evaluation_criteria:
        - predictable_pricing: å¯é¢„æµ‹å®šä»·
        - pay_as_you_go: æŒ‰éœ€ä»˜è´¹æ¨¡å¼
        - reserved_instances: é¢„ç•™å®ä¾‹ä¼˜æƒ 
        - spot_instances: ç«ä»·å®ä¾‹æ”¯æŒ

decision_matrix:
  scoring_system:
    critical: 30
    high: 20
    medium: 10
    low: 5
  
  provider_evaluation:
    aws:
      strengths:
        - comprehensive_service_portfolio
        - mature_ecosystem
        - excellent_performance
        - strong_enterprise_features
      weaknesses:
        - pricing_complexity
        - steep_learning_curve
        - vendor_lock_in_concerns
      
    azure:
      strengths:
        - enterprise_integration
        - hybrid_cloud_capabilities
        - strong_compliance_support
        - good_net_developer_experience
      weaknesses:
        - limited_service_availability
        - higher_pricing_than_competitors
        - smaller_community
      
    gcp:
      strengths:
        - superior_performance
        - innovative_features
        - competitive_pricing
        - strong_analytics_integration
      weaknesses:
        - smaller_ecosystem
        - limited_enterprise_features
        - regional_availability_gaps
```

## 2. AWS RDSæ·±åº¦å®è·µ

### 2.1 RDSåŸºç¡€æ¶æ„ä¸éƒ¨ç½²

#### RDSæ¶æ„è®¾è®¡
```mermaid
graph LR
    A[åº”ç”¨å±‚] --> B[è¿æ¥æ± /ä»£ç†]
    B --> C[RDSä¸»å®ä¾‹]
    C --> D[Multi-AZå¤‡ç”¨å®ä¾‹]
    
    C --> E[åªè¯»å‰¯æœ¬1]
    C --> F[åªè¯»å‰¯æœ¬2]
    C --> G[åªè¯»å‰¯æœ¬3]
    
    subgraph "å­˜å‚¨å±‚"
        H[S3å¤‡ä»½å­˜å‚¨]
        I[PITRæ—¥å¿—å­˜å‚¨]
    end
    
    subgraph "ç›‘æ§å‘Šè­¦"
        J[CloudWatch]
        K[RDSå¢å¼ºç›‘æ§]
    end
    
    C --> H
    C --> I
    J --> C
    K --> C
    J --> E
    J --> F
    J --> G
```

#### RDSéƒ¨ç½²é…ç½®æ¨¡æ¿
```yaml
# rds-deployment-template.yaml
AWSTemplateFormatVersion: '2010-09-09'
Description: 'Production RDS MySQL Database Template'

Parameters:
  DBInstanceIdentifier:
    Type: String
    Default: production-mysql
    Description: æ•°æ®åº“å®ä¾‹æ ‡è¯†ç¬¦
  
  DBName:
    Type: String
    Default: production_db
    Description: æ•°æ®åº“åç§°
  
  DBUsername:
    Type: String
    Default: admin
    Description: æ•°æ®åº“ç”¨æˆ·å
  
  DBPassword:
    Type: String
    NoEcho: true
    Description: æ•°æ®åº“å¯†ç 
  
  DBInstanceClass:
    Type: String
    Default: db.r6g.large
    AllowedValues:
      - db.t4g.micro
      - db.t4g.small
      - db.r6g.large
      - db.r6g.xlarge
      - db.r6g.2xlarge
    Description: æ•°æ®åº“å®ä¾‹ç±»å‹
  
  AllocatedStorage:
    Type: Number
    Default: 100
    Description: åˆ†é…çš„å­˜å‚¨ç©ºé—´(GB)
  
  VPCId:
    Type: AWS::EC2::VPC::Id
    Description: VPC ID
  
  SubnetIds:
    Type: List<AWS::EC2::Subnet::Id>
    Description: å­ç½‘IDs

Resources:
  # æ•°æ®åº“å­ç½‘ç»„
  DBSubnetGroup:
    Type: AWS::RDS::DBSubnetGroup
    Properties:
      DBSubnetGroupDescription: Subnet group for RDS instances
      SubnetIds: !Ref SubnetIds
      Tags:
        - Key: Name
          Value: !Sub '${AWS::StackName}-subnet-group'

  # å®‰å…¨ç»„
  DatabaseSecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: Security group for RDS database
      VpcId: !Ref VPCId
      SecurityGroupIngress:
        - IpProtocol: tcp
          FromPort: 3306
          ToPort: 3306
          SourceSecurityGroupId: !Ref ApplicationSecurityGroup
      Tags:
        - Key: Name
          Value: !Sub '${AWS::StackName}-db-sg'

  # RDSå‚æ•°ç»„
  DBParameterGroup:
    Type: AWS::RDS::DBParameterGroup
    Properties:
      Family: mysql8.0
      Description: Parameter group for production MySQL
      Parameters:
        max_connections: '2000'
        innodb_buffer_pool_size: '{DBInstanceClassMemory*3/4}'
        slow_query_log: '1'
        long_query_time: '2'
        log_queries_not_using_indexes: '1'
      Tags:
        - Key: Name
          Value: !Sub '${AWS::StackName}-param-group'

  # é€‰é¡¹ç»„
  DBOptionGroup:
    Type: AWS::RDS::OptionGroup
    Properties:
      EngineName: mysql
      MajorEngineVersion: '8.0'
      OptionGroupDescription: Option group for MySQL with monitoring
      Options:
        - OptionName: MARIADB_AUDIT_PLUGIN
        - OptionName: MYSQL_BACKUP_RESTORE

  # ä¸»æ•°æ®åº“å®ä¾‹
  DBInstance:
    Type: AWS::RDS::DBInstance
    Properties:
      DBInstanceIdentifier: !Ref DBInstanceIdentifier
      DBName: !Ref DBName
      DBInstanceClass: !Ref DBInstanceClass
      AllocatedStorage: !Ref AllocatedStorage
      Engine: mysql
      EngineVersion: '8.0.35'
      MasterUsername: !Ref DBUsername
      MasterUserPassword: !Ref DBPassword
      StorageType: gp3
      StorageEncrypted: true
      KmsKeyId: !Ref DatabaseKMSKey
      BackupRetentionPeriod: 7
      MultiAZ: true
      AutoMinorVersionUpgrade: true
      DeletionProtection: true
      CopyTagsToSnapshot: true
      EnablePerformanceInsights: true
      PerformanceInsightsRetentionPeriod: 7
      EnableCloudwatchLogsExports:
        - error
        - general
        - slowquery
      DBSubnetGroupName: !Ref DBSubnetGroup
      VPCSecurityGroups:
        - !Ref DatabaseSecurityGroup
      DBParameterGroupName: !Ref DBParameterGroup
      OptionGroupName: !Ref DBOptionGroup
      Tags:
        - Key: Environment
          Value: Production
        - Key: Application
          Value: !Ref AWS::StackName

  # åªè¯»å‰¯æœ¬
  ReadOnlyReplica:
    Type: AWS::RDS::DBInstance
    Properties:
      SourceDBInstanceIdentifier: !Ref DBInstance
      DBInstanceClass: !Ref DBInstanceClass
      PubliclyAccessible: false
      Tags:
        - Key: Name
          Value: !Sub '${DBInstanceIdentifier}-readonly'

Outputs:
  DBEndpoint:
    Description: Database endpoint
    Value: !GetAtt DBInstance.Endpoint.Address
    Export:
      Name: !Sub '${AWS::StackName}-DBEndpoint'
  
  DBPort:
    Description: Database port
    Value: !GetAtt DBInstance.Endpoint.Port
    Export:
      Name: !Sub '${AWS::StackName}-DBPort'
  
  DBResourceId:
    Description: Database resource ID
    Value: !Ref DBInstance
    Export:
      Name: !Sub '${AWS::StackName}-DBResourceId'
```

### 2.2 RDSé«˜çº§é…ç½®ä¸ä¼˜åŒ–

#### æ€§èƒ½ä¼˜åŒ–é…ç½®
```python
# rds_performance_optimizer.py
import boto3
import json
from typing import Dict, List, Any

class RDSPerformanceOptimizer:
    def __init__(self, region: str = 'us-east-1'):
        self.rds_client = boto3.client('rds', region_name=region)
        self.cloudwatch_client = boto3.client('cloudwatch', region_name=region)
    
    def analyze_instance_performance(self, db_instance_identifier: str) -> Dict[str, Any]:
        """åˆ†æRDSå®ä¾‹æ€§èƒ½"""
        # è·å–å®ä¾‹ä¿¡æ¯
        response = self.rds_client.describe_db_instances(
            DBInstanceIdentifier=db_instance_identifier
        )
        instance_info = response['DBInstances'][0]
        
        # æ”¶é›†CloudWatchæŒ‡æ ‡
        metrics = self._collect_performance_metrics(db_instance_identifier)
        
        # åˆ†ææ€§èƒ½ç“¶é¢ˆ
        bottlenecks = self._identify_bottlenecks(metrics, instance_info)
        
        return {
            'instance_info': instance_info,
            'performance_metrics': metrics,
            'bottlenecks': bottlenecks,
            'optimization_recommendations': self._generate_recommendations(bottlenecks, instance_info)
        }
    
    def _collect_performance_metrics(self, db_instance_identifier: str) -> Dict[str, List[float]]:
        """æ”¶é›†æ€§èƒ½æŒ‡æ ‡"""
        metric_names = [
            'CPUUtilization',
            'DatabaseConnections',
            'FreeableMemory',
            'ReadIOPS',
            'WriteIOPS',
            'ReadLatency',
            'WriteLatency',
            'ReadThroughput',
            'WriteThroughput'
        ]
        
        metrics_data = {}
        
        for metric_name in metric_names:
            response = self.cloudwatch_client.get_metric_statistics(
                Namespace='AWS/RDS',
                MetricName=metric_name,
                Dimensions=[
                    {
                        'Name': 'DBInstanceIdentifier',
                        'Value': db_instance_identifier
                    }
                ],
                StartTime=datetime.utcnow() - timedelta(hours=1),
                EndTime=datetime.utcnow(),
                Period=300,  # 5åˆ†é’Ÿé—´éš”
                Statistics=['Average', 'Maximum']
            )
            
            metrics_data[metric_name] = [
                datapoint['Average'] for datapoint in response['Datapoints']
            ]
        
        return metrics_data
    
    def _identify_bottlenecks(self, metrics: Dict[str, List[float]], instance_info: Dict[str, Any]) -> List[Dict[str, Any]]:
        """è¯†åˆ«æ€§èƒ½ç“¶é¢ˆ"""
        bottlenecks = []
        
        # CPUç“¶é¢ˆæ£€æµ‹
        avg_cpu = sum(metrics.get('CPUUtilization', [])) / len(metrics.get('CPUUtilization', [1]))
        if avg_cpu > 70:
            bottlenecks.append({
                'type': 'cpu',
                'severity': 'high' if avg_cpu > 85 else 'medium',
                'current_value': avg_cpu,
                'threshold': 70,
                'description': f'CPUä½¿ç”¨ç‡è¿‡é«˜: {avg_cpu:.1f}%'
            })
        
        # å†…å­˜ç“¶é¢ˆæ£€æµ‹
        free_memory_mb = sum(metrics.get('FreeableMemory', [])) / len(metrics.get('FreeableMemory', [1])) / (1024 * 1024)
        total_memory_gb = instance_info.get('DBInstanceClassMemory', 0) / (1024 * 1024 * 1024)
        memory_utilization = (total_memory_gb - free_memory_mb / 1024) / total_memory_gb * 100
        
        if memory_utilization > 80:
            bottlenecks.append({
                'type': 'memory',
                'severity': 'high' if memory_utilization > 90 else 'medium',
                'current_value': memory_utilization,
                'threshold': 80,
                'description': f'å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜: {memory_utilization:.1f}%'
            })
        
        # I/Oç“¶é¢ˆæ£€æµ‹
        avg_read_iops = sum(metrics.get('ReadIOPS', [])) / len(metrics.get('ReadIOPS', [1]))
        avg_write_iops = sum(metrics.get('WriteIOPS', [])) / len(metrics.get('WriteIOPS', [1]))
        max_iops = self._get_max_iops(instance_info['DBInstanceClass'])
        
        total_iops_utilization = (avg_read_iops + avg_write_iops) / max_iops * 100
        if total_iops_utilization > 70:
            bottlenecks.append({
                'type': 'io',
                'severity': 'high' if total_iops_utilization > 85 else 'medium',
                'current_value': total_iops_utilization,
                'threshold': 70,
                'description': f'I/Oä½¿ç”¨ç‡è¿‡é«˜: {total_iops_utilization:.1f}%'
            })
        
        return bottlenecks
    
    def _get_max_iops(self, instance_class: str) -> int:
        """è·å–å®ä¾‹ç±»å‹çš„æœ€å¤§IOPS"""
        iops_map = {
            'db.t4g.micro': 3000,
            'db.t4g.small': 3000,
            'db.r6g.large': 16000,
            'db.r6g.xlarge': 16000,
            'db.r6g.2xlarge': 16000,
            'db.r6g.4xlarge': 16000
        }
        return iops_map.get(instance_class, 3000)
    
    def _generate_recommendations(self, bottlenecks: List[Dict[str, Any]], instance_info: Dict[str, Any]) -> List[Dict[str, str]]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []
        
        for bottleneck in bottlenecks:
            if bottleneck['type'] == 'cpu':
                recommendations.append({
                    'category': 'instance_scaling',
                    'action': 'å‡çº§å®ä¾‹è§„æ ¼',
                    'details': f'å½“å‰CPUä½¿ç”¨ç‡{bottleneck["current_value"]:.1f}%ï¼Œå»ºè®®å‡çº§åˆ°æ›´é«˜è§„æ ¼çš„å®ä¾‹'
                })
            
            elif bottleneck['type'] == 'memory':
                recommendations.append({
                    'category': 'memory_optimization',
                    'action': 'è°ƒæ•´ç¼“å†²æ± å¤§å°',
                    'details': f'ä¼˜åŒ–InnoDBç¼“å†²æ± é…ç½®ï¼Œå½“å‰å†…å­˜ä½¿ç”¨ç‡{bottleneck["current_value"]:.1f}%'
                })
            
            elif bottleneck['type'] == 'io':
                if 'gp3' in instance_info.get('StorageType', ''):
                    recommendations.append({
                        'category': 'storage_optimization',
                        'action': 'å¢åŠ å­˜å‚¨IOPS',
                        'details': f'å½“å‰I/Oä½¿ç”¨ç‡{bottleneck["current_value"]:.1f}%ï¼Œå»ºè®®å¢åŠ GP3å­˜å‚¨çš„IOPSé…ç½®'
                    })
                else:
                    recommendations.append({
                        'category': 'storage_upgrade',
                        'action': 'å‡çº§å­˜å‚¨ç±»å‹',
                        'details': f'å½“å‰I/Oä½¿ç”¨ç‡{bottleneck["current_value"]:.1f}%ï¼Œå»ºè®®å‡çº§åˆ°GP3å­˜å‚¨ç±»å‹'
                    })
        
        return recommendations
    
    def apply_optimizations(self, db_instance_identifier: str, optimizations: List[Dict[str, str]]) -> bool:
        """åº”ç”¨ä¼˜åŒ–é…ç½®"""
        try:
            for optimization in optimizations:
                if optimization['category'] == 'instance_scaling':
                    # å®ä¾‹è§„æ ¼å‡çº§é€»è¾‘
                    self._upgrade_instance_class(db_instance_identifier)
                
                elif optimization['category'] == 'memory_optimization':
                    # å†…å­˜ä¼˜åŒ–é€»è¾‘
                    self._optimize_memory_parameters(db_instance_identifier)
                
                elif optimization['category'] == 'storage_optimization':
                    # å­˜å‚¨ä¼˜åŒ–é€»è¾‘
                    self._optimize_storage_configuration(db_instance_identifier)
            
            return True
        except Exception as e:
            print(f"ä¼˜åŒ–åº”ç”¨å¤±è´¥: {str(e)}")
            return False
    
    def _upgrade_instance_class(self, db_instance_identifier: str):
        """å‡çº§å®ä¾‹è§„æ ¼"""
        # è·å–å½“å‰å®ä¾‹ä¿¡æ¯
        response = self.rds_client.describe_db_instances(
            DBInstanceIdentifier=db_instance_identifier
        )
        current_class = response['DBInstances'][0]['DBInstanceClass']
        
        # ç¡®å®šå‡çº§åçš„å®ä¾‹è§„æ ¼
        upgrade_map = {
            'db.t4g.micro': 'db.t4g.small',
            'db.t4g.small': 'db.r6g.large',
            'db.r6g.large': 'db.r6g.xlarge'
        }
        
        new_class = upgrade_map.get(current_class, current_class)
        if new_class != current_class:
            self.rds_client.modify_db_instance(
                DBInstanceIdentifier=db_instance_identifier,
                DBInstanceClass=new_class,
                ApplyImmediately=False  # åœ¨ç»´æŠ¤çª—å£æœŸé—´åº”ç”¨
            )
            print(f"å®ä¾‹è§„æ ¼ä» {current_class} å‡çº§åˆ° {new_class}")
    
    def _optimize_memory_parameters(self, db_instance_identifier: str):
        """ä¼˜åŒ–å†…å­˜ç›¸å…³å‚æ•°"""
        # ä¿®æ”¹å‚æ•°ç»„ä¸­çš„å†…å­˜ç›¸å…³å‚æ•°
        parameter_group_name = f"{db_instance_identifier}-optimized-params"
        
        # åˆ›å»ºæ–°çš„å‚æ•°ç»„ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        try:
            self.rds_client.create_db_parameter_group(
                DBParameterGroupName=parameter_group_name,
                DBParameterGroupFamily='mysql8.0',
                Description='Optimized parameters for memory usage'
            )
        except self.rds_client.exceptions.DBParameterGroupAlreadyExistsFault:
            pass  # å‚æ•°ç»„å·²å­˜åœ¨
        
        # ä¿®æ”¹å‚æ•°
        parameters_to_modify = [
            {
                'ParameterName': 'innodb_buffer_pool_size',
                'ParameterValue': '{DBInstanceClassMemory*3/4}',
                'ApplyMethod': 'pending-reboot'
            },
            {
                'ParameterName': 'innodb_log_buffer_size',
                'ParameterValue': '67108864',  # 64MB
                'ApplyMethod': 'pending-reboot'
            },
            {
                'ParameterName': 'key_buffer_size',
                'ParameterValue': '{DBInstanceClassMemory*1/8}',
                'ApplyMethod': 'immediate'
            }
        ]
        
        self.rds_client.modify_db_parameter_group(
            DBParameterGroupName=parameter_group_name,
            Parameters=parameters_to_modify
        )
        
        # å°†å‚æ•°ç»„åº”ç”¨åˆ°å®ä¾‹
        self.rds_client.modify_db_instance(
            DBInstanceIdentifier=db_instance_identifier,
            DBParameterGroupName=parameter_group_name,
            ApplyImmediately=False
        )

# ä½¿ç”¨ç¤ºä¾‹
optimizer = RDSPerformanceOptimizer(region='us-east-1')
analysis_result = optimizer.analyze_instance_performance('production-mysql')

print("æ€§èƒ½åˆ†æç»“æœ:")
print(json.dumps(analysis_result, indent=2, ensure_ascii=False))

# åº”ç”¨ä¼˜åŒ–å»ºè®®
if analysis_result['optimization_recommendations']:
    optimizer.apply_optimizations(
        'production-mysql',
        analysis_result['optimization_recommendations']
    )
```

### 2.3 RDSå®‰å…¨ä¸åˆè§„é…ç½®

#### å®‰å…¨é…ç½®æœ€ä½³å®è·µ
```yaml
# rds-security-configuration.yaml
AWSTemplateFormatVersion: '2010-09-09'
Description: 'RDS Security Best Practices Configuration'

Parameters:
  Environment:
    Type: String
    Default: production
    AllowedValues:
      - development
      - staging
      - production
    Description: ç¯å¢ƒç±»å‹

Resources:
  # KMSå¯†é’¥ç”¨äºåŠ å¯†
  DatabaseKMSKey:
    Type: AWS::KMS::Key
    Properties:
      Description: KMS key for RDS database encryption
      KeyPolicy:
        Version: '2012-10-17'
        Statement:
          - Sid: Enable IAM User Permissions
            Effect: Allow
            Principal:
              AWS: !Sub 'arn:aws:iam::${AWS::AccountId}:root'
            Action: 'kms:*'
            Resource: '*'
          - Sid: Allow RDS service to use the key
            Effect: Allow
            Principal:
              Service: 'rds.amazonaws.com'
            Action:
              - 'kms:Encrypt'
              - 'kms:Decrypt'
              - 'kms:ReEncrypt*'
              - 'kms:GenerateDataKey*'
              - 'kms:DescribeKey'
            Resource: '*'
      EnableKeyRotation: true
      PendingWindowInDays: 30

  # æ•°æ®åº“æ´»åŠ¨æµé…ç½®
  DatabaseActivityStream:
    Type: AWS::RDS::DBClusterParameterGroup
    Properties:
      Description: Parameter group with audit logging enabled
      Family: aurora-mysql5.7
      Parameters:
        server_audit_logging: '1'
        server_audit_events: 'CONNECT,QUERY,TABLE'
        server_audit_excl_users: ''
        server_audit_incl_users: '*'
        log_output: 'FILE'
        general_log: '1'
        slow_query_log: '1'
        long_query_time: '2'

  # CloudTrailé…ç½®ç”¨äºå®¡è®¡
  DatabaseCloudTrail:
    Type: AWS::CloudTrail::Trail
    Properties:
      TrailName: !Sub '${AWS::StackName}-database-audit-trail'
      S3BucketName: !Ref AuditLogBucket
      IsLogging: true
      IncludeGlobalServiceEvents: true
      IsMultiRegionTrail: true
      EventSelectors:
        - ReadWriteType: All
          IncludeManagementEvents: true
          DataResources:
            - Type: AWS::RDS::DBInstance
              Values:
                - !Sub 'arn:aws:rds:${AWS::Region}:${AWS::AccountId}:db:*'
            - Type: AWS::RDS::DBCluster
              Values:
                - !Sub 'arn:aws:rds:${AWS::Region}:${AWS::AccountId}:cluster:*'

  # å®¡è®¡æ—¥å¿—å­˜å‚¨æ¡¶
  AuditLogBucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: !Sub '${AWS::StackName}-audit-logs-${AWS::AccountId}'
      VersioningConfiguration:
        Status: Enabled
      BucketEncryption:
        ServerSideEncryptionConfiguration:
          - ServerSideEncryptionByDefault:
              SSEAlgorithm: AES256
      PublicAccessBlockConfiguration:
        BlockPublicAcls: true
        BlockPublicPolicy: true
        IgnorePublicAcls: true
        RestrictPublicBuckets: true
      LifecycleConfiguration:
        Rules:
          - Id: ArchiveOldLogs
            Status: Enabled
            Transitions:
              - TransitionInDays: 90
                StorageClass: STANDARD_IA
              - TransitionInDays: 365
                StorageClass: GLACIER
            ExpirationInDays: 1825  # 5å¹´ä¿ç•™

  # IAMè§’è‰²å’Œç­–ç•¥
  DatabaseAdministratorRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub '${AWS::StackName}-db-admin-role'
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: 'rds.amazonaws.com'
            Action: 'sts:AssumeRole'
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/AmazonRDSFullAccess
      Policies:
        - PolicyName: DatabaseSecurityPolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - 'kms:Encrypt'
                  - 'kms:Decrypt'
                  - 'kms:ReEncrypt*'
                  - 'kms:GenerateDataKey*'
                  - 'kms:DescribeKey'
                Resource: !GetAtt DatabaseKMSKey.Arn
              - Effect: Allow
                Action:
                  - 's3:GetObject'
                  - 's3:PutObject'
                  - 's3:DeleteObject'
                Resource: !Sub '${AuditLogBucket}/*'

Outputs:
  KMSKeyArn:
    Description: Database encryption KMS key ARN
    Value: !GetAtt DatabaseKMSKey.Arn
  
  AuditBucketName:
    Description: Audit log S3 bucket name
    Value: !Ref AuditLogBucket
  
  AdminRoleArn:
    Description: Database administrator role ARN
    Value: !GetAtt DatabaseAdministratorRole.Arn
```

## 3. Azure DatabaseæœåŠ¡

### 3.1 Azure Database for MySQLé…ç½®

#### Azure MySQLéƒ¨ç½²é…ç½®
```json
{
  "$schema": "https://schema.management.azure.com/schemas/2019-04-01/deploymentTemplate.json#",
  "contentVersion": "1.0.0.0",
  "parameters": {
    "serverName": {
      "type": "string",
      "defaultValue": "[concat('mysql-', uniqueString(resourceGroup().id))]",
      "metadata": {
        "description": "æœåŠ¡å™¨åç§°"
      }
    },
    "administratorLogin": {
      "type": "string",
      "metadata": {
        "description": "ç®¡ç†å‘˜ç™»å½•å"
      }
    },
    "administratorLoginPassword": {
      "type": "securestring",
      "metadata": {
        "description": "ç®¡ç†å‘˜å¯†ç "
      }
    },
    "skuCapacity": {
      "type": "int",
      "defaultValue": 2,
      "metadata": {
        "description": "æ•°æ®åº“è®¡ç®—å•ä½"
      }
    },
    "skuName": {
      "type": "string",
      "defaultValue": "GP_Gen5_2",
      "metadata": {
        "description": "SKUåç§°"
      }
    },
    "skuSizeMB": {
      "type": "int",
      "defaultValue": 51200,
      "metadata": {
        "description": "å­˜å‚¨å¤§å°(MB)"
      }
    },
    "skuTier": {
      "type": "string",
      "defaultValue": "GeneralPurpose",
      "metadata": {
        "description": "SKUå±‚çº§"
      }
    },
    "version": {
      "type": "string",
      "defaultValue": "8.0",
      "metadata": {
        "description": "MySQLç‰ˆæœ¬"
      }
    },
    "location": {
      "type": "string",
      "defaultValue": "[resourceGroup().location]",
      "metadata": {
        "description": "ä½ç½®"
      }
    }
  },
  "variables": {
    "databaseName": "production_db"
  },
  "resources": [
    {
      "type": "Microsoft.DBforMySQL/servers",
      "apiVersion": "2017-12-01",
      "name": "[parameters('serverName')]",
      "location": "[parameters('location')]",
      "sku": {
        "name": "[parameters('skuName')]",
        "tier": "[parameters('skuTier')]",
        "capacity": "[parameters('skuCapacity')]"
      },
      "properties": {
        "version": "[parameters('version')]",
        "administratorLogin": "[parameters('administratorLogin')]",
        "administratorLoginpassword: "${DB_PASSWORD}"administratorLoginPassword')]",
        "storageProfile": {
          "storageMB": "[parameters('skuSizeMB')]",
          "backupRetentionDays": 7,
          "geoRedundantBackup": "Disabled",
          "storageAutogrow": "Enabled"
        }
      },
      "resources": [
        {
          "type": "firewallRules",
          "apiVersion": "2017-12-01",
          "name": "AllowAllWindowsAzureIps",
          "dependsOn": [
            "[resourceId('Microsoft.DBforMySQL/servers', parameters('serverName'))]"
          ],
          "properties": {
            "startIpAddress": "0.0.0.0",
            "endIpAddress": "0.0.0.0"
          }
        },
        {
          "type": "databases",
          "apiVersion": "2017-12-01",
          "name": "[variables('databaseName')]",
          "dependsOn": [
            "[resourceId('Microsoft.DBforMySQL/servers', parameters('serverName'))]"
          ],
          "properties": {}
        }
      ]
    }
  ],
  "outputs": {
    "fullyQualifiedDomainName": {
      "type": "string",
      "value": "[reference(resourceId('Microsoft.DBforMySQL/servers', parameters('serverName'))).fullyQualifiedDomainName]"
    }
  }
}
```

### 3.2 Azureå®‰å…¨é…ç½®
```powershell
# Azure Databaseå®‰å…¨é…ç½®è„šæœ¬
param(
    [string]$ResourceGroupName = "Production-RG",
    [string]$ServerName = "mysql-production",
    [string]$Location = "East US"
)

# ç™»å½•Azure
Connect-AzAccount

# åˆ›å»ºèµ„æºç»„
New-AzResourceGroup -Name $ResourceGroupName -Location $Location

# åˆ›å»ºMySQLæœåŠ¡å™¨
$Password = ConvertTo-SecureString "StrongPassword123!" -AsPlainText -Force
$Credential = New-Object System.Management.Automation.PSCredential ("adminuser", $Password)

New-AzMySqlServer -ResourceGroupName $ResourceGroupName -ServerName $ServerName -Location $Location -SqlAdministratorCredentials $Credential -SslEnforcement Enabled -MinimalTlsVersion TLSEnforcementDisabled -PublicNetworkAccess Disabled

# é…ç½®é˜²ç«å¢™è§„åˆ™
New-AzMySqlFirewallRule -ResourceGroupName $ResourceGroupName -ServerName $ServerName -FirewallRuleName "AllowAppServers" -StartIpAddress "10.0.1.0" -EndIpAddress "10.0.1.255"

# å¯ç”¨å®¡è®¡æ—¥å¿—
Update-AzMySqlServerConfiguration -ResourceGroupName $ResourceGroupName -ServerName $ServerName -Name audit_log_enabled -Value ON
Update-AzMySqlServerConfiguration -ResourceGroupName $ResourceGroupName -ServerName $ServerName -Name audit_log_events -Value "CONNECTION,DML,DDL"

# é…ç½®å¨èƒæ£€æµ‹
Set-AzMySqlThreatDetectionPolicy -ResourceGroupName $ResourceGroupName -ServerName $ServerName -EmailAdmins Enabled -NotificationRecipientsEmails "security@company.com"

Write-Host "Azure MySQLå®‰å…¨é…ç½®å®Œæˆ"
```

## 4. Google Cloud SQLè¯¦è§£

### 4.1 Cloud SQLåŸºç¡€é…ç½®

#### Terraformé…ç½®æ–‡ä»¶
```hcl
# main.tf - Google Cloud SQLé…ç½®
terraform {
  required_providers {
    google = {
      source  = "hashicorp/google"
      version = "~> 4.0"
    }
  }
}

provider "google" {
  project = var.project_id
  region  = var.region
}

# åˆ›å»ºCloud SQLå®ä¾‹
resource "google_sql_database_instance" "mysql_instance" {
  name             = var.instance_name
  database_version = "MYSQL_8_0"
  region           = var.region

  settings {
    tier = var.machine_type
    
    # å­˜å‚¨é…ç½®
    disk_size       = var.disk_size_gb
    disk_type       = "PD_SSD"
    disk_autoresize = true

    # å¤‡ä»½é…ç½®
    backup_configuration {
      enabled                        = true
      start_time                     = "02:00"
      point_in_time_recovery_enabled = true
      transaction_log_retention_days = 7
    }

    # é«˜å¯ç”¨é…ç½®
    availability_type = "REGIONAL"
    
    # ç»´æŠ¤çª—å£
    maintenance_window {
      day          = 7  # æ˜ŸæœŸæ—¥
      hour         = 2  # å‡Œæ™¨2ç‚¹
      update_track = "stable"
    }

    # IPé…ç½®
    ip_configuration {
      ipv4_enabled    = true
      private_network = var.vpc_network
      
      authorized_networks {
        name  = "application-servers"
        value = var.app_server_cidr
      }
    }

    # æ•°æ®åº“æ ‡å¿—
    database_flags {
      name  = "max_connections"
      value = "2000"
    }
    
    database_flags {
      name  = "innodb_buffer_pool_size"
      value = "2147483648"  # 2GB
    }
  }

  # åŠ å¯†é…ç½®
  encryption_key_name = google_kms_crypto_key.sql_key.id

  deletion_protection = true
}

# åˆ›å»ºæ•°æ®åº“
resource "google_sql_database" "main_database" {
  name     = var.database_name
  instance = google_sql_database_instance.mysql_instance.name
}

# åˆ›å»ºç”¨æˆ·
resource "google_sql_user" "app_user" {
  name     = var.db_username
  instance = google_sql_database_instance.mysql_instance.name
  password = var.db_password
}

# KMSå¯†é’¥ç”¨äºåŠ å¯†
resource "google_kms_key_ring" "sql_keyring" {
  name     = "${var.instance_name}-keyring"
  location = var.region
}

resource "google_kms_crypto_key" "sql_key" {
  name     = "${var.instance_name}-crypto-key"
  key_ring = google_kms_key_ring.sql_keyring.id
}
```

### 4.2 Cloud SQLç›‘æ§é…ç½®
```python
# cloud_sql_monitoring.py
from google.cloud import monitoring_v3
from google.cloud.sql import connector
import pymysql
import time

class CloudSQLMonitor:
    def __init__(self, project_id: str, instance_name: str):
        self.project_id = project_id
        self.instance_name = instance_name
        self.monitoring_client = monitoring_v3.MetricServiceClient()
        
    def collect_metrics(self) -> dict:
        """æ”¶é›†Cloud SQLæŒ‡æ ‡"""
        metrics = {}
        
        # CPUä½¿ç”¨ç‡
        cpu_query = f"""
        fetch cloudsql_database
        | metric 'cloudsql.googleapis.com/database/cpu/utilization'
        | filter resource.database_id == '{self.project_id}:{self.instance_name}'
        | within 1h
        | align rate(1m)
        """
        
        # è¿æ¥æ•°
        connections_query = f"""
        fetch cloudsql_database
        | metric 'cloudsql.googleapis.com/database/network/connections'
        | filter resource.database_id == '{self.project_id}:{self.instance_name}'
        | within 1h
        | align delta(1m)
        """
        
        # æŸ¥è¯¢å»¶è¿Ÿ
        latency_query = f"""
        fetch cloudsql_database
        | metric 'cloudsql.googleapis.com/database/query/latency'
        | filter resource.database_id == '{self.project_id}:{self.instance_name}'
        | within 1h
        | align delta(1m)
        """
        
        metrics['cpu_utilization'] = self._execute_query(cpu_query)
        metrics['connections'] = self._execute_query(connections_query)
        metrics['query_latency'] = self._execute_query(latency_query)
        
        return metrics
    
    def _execute_query(self, query: str) -> list:
        """æ‰§è¡Œç›‘æ§æŸ¥è¯¢"""
        request = monitoring_v3.QueryTimeSeriesRequest(
            name=f"projects/{self.project_id}",
            query=query
        )
        response = self.monitoring_client.query_time_series(request=request)
        return list(response)
    
    def check_performance_thresholds(self, metrics: dict) -> list:
        """æ£€æŸ¥æ€§èƒ½é˜ˆå€¼"""
        alerts = []
        
        # CPUä½¿ç”¨ç‡é˜ˆå€¼æ£€æŸ¥
        if metrics['cpu_utilization'] and max(metrics['cpu_utilization']) > 0.8:
            alerts.append({
                'type': 'HIGH_CPU',
                'severity': 'WARNING',
                'message': f'CPUä½¿ç”¨ç‡è¶…è¿‡80%: {max(metrics["cpu_utilization"]):.2%}'
            })
        
        # è¿æ¥æ•°é˜ˆå€¼æ£€æŸ¥
        if metrics['connections'] and max(metrics['connections']) > 1000:
            alerts.append({
                'type': 'HIGH_CONNECTIONS',
                'severity': 'WARNING',
                'message': f'è¿æ¥æ•°è¶…è¿‡1000: {max(metrics["connections"])}'
            })
        
        return alerts

# ä½¿ç”¨ç¤ºä¾‹
monitor = CloudSQLMonitor('my-project', 'mysql-instance-1')
metrics = monitor.collect_metrics()
alerts = monitor.check_performance_thresholds(metrics)

for alert in alerts:
    print(f"{alert['type']}: {alert['message']}")
```

## 5. å¤šäº‘æ•°æ®åº“ç­–ç•¥

### 5.1 å¤šäº‘æ¶æ„è®¾è®¡

#### å¤šäº‘æ•°æ®åº“æ¶æ„å›¾
```mermaid
graph TD
    A[åº”ç”¨å±‚] --> B[æ•°æ®åº“æŠ½è±¡å±‚]
    B --> C[AWS RDS]
    B --> D[Azure Database]
    B --> E[GCP Cloud SQL]
    
    subgraph "ä¸»åŒºåŸŸ"
        C --> F[ä¸»æ•°æ®åº“]
        D --> G[ä¸»æ•°æ®åº“]
        E --> H[ä¸»æ•°æ®åº“]
    end
    
    subgraph "ç¾å¤‡åŒºåŸŸ"
        I[RDSåªè¯»å‰¯æœ¬]
        J[Azureåªè¯»å‰¯æœ¬]
        K[Cloud SQLåªè¯»å‰¯æœ¬]
    end
    
    F --> I
    G --> J
    H --> K
    
    subgraph "ç›‘æ§æ²»ç†"
        L[ç»Ÿä¸€ç›‘æ§å¹³å°]
        M[æˆæœ¬ç®¡ç†]
        N[å®‰å…¨åˆè§„]
    end
    
    L --> C
    L --> D
    L --> E
    M --> C
    M --> D
    M --> E
    N --> C
    N --> D
    N --> E
```

### 5.2 å¤šäº‘æ•°æ®åº“ç®¡ç†å·¥å…·
```python
# multi_cloud_database_manager.py
import boto3
from azure.cosmos import CosmosClient
from google.cloud import sql_v1beta4
import json

class MultiCloudDatabaseManager:
    def __init__(self):
        # AWSå®¢æˆ·ç«¯
        self.aws_rds = boto3.client('rds')
        
        # Azureå®¢æˆ·ç«¯
        self.azure_cosmos = CosmosClient(
            'https://your-account.documents.azure.com:443/',
            credential='your-key'
        )
        
        # GCPå®¢æˆ·ç«¯
        self.gcp_sql = sql_v1beta4.SqlInstancesServiceClient()
    
    def get_all_database_instances(self) -> dict:
        """è·å–æ‰€æœ‰äº‘å¹³å°çš„æ•°æ®åº“å®ä¾‹"""
        instances = {
            'aws': self._get_aws_instances(),
            'azure': self._get_azure_instances(),
            'gcp': self._get_gcp_instances()
        }
        return instances
    
    def _get_aws_instances(self) -> list:
        """è·å–AWS RDSå®ä¾‹"""
        response = self.aws_rds.describe_db_instances()
        return [
            {
                'name': instance['DBInstanceIdentifier'],
                'engine': instance['Engine'],
                'status': instance['DBInstanceStatus'],
                'endpoint': instance['Endpoint']['Address'],
                'region': instance['AvailabilityZone'][:-1]
            }
            for instance in response['DBInstances']
        ]
    
    def _get_azure_instances(self) -> list:
        """è·å–Azureæ•°æ®åº“å®ä¾‹"""
        # å®ç°Azureæ•°æ®åº“å®ä¾‹è·å–é€»è¾‘
        return []
    
    def _get_gcp_instances(self) -> list:
        """è·å–GCP Cloud SQLå®ä¾‹"""
        # å®ç°GCPæ•°æ®åº“å®ä¾‹è·å–é€»è¾‘
        return []
    
    def compare_performance(self, instances: dict) -> dict:
        """æ¯”è¾ƒä¸åŒäº‘å¹³å°çš„æ€§èƒ½"""
        performance_comparison = {}
        
        for cloud, cloud_instances in instances.items():
            performance_comparison[cloud] = {
                'average_cpu': self._calculate_average_cpu(cloud_instances),
                'average_connections': self._calculate_average_connections(cloud_instances),
                'cost_efficiency': self._calculate_cost_efficiency(cloud_instances)
            }
        
        return performance_comparison
    
    def _calculate_average_cpu(self, instances: list) -> float:
        """è®¡ç®—å¹³å‡CPUä½¿ç”¨ç‡"""
        # å®ç°CPUä½¿ç”¨ç‡è®¡ç®—é€»è¾‘
        return 0.0
    
    def _calculate_average_connections(self, instances: list) -> float:
        """è®¡ç®—å¹³å‡è¿æ¥æ•°"""
        # å®ç°è¿æ¥æ•°è®¡ç®—é€»è¾‘
        return 0.0
    
    def _calculate_cost_efficiency(self, instances: list) -> float:
        """è®¡ç®—æˆæœ¬æ•ˆç›Š"""
        # å®ç°æˆæœ¬æ•ˆç›Šè®¡ç®—é€»è¾‘
        return 0.0

# ä½¿ç”¨ç¤ºä¾‹
manager = MultiCloudDatabaseManager()
all_instances = manager.get_all_database_instances()
performance_comparison = manager.compare_performance(all_instances)

print(json.dumps(performance_comparison, indent=2))
```

## 6. æˆæœ¬ä¼˜åŒ–ä¸æ€§èƒ½è°ƒä¼˜

### 6.1 æˆæœ¬ä¼˜åŒ–ç­–ç•¥

#### æˆæœ¬åˆ†æå’Œä¼˜åŒ–å·¥å…·
```python
# cost_optimizer.py
import boto3
import pandas as pd
from datetime import datetime, timedelta

class DatabaseCostOptimizer:
    def __init__(self, region: str = 'us-east-1'):
        self.ce_client = boto3.client('ce', region_name=region)
        self.rds_client = boto3.client('rds', region_name=region)
    
    def analyze_monthly_costs(self, months: int = 3) -> dict:
        """åˆ†ææœˆåº¦æ•°æ®åº“æˆæœ¬"""
        end_date = datetime.now()
        start_date = end_date - timedelta(days=30 * months)
        
        response = self.ce_client.get_cost_and_usage(
            TimePeriod={
                'Start': start_date.strftime('%Y-%m-%d'),
                'End': end_date.strftime('%Y-%m-%d')
            },
            Granularity='MONTHLY',
            Metrics=['UNBLENDEDCOST'],
            GroupBy=[
                {
                    'Type': 'DIMENSION',
                    'Key': 'SERVICE'
                }
            ],
            Filter={
                'Dimensions': {
                    'Key': 'SERVICE',
                    'Values': ['Amazon Relational Database Service']
                }
            }
        )
        
        monthly_costs = {}
        for result in response['ResultsByTime']:
            month = result['TimePeriod']['Start'][:7]
            cost = float(result['Total']['UnblendedCost']['Amount'])
            monthly_costs[month] = cost
        
        return monthly_costs
    
    def identify_cost_saving_opportunities(self) -> list:
        """è¯†åˆ«æˆæœ¬èŠ‚çº¦æœºä¼š"""
        opportunities = []
        
        # è·å–æ‰€æœ‰RDSå®ä¾‹
        instances = self.rds_client.describe_db_instances()['DBInstances']
        
        for instance in instances:
            # æ£€æŸ¥æœªä½¿ç”¨çš„å®ä¾‹
            if self._is_underutilized(instance):
                opportunities.append({
                    'instance_id': instance['DBInstanceIdentifier'],
                    'type': 'underutilized',
                    'savings_estimate': self._calculate_savings(instance, 'downsize'),
                    'recommendation': 'è€ƒè™‘é™çº§å®ä¾‹è§„æ ¼'
                })
            
            # æ£€æŸ¥é¢„ç•™å®ä¾‹æœºä¼š
            if self._eligible_for_reserved_instances(instance):
                opportunities.append({
                    'instance_id': instance['DBInstanceIdentifier'],
                    'type': 'reserved_instance',
                    'savings_estimate': self._calculate_savings(instance, 'ri'),
                    'recommendation': 'è´­ä¹°é¢„ç•™å®ä¾‹ä»¥èŠ‚çœæˆæœ¬'
                })
        
        return opportunities
    
    def _is_underutilized(self, instance: dict) -> bool:
        """åˆ¤æ–­å®ä¾‹æ˜¯å¦æœªå……åˆ†åˆ©ç”¨"""
        # å®ç°åˆ©ç”¨ç‡æ£€æŸ¥é€»è¾‘
        return False
    
    def _eligible_for_reserved_instances(self, instance: dict) -> bool:
        """åˆ¤æ–­æ˜¯å¦é€‚åˆé¢„ç•™å®ä¾‹"""
        # å®ç°é¢„ç•™å®ä¾‹é€‚ç”¨æ€§æ£€æŸ¥
        return True
    
    def _calculate_savings(self, instance: dict, optimization_type: str) -> float:
        """è®¡ç®—èŠ‚çº¦é‡‘é¢"""
        hourly_rate = self._get_hourly_rate(instance)
        monthly_hours = 730  # å¹³å‡æ¯æœˆå°æ—¶æ•°
        
        if optimization_type == 'downsize':
            # é™çº§èŠ‚çº¦ä¼°ç®—
            return hourly_rate * monthly_hours * 0.3  # å‡è®¾èŠ‚çº¦30%
        elif optimization_type == 'ri':
            # é¢„ç•™å®ä¾‹èŠ‚çº¦ä¼°ç®—
            return hourly_rate * monthly_hours * 0.6  # å‡è®¾èŠ‚çº¦60%
        
        return 0.0
    
    def _get_hourly_rate(self, instance: dict) -> float:
        """è·å–å®ä¾‹å°æ—¶è´¹ç‡"""
        instance_class = instance['DBInstanceClass']
        # ç®€åŒ–çš„è´¹ç‡æ˜ å°„
        rates = {
            'db.t3.micro': 0.017,
            'db.t3.small': 0.034,
            'db.t3.medium': 0.068,
            'db.m5.large': 0.152
        }
        return rates.get(instance_class, 0.1)

# ä½¿ç”¨ç¤ºä¾‹
optimizer = DatabaseCostOptimizer()
monthly_costs = optimizer.analyze_monthly_costs(months=6)
opportunities = optimizer.identify_cost_saving_opportunities()

print("æœˆåº¦æˆæœ¬åˆ†æ:")
for month, cost in monthly_costs.items():
    print(f"{month}: ${cost:.2f}")

print("\næˆæœ¬èŠ‚çº¦æœºä¼š:")
for opportunity in opportunities:
    print(f"{opportunity['instance_id']}: {opportunity['recommendation']}")
    print(f"é¢„è®¡æœˆèŠ‚çº¦: ${opportunity['savings_estimate']:.2f}")
```

### 6.2 æ€§èƒ½åŸºå‡†æµ‹è¯•å·¥å…·

#### æ•°æ®åº“æ€§èƒ½æµ‹è¯•æ¡†æ¶
```python
# performance_benchmark.py
import time
import threading
from concurrent.futures import ThreadPoolExecutor
import mysql.connector
import psycopg2
import pymongo
from typing import Dict, List, Any

class DatabaseBenchmark:
    def __init__(self, db_config: Dict[str, Any]):
        self.db_config = db_config
        self.db_type = db_config['type']
        self.connection = None
        self.test_results = {}
    
    def connect(self):
        """å»ºç«‹æ•°æ®åº“è¿æ¥"""
        if self.db_type == 'mysql':
            self.connection = mysql.connector.connect(
                host=self.db_config['host'],
                port=self.db_config['port'],
                user=self.db_config['user'],
                password=self.db_config['password'],
                database=self.db_config['database']
            )
        elif self.db_type == 'postgresql':
            self.connection = psycopg2.connect(
                host=self.db_config['host'],
                port=self.db_config['port'],
                user=self.db_config['user'],
                password=self.db_config['password'],
                database=self.db_config['database']
            )
        elif self.db_type == 'mongodb':
            self.connection = pymongo.MongoClient(
                f"mongodb://{self.db_config['host']}:{self.db_config['port']}/"
            )
    
    def run_benchmark_suite(self) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´çš„åŸºå‡†æµ‹è¯•å¥—ä»¶"""
        self.connect()
        
        benchmarks = {
            'connection_latency': self.test_connection_latency(),
            'query_performance': self.test_query_performance(),
            'write_performance': self.test_write_performance(),
            'concurrency_handling': self.test_concurrency()
        }
        
        self.connection.close()
        return benchmarks
    
    def test_connection_latency(self, iterations: int = 100) -> Dict[str, float]:
        """æµ‹è¯•è¿æ¥å»¶è¿Ÿ"""
        latencies = []
        
        for _ in range(iterations):
            start_time = time.time()
            if self.db_type in ['mysql', 'postgresql']:
                cursor = self.connection.cursor()
                cursor.execute("SELECT 1")
                cursor.fetchone()
                cursor.close()
            elif self.db_type == 'mongodb':
                self.connection.admin.command('ping')
            
            end_time = time.time()
            latencies.append((end_time - start_time) * 1000)  # è½¬æ¢ä¸ºæ¯«ç§’
        
        return {
            'min': min(latencies),
            'max': max(latencies),
            'avg': sum(latencies) / len(latencies),
            'p95': sorted(latencies)[int(len(latencies) * 0.95)],
            'p99': sorted(latencies)[int(len(latencies) * 0.99)]
        }
    
    def test_query_performance(self) -> Dict[str, Any]:
        """æµ‹è¯•æŸ¥è¯¢æ€§èƒ½"""
        queries = [
            "SELECT COUNT(*) FROM users",
            "SELECT * FROM orders WHERE status = 'completed' LIMIT 100",
            "SELECT u.name, o.total FROM users u JOIN orders o ON u.id = o.user_id WHERE o.created_at > DATE_SUB(NOW(), INTERVAL 1 DAY)"
        ]
        
        results = {}
        for i, query in enumerate(queries):
            start_time = time.time()
            
            if self.db_type in ['mysql', 'postgresql']:
                cursor = self.connection.cursor()
                cursor.execute(query)
                cursor.fetchall()
                cursor.close()
            elif self.db_type == 'mongodb':
                # MongoDBæŸ¥è¯¢ç¤ºä¾‹
                pass
            
            end_time = time.time()
            results[f'query_{i+1}'] = (end_time - start_time) * 1000
        
        return results
    
    def test_write_performance(self) -> Dict[str, float]:
        """æµ‹è¯•å†™å…¥æ€§èƒ½"""
        # å‡†å¤‡æµ‹è¯•æ•°æ®
        test_data = [
            {'name': f'user_{i}', 'email': f'user{i}@test.com'} 
            for i in range(1000)
        ]
        
        start_time = time.time()
        
        if self.db_type in ['mysql', 'postgresql']:
            cursor = self.connection.cursor()
            if self.db_type == 'mysql':
                insert_query = "INSERT INTO users (name, email) VALUES (%s, %s)"
            else:
                insert_query = "INSERT INTO users (name, email) VALUES (%s, %s)"
            
            cursor.executemany(insert_query, [(user['name'], user['email']) for user in test_data])
            self.connection.commit()
            cursor.close()
        
        end_time = time.time()
        
        return {
            'insert_1000_records_ms': (end_time - start_time) * 1000,
            'records_per_second': 1000 / (end_time - start_time)
        }
    
    def test_concurrency(self, concurrent_users: int = 50) -> Dict[str, Any]:
        """æµ‹è¯•å¹¶å‘å¤„ç†èƒ½åŠ›"""
        def worker(worker_id: int) -> float:
            local_conn = None
            try:
                # å»ºç«‹ç‹¬ç«‹è¿æ¥
                if self.db_type == 'mysql':
                    local_conn = mysql.connector.connect(
                        host=self.db_config['host'],
                        port=self.db_config['port'],
                        user=self.db_config['user'],
                        password=self.db_config['password'],
                        database=self.db_config['database']
                    )
                elif self.db_type == 'postgresql':
                    local_conn = psycopg2.connect(
                        host=self.db_config['host'],
                        port=self.db_config['port'],
                        user=self.db_config['user'],
                        password=self.db_config['password'],
                        database=self.db_config['database']
                    )
                
                start_time = time.time()
                
                # æ‰§è¡ŒæŸ¥è¯¢
                if self.db_type in ['mysql', 'postgresql']:
                    cursor = local_conn.cursor()
                    cursor.execute("SELECT COUNT(*) FROM users")
                    cursor.fetchone()
                    cursor.close()
                
                end_time = time.time()
                return (end_time - start_time) * 1000
                
            finally:
                if local_conn:
                    local_conn.close()
        
        # å¹¶å‘æ‰§è¡Œæµ‹è¯•
        start_time = time.time()
        
        with ThreadPoolExecutor(max_workers=concurrent_users) as executor:
            futures = [executor.submit(worker, i) for i in range(concurrent_users)]
            results = [future.result() for future in futures]
        
        total_time = time.time() - start_time
        
        return {
            'concurrent_users': concurrent_users,
            'total_execution_time': total_time,
            'average_response_time': sum(results) / len(results),
            'max_response_time': max(results),
            'throughput_qps': concurrent_users / total_time
        }

# ä½¿ç”¨ç¤ºä¾‹
mysql_config = {
    'type': 'mysql',
    'host': 'localhost',
    'port': 3306,
    'user': 'benchmark_user',
    'password: "${DB_PASSWORD}",
    'database': 'benchmark_db'
}

benchmark = DatabaseBenchmark(mysql_config)
results = benchmark.run_benchmark_suite()

print("åŸºå‡†æµ‹è¯•ç»“æœ:")
print(json.dumps(results, indent=2, ensure_ascii=False))
```

---

## ğŸ” å…³é”®è¦ç‚¹æ€»ç»“

### âœ… äº‘æ•°æ®åº“æœåŠ¡æˆåŠŸè¦ç´ 
- **åˆç†é€‰å‹**ï¼šåŸºäºä¸šåŠ¡éœ€æ±‚ã€æŠ€æœ¯è¦æ±‚å’Œæˆæœ¬è€ƒè™‘é€‰æ‹©åˆé€‚çš„äº‘æ•°æ®åº“æœåŠ¡
- **å®‰å…¨é…ç½®**ï¼šå®æ–½å®Œå–„çš„åŠ å¯†ã€è®¿é—®æ§åˆ¶å’Œå®¡è®¡æ—¥å¿—é…ç½®
- **æ€§èƒ½ä¼˜åŒ–**ï¼šé€šè¿‡å®ä¾‹è§„æ ¼ã€å­˜å‚¨é…ç½®å’Œå‚æ•°è°ƒä¼˜æå‡æ€§èƒ½
- **æˆæœ¬ç®¡æ§**ï¼šå»ºç«‹æˆæœ¬ç›‘æ§å’Œä¼˜åŒ–æœºåˆ¶ï¼Œé¿å…ä¸å¿…è¦çš„æ”¯å‡º

### âš ï¸ å¸¸è§é£é™©æé†’
- **å‚å•†é”å®š**ï¼šè¿‡åº¦ä¾èµ–ç‰¹å®šäº‘æœåŠ¡å•†å¯èƒ½å¸¦æ¥è¿ç§»å›°éš¾
- **ç½‘ç»œå»¶è¿Ÿ**ï¼šè·¨åŒºåŸŸè®¿é—®å¯èƒ½äº§ç”Ÿè¾ƒé«˜çš„ç½‘ç»œå»¶è¿Ÿ
- **æˆæœ¬å¤±æ§**ï¼šä¸åˆç†çš„èµ„æºé…ç½®å¯èƒ½å¯¼è‡´æˆæœ¬æ€¥å‰§ä¸Šå‡
- **åˆè§„æŒ‘æˆ˜**ï¼šä¸åŒåœ°åŒºçš„æ•°æ®åˆè§„è¦æ±‚å¯èƒ½å¢åŠ å¤æ‚æ€§

### ğŸ¯ æœ€ä½³å®è·µå»ºè®®
1. **å¤šäº‘ç­–ç•¥**ï¼šé‡‡ç”¨å¤šäº‘éƒ¨ç½²é™ä½å•ä¸€ä¾›åº”å•†é£é™©
2. **è‡ªåŠ¨åŒ–è¿ç»´**ï¼šåˆ©ç”¨äº‘æœåŠ¡å•†æä¾›çš„è‡ªåŠ¨åŒ–å·¥å…·å‡å°‘äººå·¥å¹²é¢„
3. **æŒç»­ç›‘æ§**ï¼šå»ºç«‹å…¨é¢çš„æ€§èƒ½å’Œæˆæœ¬ç›‘æ§ä½“ç³»
4. **å®šæœŸè¯„ä¼°**ï¼šå®šæœŸé‡æ–°è¯„ä¼°äº‘æ•°æ®åº“æœåŠ¡çš„é€‚ç”¨æ€§å’Œæ€§ä»·æ¯”
5. **ç¾éš¾æ¢å¤**ï¼šåˆ¶å®šå®Œå–„çš„å¤‡ä»½å’Œç¾å¤‡æ¢å¤æ–¹æ¡ˆ

é€šè¿‡ç§‘å­¦çš„äº‘æ•°æ®åº“æœåŠ¡é€‰å‹å’Œç®¡ç†ï¼Œä¼ä¸šå¯ä»¥è·å¾—æ›´å¥½çš„çµæ´»æ€§ã€å¯æ‰©å±•æ€§å’Œæˆæœ¬æ•ˆç›Šï¼ŒåŒæ—¶é™ä½è¿ç»´å¤æ‚åº¦å’ŒæŠ€æœ¯é£é™©ã€‚