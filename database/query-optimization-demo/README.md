# æ•°æ®åº“æŸ¥è¯¢ä¼˜åŒ–æŠ€æœ¯å®æˆ˜æ¼”ç¤º

## ğŸ¯ å­¦ä¹ ç›®æ ‡

é€šè¿‡æœ¬æ¡ˆä¾‹ä½ å°†æŒæ¡ä¼ä¸šçº§æ•°æ®åº“æŸ¥è¯¢ä¼˜åŒ–çš„æ ¸å¿ƒæŠ€èƒ½ï¼š

- æ·±å…¥ç†è§£æŸ¥è¯¢æ‰§è¡Œè®¡åˆ’å’Œä¼˜åŒ–å™¨å·¥ä½œåŸç†
- æŒæ¡SQLè¯­å¥æ€§èƒ½åˆ†æå’Œä¼˜åŒ–æŠ€å·§
- å­¦ä¼šç´¢å¼•ä¼˜åŒ–ç­–ç•¥å’Œæœ€ä½³å®è·µ
- å®æ–½æŸ¥è¯¢é‡å†™å’Œé‡æ„æŠ€æœ¯
- å»ºç«‹ç³»ç»Ÿæ€§çš„æ€§èƒ½ä¼˜åŒ–æ–¹æ³•è®º
- æ»¡è¶³ç”Ÿäº§ç¯å¢ƒçš„é«˜æ€§èƒ½è¦æ±‚

## ğŸ› ï¸ ç¯å¢ƒå‡†å¤‡

### ç³»ç»Ÿè¦æ±‚
- å·²å®Œæˆæ•°æ®åº“å®‰è£…é…ç½®ç¯å¢ƒ
- å…·å¤‡åŸºç¡€SQLæŸ¥è¯¢èƒ½åŠ›
- ç†è§£æ•°æ®åº“ç´¢å¼•åŸºæœ¬æ¦‚å¿µ
- å‡†å¤‡æµ‹è¯•æ•°æ®é›†ç”¨äºæ€§èƒ½éªŒè¯

### å‰ç½®æ¡ä»¶éªŒè¯
```bash
# éªŒè¯æ•°æ®åº“æœåŠ¡çŠ¶æ€
systemctl is-active mysqld postgresql-14 mongod redis

# åˆ›å»ºæµ‹è¯•æ•°æ®åº“
mysql -u root -p -e "CREATE DATABASE IF NOT EXISTS query_optimization_test;"
psql -U postgres -c "CREATE DATABASE query_optimization_test;"

# å‡†å¤‡æµ‹è¯•æ•°æ®
mysql -u root -p query_optimization_test < /opt/test_data/sample_data.sql
```

## ğŸ“ é¡¹ç›®ç»“æ„

```
query-optimization-demo/
â”œâ”€â”€ README.md                           # æœ¬è¯´æ˜æ–‡æ¡£
â”œâ”€â”€ metadata.json                       # å…ƒæ•°æ®é…ç½®
â”œâ”€â”€ scripts/                           # ä¼˜åŒ–è„šæœ¬
â”‚   â”œâ”€â”€ mysql_query_optimizer.sh       # MySQLæŸ¥è¯¢ä¼˜åŒ–è„šæœ¬
â”‚   â”œâ”€â”€ postgresql_query_optimizer.sh  # PostgreSQLæŸ¥è¯¢ä¼˜åŒ–è„šæœ¬
â”‚   â”œâ”€â”€ mongodb_query_optimizer.js     # MongoDBæŸ¥è¯¢ä¼˜åŒ–è„šæœ¬
â”‚   â”œâ”€â”€ query_analysis_tool.py         # æŸ¥è¯¢åˆ†æå·¥å…·
â”‚   â””â”€â”€ performance_benchmark.sh       # æ€§èƒ½åŸºå‡†æµ‹è¯•è„šæœ¬
â”œâ”€â”€ configs/                           # é…ç½®æ–‡ä»¶
â”‚   â”œâ”€â”€ optimization_rules/            # ä¼˜åŒ–è§„åˆ™é…ç½®
â”‚   â”œâ”€â”€ index_strategies/              # ç´¢å¼•ç­–ç•¥é…ç½®
â”‚   â”œâ”€â”€ cost_models/                   # æˆæœ¬æ¨¡å‹é…ç½®
â”‚   â””â”€â”€ workload_profiles/             # å·¥ä½œè´Ÿè½½é…ç½®
â”œâ”€â”€ examples/                          # ä¼˜åŒ–æ¡ˆä¾‹
â”‚   â”œâ”€â”€ slow_query_examples/           # æ…¢æŸ¥è¯¢ä¼˜åŒ–æ¡ˆä¾‹
â”‚   â”œâ”€â”€ join_optimization/             # è¿æ¥ä¼˜åŒ–æ¡ˆä¾‹
â”‚   â”œâ”€â”€ subquery_rewrite/              # å­æŸ¥è¯¢é‡å†™æ¡ˆä¾‹
â”‚   â””â”€â”€ index_design_patterns/         # ç´¢å¼•è®¾è®¡æ¨¡å¼
â”œâ”€â”€ benchmarks/                        # åŸºå‡†æµ‹è¯•
â”‚   â”œâ”€â”€ tpcc_benchmark/                # TPC-CåŸºå‡†æµ‹è¯•
â”‚   â”œâ”€â”€ sysbench_tests/                # Sysbenchæµ‹è¯•
â”‚   â”œâ”€â”€ custom_workloads/              # è‡ªå®šä¹‰å·¥ä½œè´Ÿè½½
â”‚   â””â”€â”€ performance_reports/           # æ€§èƒ½æŠ¥å‘Š
â””â”€â”€ docs/                              # è¯¦ç»†æ–‡æ¡£
    â”œâ”€â”€ optimization_methodology.md    # ä¼˜åŒ–æ–¹æ³•è®º
    â”œâ”€â”€ execution_plan_analysis.md     # æ‰§è¡Œè®¡åˆ’åˆ†æ
    â”œâ”€â”€ index_design_guide.md          # ç´¢å¼•è®¾è®¡æŒ‡å—
    â””â”€â”€ best_practices.md              # æœ€ä½³å®è·µæŒ‡å—
```

## ğŸ“Š æŸ¥è¯¢ä¼˜åŒ–æ–¹æ³•è®º

### ä¼˜åŒ–é‡‘å­—å¡”æ¨¡å‹
```yaml
# æŸ¥è¯¢ä¼˜åŒ–é‡‘å­—å¡”
optimization_pyramid:
  foundation_layer:          # åŸºç¡€å±‚
    - schema_design          # æ¨¡å¼è®¾è®¡ä¼˜åŒ–
    - data_modeling          # æ•°æ®å»ºæ¨¡ä¼˜åŒ–
    - normalization          # è§„èŒƒåŒ–è®¾è®¡
  
  structural_layer:          # ç»“æ„å±‚
    - indexing_strategy      # ç´¢å¼•ç­–ç•¥
    - partitioning_design    # åˆ†åŒºè®¾è®¡
    - clustering_keys        # èšç°‡é”®è®¾è®¡
  
  operational_layer:         # æ“ä½œå±‚
    - query_rewrite          # æŸ¥è¯¢é‡å†™
    - execution_planning     # æ‰§è¡Œè®¡åˆ’ä¼˜åŒ–
    - resource_allocation    # èµ„æºåˆ†é…ä¼˜åŒ–
  
  tactical_layer:            # æˆ˜æœ¯å±‚
    - caching_strategies     # ç¼“å­˜ç­–ç•¥
    - connection_pooling     # è¿æ¥æ± ä¼˜åŒ–
    - batch_processing       # æ‰¹å¤„ç†ä¼˜åŒ–
  
  strategic_layer:           # æˆ˜ç•¥å±‚
    - workload_analysis      # å·¥ä½œè´Ÿè½½åˆ†æ
    - capacity_planning      # å®¹é‡è§„åˆ’
    - performance_monitoring # æ€§èƒ½ç›‘æ§
```

## ğŸ”§ æ ¸å¿ƒä¼˜åŒ–æŠ€æœ¯å®ç°

### 1. MySQLæŸ¥è¯¢ä¼˜åŒ–æŠ€æœ¯

```sql
-- MySQLæŸ¥è¯¢ä¼˜åŒ–å®Œæ•´ç¤ºä¾‹

-- 1. æ‰§è¡Œè®¡åˆ’åˆ†æ
-- åŸºç¡€æ‰§è¡Œè®¡åˆ’æŸ¥çœ‹
EXPLAIN SELECT u.name, o.order_date, p.product_name 
FROM users u 
JOIN orders o ON u.id = o.user_id 
JOIN order_items oi ON o.id = oi.order_id 
JOIN products p ON oi.product_id = p.id 
WHERE u.created_at > '2024-01-01' 
AND o.status = 'completed';

-- è¯¦ç»†æ‰§è¡Œè®¡åˆ’åˆ†æ
EXPLAIN FORMAT=JSON SELECT u.name, o.order_date, p.product_name 
FROM users u 
JOIN orders o ON u.id = o.user_id 
JOIN order_items oi ON o.id = oi.order_id 
JOIN products p ON oi.product_id = p.id 
WHERE u.created_at > '2024-01-01' 
AND o.status = 'completed';

-- 2. ç´¢å¼•ä¼˜åŒ–ç­–ç•¥
-- åˆ›å»ºå¤åˆç´¢å¼•ä¼˜åŒ–è¿æ¥æŸ¥è¯¢
CREATE INDEX idx_users_created_at ON users(created_at);
CREATE INDEX idx_orders_user_status ON orders(user_id, status);
CREATE INDEX idx_order_items_order_product ON order_items(order_id, product_id);
CREATE INDEX idx_products_id_name ON products(id, product_name);

-- 3. æŸ¥è¯¢é‡å†™ä¼˜åŒ–
-- åŸå§‹æ…¢æŸ¥è¯¢
SELECT u.name, COUNT(o.id) as order_count, SUM(oi.quantity * p.price) as total_amount
FROM users u
LEFT JOIN orders o ON u.id = o.user_id
LEFT JOIN order_items oi ON o.id = oi.order_id
LEFT JOIN products p ON oi.product_id = p.id
WHERE u.created_at > DATE_SUB(NOW(), INTERVAL 1 YEAR)
GROUP BY u.id, u.name
HAVING order_count > 10
ORDER BY total_amount DESC
LIMIT 100;

-- ä¼˜åŒ–åçš„æŸ¥è¯¢
SELECT u.name, 
       user_stats.order_count,
       user_stats.total_amount
FROM users u
JOIN (
    SELECT o.user_id,
           COUNT(*) as order_count,
           SUM(oi.quantity * p.price) as total_amount
    FROM orders o
    JOIN order_items oi ON o.id = oi.order_id
    JOIN products p ON oi.product_id = p.id
    WHERE o.status = 'completed'
    AND o.order_date > DATE_SUB(NOW(), INTERVAL 1 YEAR)
    GROUP BY o.user_id
    HAVING COUNT(*) > 10
) user_stats ON u.id = user_stats.user_id
WHERE u.created_at > DATE_SUB(NOW(), INTERVAL 1 YEAR)
ORDER BY user_stats.total_amount DESC
LIMIT 100;

-- 4. å­æŸ¥è¯¢ä¼˜åŒ–
-- ç›¸å…³å­æŸ¥è¯¢ä¼˜åŒ–ä¸ºè¿æ¥æŸ¥è¯¢
-- åŸå§‹æŸ¥è¯¢ï¼ˆç›¸å…³å­æŸ¥è¯¢ï¼‰
SELECT u.name, u.email
FROM users u
WHERE u.id IN (
    SELECT o.user_id 
    FROM orders o 
    WHERE o.order_date > DATE_SUB(NOW(), INTERVAL 30 DAY)
    AND o.total_amount > 1000
);

-- ä¼˜åŒ–åï¼ˆä½¿ç”¨EXISTSï¼‰
SELECT u.name, u.email
FROM users u
WHERE EXISTS (
    SELECT 1 
    FROM orders o 
    WHERE o.user_id = u.id
    AND o.order_date > DATE_SUB(NOW(), INTERVAL 30 DAY)
    AND o.total_amount > 1000
);

-- 5. åˆ†é¡µæŸ¥è¯¢ä¼˜åŒ–
-- å¤§æ•°æ®é‡åˆ†é¡µä¼˜åŒ–
-- åŸå§‹æ…¢åˆ†é¡µæŸ¥è¯¢
SELECT * FROM orders 
WHERE status = 'completed' 
ORDER BY created_at DESC 
LIMIT 100000, 20;

-- ä¼˜åŒ–åçš„åˆ†é¡µæŸ¥è¯¢ï¼ˆä½¿ç”¨æ¸¸æ ‡ï¼‰
SELECT o.* FROM orders o
INNER JOIN (
    SELECT id FROM orders 
    WHERE status = 'completed' 
    ORDER BY created_at DESC 
    LIMIT 100000, 20
) AS pagination_cursor ON o.id = pagination_cursor.id;

-- 6. èšåˆæŸ¥è¯¢ä¼˜åŒ–
-- å¤æ‚èšåˆæŸ¥è¯¢ä¼˜åŒ–
SELECT 
    DATE_FORMAT(o.order_date, '%Y-%m') as month,
    p.category,
    COUNT(DISTINCT o.id) as order_count,
    COUNT(oi.id) as item_count,
    SUM(oi.quantity) as total_quantity,
    AVG(oi.quantity * p.price) as avg_order_value
FROM orders o
JOIN order_items oi ON o.id = oi.order_id
JOIN products p ON oi.product_id = p.id
WHERE o.order_date >= DATE_SUB(NOW(), INTERVAL 12 MONTH)
GROUP BY DATE_FORMAT(o.order_date, '%Y-%m'), p.category
ORDER BY month DESC, order_count DESC;

-- ä¼˜åŒ–ç´¢å¼•æ”¯æŒ
CREATE INDEX idx_orders_date_status ON orders(order_date, status);
CREATE INDEX idx_order_items_order_product ON order_items(order_id, product_id);
CREATE INDEX idx_products_category_price ON products(category, price);

-- 7. ç»Ÿè®¡ä¿¡æ¯æ›´æ–°
-- æ›´æ–°è¡¨ç»Ÿè®¡ä¿¡æ¯ä»¥ä¼˜åŒ–æŸ¥è¯¢è®¡åˆ’
ANALYZE TABLE users, orders, order_items, products;

-- 8. é…ç½®å‚æ•°ä¼˜åŒ–
-- è°ƒæ•´MySQLä¼˜åŒ–å™¨å‚æ•°
SET SESSION optimizer_search_depth = 0;  -- è‡ªåŠ¨é€‰æ‹©æœç´¢æ·±åº¦
SET SESSION join_buffer_size = 2097152;  -- 2MBè¿æ¥ç¼“å†²åŒº
SET SESSION sort_buffer_size = 2097152;  -- 2MBæ’åºç¼“å†²åŒº
SET SESSION read_rnd_buffer_size = 524288; -- 512KBéšæœºè¯»ç¼“å†²åŒº

-- 9. æŸ¥è¯¢ç¼“å­˜ä¼˜åŒ–
-- å¯ç”¨æŸ¥è¯¢ç¼“å­˜ï¼ˆé€‚ç”¨äºè¯»å¤šå†™å°‘åœºæ™¯ï¼‰
SET GLOBAL query_cache_type = ON;
SET GLOBAL query_cache_size = 268435456;  -- 256MBç¼“å­˜å¤§å°

-- 10. æ€§èƒ½åŸºå‡†æµ‹è¯•
-- ä½¿ç”¨sysbenchè¿›è¡ŒæŸ¥è¯¢æ€§èƒ½æµ‹è¯•
sysbench /usr/share/sysbench/oltp_read_only.lua \
  --mysql-host=localhost \
  --mysql-port=3306 \
  --mysql-user=sbtest \
  --mysql-password=sbtest \
  --mysql-db=sbtest \
  --table-size=1000000 \
  --tables=10 \
  --threads=16 \
  --time=300 \
  run
```

### 2. PostgreSQLæŸ¥è¯¢ä¼˜åŒ–æŠ€æœ¯

```sql
-- PostgreSQLæŸ¥è¯¢ä¼˜åŒ–å®Œæ•´ç¤ºä¾‹

-- 1. æ‰§è¡Œè®¡åˆ’è¯¦ç»†åˆ†æ
-- åŸºç¡€æ‰§è¡Œè®¡åˆ’
EXPLAIN (ANALYZE, BUFFERS, VERBOSE, FORMAT JSON)
SELECT u.name, o.order_date, p.product_name 
FROM users u 
JOIN orders o ON u.id = o.user_id 
JOIN order_items oi ON o.id = oi.order_id 
JOIN products p ON oi.product_id = p.id 
WHERE u.created_at > '2024-01-01' 
AND o.status = 'completed';

-- 2. ç´¢å¼•ä¼˜åŒ–ç­–ç•¥
-- åˆ›å»ºæ”¯æŒè¿æ¥æŸ¥è¯¢çš„å¤åˆç´¢å¼•
CREATE INDEX CONCURRENTLY idx_users_created_at ON users(created_at);
CREATE INDEX CONCURRENTLY idx_orders_user_status_date ON orders(user_id, status, order_date);
CREATE INDEX CONCURRENTLY idx_order_items_order_product ON order_items(order_id, product_id);
CREATE INDEX CONCURRENTLY idx_products_category_name ON products(category, product_name);

-- 3. æŸ¥è¯¢é‡å†™ä¼˜åŒ–
-- ä½¿ç”¨CTEä¼˜åŒ–å¤æ‚æŸ¥è¯¢
WITH user_order_stats AS (
    SELECT 
        o.user_id,
        COUNT(*) as order_count,
        SUM(oi.quantity * p.price) as total_amount,
        AVG(oi.quantity * p.price) as avg_order_value
    FROM orders o
    JOIN order_items oi ON o.id = oi.order_id
    JOIN products p ON oi.product_id = p.id
    WHERE o.status = 'completed'
    AND o.order_date > CURRENT_DATE - INTERVAL '1 year'
    GROUP BY o.user_id
    HAVING COUNT(*) > 10
)
SELECT 
    u.name,
    uos.order_count,
    uos.total_amount,
    uos.avg_order_value
FROM users u
JOIN user_order_stats uos ON u.id = uos.user_id
WHERE u.created_at > CURRENT_DATE - INTERVAL '1 year'
ORDER BY uos.total_amount DESC
LIMIT 100;

-- 4. çª—å£å‡½æ•°ä¼˜åŒ–
-- ä½¿ç”¨çª—å£å‡½æ•°æ›¿ä»£å­æŸ¥è¯¢
SELECT 
    u.name,
    o.order_date,
    o.total_amount,
    ROW_NUMBER() OVER (PARTITION BY u.id ORDER BY o.order_date DESC) as order_rank,
    SUM(o.total_amount) OVER (PARTITION BY u.id) as user_total_spent,
    AVG(o.total_amount) OVER (PARTITION BY u.id) as user_avg_order
FROM users u
JOIN orders o ON u.id = o.user_id
WHERE o.status = 'completed'
AND o.order_date > CURRENT_DATE - INTERVAL '1 year';

-- 5. åˆ†åŒºè¡¨ä¼˜åŒ–
-- åˆ›å»ºåˆ†åŒºè¡¨æé«˜æŸ¥è¯¢æ€§èƒ½
CREATE TABLE orders_partitioned (
    id SERIAL,
    user_id INTEGER,
    order_date DATE,
    status VARCHAR(20),
    total_amount DECIMAL(10,2)
) PARTITION BY RANGE (order_date);

-- åˆ›å»ºæœˆåº¦åˆ†åŒº
CREATE TABLE orders_2024_01 PARTITION OF orders_partitioned
FOR VALUES FROM ('2024-01-01') TO ('2024-02-01');

CREATE TABLE orders_2024_02 PARTITION OF orders_partitioned
FOR VALUES FROM ('2024-02-01') TO ('2024-03-01');

-- 6. ç‰©åŒ–è§†å›¾ä¼˜åŒ–
-- åˆ›å»ºç‰©åŒ–è§†å›¾é¢„è®¡ç®—å¤æ‚èšåˆ
CREATE MATERIALIZED VIEW mv_user_statistics AS
SELECT 
    u.id as user_id,
    u.name,
    COUNT(o.id) as total_orders,
    SUM(o.total_amount) as total_spent,
    AVG(o.total_amount) as avg_order_value,
    MAX(o.order_date) as last_order_date
FROM users u
LEFT JOIN orders o ON u.id = o.user_id
GROUP BY u.id, u.name;

-- åˆ·æ–°ç‰©åŒ–è§†å›¾
REFRESH MATERIALIZED VIEW mv_user_statistics;

-- 7. ç»Ÿè®¡ä¿¡æ¯æ›´æ–°
-- æ›´æ–°è¡¨å’Œç´¢å¼•ç»Ÿè®¡ä¿¡æ¯
ANALYZE users;
ANALYZE orders;
ANALYZE order_items;
ANALYZE products;

-- 8. é…ç½®å‚æ•°ä¼˜åŒ–
-- è°ƒæ•´PostgreSQLä¼˜åŒ–å™¨å‚æ•°
ALTER SYSTEM SET shared_buffers = '2GB';
ALTER SYSTEM SET effective_cache_size = '6GB';
ALTER SYSTEM SET work_mem = '64MB';
ALTER SYSTEM SET maintenance_work_mem = '512MB';
ALTER SYSTEM SET random_page_cost = 1.1;
ALTER SYSTEM SET seq_page_cost = 1.0;
SELECT pg_reload_conf();

-- 9. å¹¶è¡ŒæŸ¥è¯¢ä¼˜åŒ–
-- å¯ç”¨å¹¶è¡ŒæŸ¥è¯¢
SET max_parallel_workers_per_gather = 4;
SET parallel_setup_cost = 1000;
SET parallel_tuple_cost = 0.1;

-- 10. æ€§èƒ½åŸºå‡†æµ‹è¯•
-- ä½¿ç”¨pgbenchè¿›è¡ŒåŸºå‡†æµ‹è¯•
pgbench -i -s 100 query_optimization_test  # åˆå§‹åŒ–æµ‹è¯•æ•°æ®
pgbench -c 50 -j 10 -T 300 query_optimization_test  # è¿è¡Œæµ‹è¯•
```

### 3. MongoDBæŸ¥è¯¢ä¼˜åŒ–æŠ€æœ¯

```javascript
// MongoDBæŸ¥è¯¢ä¼˜åŒ–å®Œæ•´ç¤ºä¾‹

// 1. æŸ¥è¯¢è®¡åˆ’åˆ†æ
// åˆ†ææŸ¥è¯¢æ‰§è¡Œè®¡åˆ’
db.orders.find({
  status: "completed",
  orderDate: {$gte: new Date("2024-01-01")}
}).explain("executionStats");

// 2. ç´¢å¼•ä¼˜åŒ–ç­–ç•¥
// åˆ›å»ºå¤åˆç´¢å¼•æ”¯æŒæŸ¥è¯¢
db.orders.createIndex({status: 1, orderDate: -1});
db.orders.createIndex({"userId": 1, "status": 1});
db.orderItems.createIndex({"orderId": 1, "productId": 1});

// åˆ›å»ºæ–‡æœ¬ç´¢å¼•æ”¯æŒå…¨æ–‡æœç´¢
db.products.createIndex({productName: "text", description: "text"});

// 3. æŸ¥è¯¢é‡å†™ä¼˜åŒ–
// åŸå§‹æ…¢æŸ¥è¯¢
db.orders.aggregate([
  {
    $lookup: {
      from: "users",
      localField: "userId",
      foreignField: "_id",
      as: "user"
    }
  },
  {
    $match: {
      "user.createdAt": {$gte: new Date("2024-01-01")},
      status: "completed"
    }
  },
  {
    $lookup: {
      from: "orderItems",
      localField: "_id",
      foreignField: "orderId",
      as: "items"
    }
  },
  {
    $project: {
      userName: {$arrayElemAt: ["$user.name", 0]},
      orderDate: 1,
      totalAmount: {$sum: "$items.amount"}
    }
  }
]);

// ä¼˜åŒ–åçš„æŸ¥è¯¢
db.orders.aggregate([
  // é¢„è¿‡æ»¤å‡å°‘è¿æ¥æ•°æ®é‡
  {$match: {status: "completed", orderDate: {$gte: new Date("2024-01-01")}}},
  
  // ä½¿ç”¨pipelineè¿›è¡Œè¿æ¥ä¼˜åŒ–
  {
    $lookup: {
      from: "users",
      let: {userId: "$userId"},
      pipeline: [
        {$match: {$expr: {$and: [
          {$eq: ["$_id", "$$userId"]},
          {$gte: ["$createdAt", new Date("2024-01-01")]}
        ]}}},
        {$project: {name: 1}}
      ],
      as: "user"
    }
  },
  
  // åªè·å–éœ€è¦çš„å­—æ®µ
  {$project: {
    userId: 1,
    orderDate: 1,
    user: 1
  }}
]);

// 4. èšåˆç®¡é“ä¼˜åŒ–
// ä¼˜åŒ–å¤æ‚çš„èšåˆæŸ¥è¯¢
db.orders.aggregate([
  {$match: {
    status: "completed",
    orderDate: {$gte: new Date(new Date().setFullYear(new Date().getFullYear() - 1))}
  }},
  
  // ä½¿ç”¨$facetè¿›è¡Œå¹¶è¡Œå¤„ç†
  {$facet: {
    orderStats: [
      {$group: {
        _id: null,
        totalOrders: {$sum: 1},
        avgOrderValue: {$avg: "$totalAmount"},
        maxOrderValue: {$max: "$totalAmount"}
      }}
    ],
    
    userStats: [
      {$group: {
        _id: "$userId",
        orderCount: {$sum: 1},
        totalSpent: {$sum: "$totalAmount"}
      }},
      {$sort: {totalSpent: -1}},
      {$limit: 100}
    ],
    
    monthlyStats: [
      {$group: {
        _id: {
          year: {$year: "$orderDate"},
          month: {$month: "$orderDate"}
        },
        orderCount: {$sum: 1},
        totalRevenue: {$sum: "$totalAmount"}
      }},
      {$sort: {"_id.year": -1, "_id.month": -1}}
    ]
  }}
]);

// 5. åˆ†ç‰‡é”®ä¼˜åŒ–
// ä¸ºåˆ†ç‰‡é›†åˆé€‰æ‹©åˆé€‚çš„åˆ†ç‰‡é”®
sh.shardCollection("ecommerce.orders", {"userId": 1, "orderDate": 1});

// 6. æŸ¥è¯¢ç¼“å­˜ä¼˜åŒ–
// ä½¿ç”¨MapReduceé¢„è®¡ç®—ç»“æœ
db.orders.mapReduce(
  function() {
    emit(this.userId, {
      orderCount: 1,
      totalAmount: this.totalAmount,
      lastOrderDate: this.orderDate
    });
  },
  function(key, values) {
    var result = {orderCount: 0, totalAmount: 0, lastOrderDate: new Date(0)};
    values.forEach(function(value) {
      result.orderCount += value.orderCount;
      result.totalAmount += value.totalAmount;
      if (value.lastOrderDate > result.lastOrderDate) {
        result.lastOrderDate = value.lastOrderDate;
      }
    });
    return result;
  },
  {
    out: "user_order_summary",
    query: {status: "completed"}
  }
);

// 7. æ€§èƒ½ç›‘æ§å’Œåˆ†æ
// å¯ç”¨æ…¢æŸ¥è¯¢æ—¥å¿—
db.setProfilingLevel(1, {slowms: 100});  // è®°å½•è¶…è¿‡100msçš„æŸ¥è¯¢

// åˆ†ææ…¢æŸ¥è¯¢æ—¥å¿—
db.system.profile.find().sort({$natural: -1}).limit(10);

// 8. å†…å­˜ä½¿ç”¨ä¼˜åŒ–
// è°ƒæ•´WiredTigerç¼“å­˜å¤§å°
db.adminCommand({
  "setParameter": 1,
  "wiredTigerEngineConfigString": "cache_size=2G"
});

// 9. è¿æ¥æ± ä¼˜åŒ–
// é…ç½®åº”ç”¨ç¨‹åºè¿æ¥æ± 
const mongoOptions = {
  maxPoolSize: 50,
  minPoolSize: 10,
  maxIdleTimeMS: 30000,
  serverSelectionTimeoutMS: 5000,
  socketTimeoutMS: 45000
};

// 10. åŸºå‡†æµ‹è¯•
// ä½¿ç”¨mongostatç›‘æ§å®æ—¶æ€§èƒ½
// mongostat --host localhost:27017 --discover 10

// ä½¿ç”¨mongoperfæµ‹è¯•ç£ç›˜æ€§èƒ½
// echo '{"nThreads":8,"fileSizeMB":1000,"sleepMicros":1000}' | mongoperf
```

### 4. RedisæŸ¥è¯¢ä¼˜åŒ–æŠ€æœ¯

```bash
#!/bin/bash
# RedisæŸ¥è¯¢ä¼˜åŒ–æŠ€æœ¯è„šæœ¬

# 1. é”®ç©ºé—´ä¼˜åŒ–
optimize_key_structure() {
  echo "ä¼˜åŒ–Redisé”®ç»“æ„..."
  
  # ä½¿ç”¨å“ˆå¸Œç»“æ„å‡å°‘é”®æ•°é‡
  # åŸå§‹æ–¹å¼ï¼ˆå¤šä¸ªé”®ï¼‰
  redis-cli SET "user:1001:name" "John Doe"
  redis-cli SET "user:1001:email" "john@example.com"
  redis-cli SET "user:1001:age" "30"
  
  # ä¼˜åŒ–æ–¹å¼ï¼ˆå“ˆå¸Œç»“æ„ï¼‰
  redis-cli HSET "user:1001" name "John Doe" email "john@example.com" age "30"
  
  # ä½¿ç”¨å‘½åç©ºé—´å‰ç¼€
  redis-cli SET "session:user:1001:last_active" "$(date +%s)"
  redis-cli SET "cache:product:2001:price" "29.99"
}

# 2. å†…å­˜ä¼˜åŒ–
optimize_memory_usage() {
  echo "ä¼˜åŒ–Rediså†…å­˜ä½¿ç”¨..."
  
  # ä½¿ç”¨è¾ƒå°çš„æ•°æ®ç±»å‹
  # åŸå§‹æ–¹å¼
  redis-cli SET "user:1001:active" "true"
  
  # ä¼˜åŒ–æ–¹å¼
  redis-cli SETBIT "user:active_flags" 1001 1
  
  # ä½¿ç”¨æ•´æ•°ç¼–ç 
  redis-cli INCR "counter:page_views"
  
  # é…ç½®å†…å­˜æ·˜æ±°ç­–ç•¥
  redis-cli CONFIG SET maxmemory 2gb
  redis-cli CONFIG SET maxmemory-policy allkeys-lru
}

# 3. ç®¡é“ä¼˜åŒ–
optimize_with_pipelining() {
  echo "ä½¿ç”¨ç®¡é“ä¼˜åŒ–æ‰¹é‡æ“ä½œ..."
  
  # åŸå§‹æ–¹å¼ï¼ˆå¤šæ¬¡ç½‘ç»œå¾€è¿”ï¼‰
  for i in {1..1000}; do
    redis-cli SET "item:$i" "value_$i"
  done
  
  # ä¼˜åŒ–æ–¹å¼ï¼ˆç®¡é“æ‰¹é‡æ“ä½œï¼‰
  redis-cli --pipe << EOF
$(for i in {1..1000}; do echo "SET item:$i value_$i"; done)
EOF
}

# 4. Luaè„šæœ¬ä¼˜åŒ–
optimize_with_lua_scripts() {
  echo "ä½¿ç”¨Luaè„šæœ¬ä¼˜åŒ–åŸå­æ“ä½œ..."
  
  # å¤æ‚çš„åŸå­æ“ä½œ
  local lua_script=$(cat << 'EOF'
local user_id = KEYS[1]
local product_id = ARGV[1]
local quantity = tonumber(ARGV[2])

-- æ£€æŸ¥åº“å­˜
local stock = redis.call('HGET', 'product:' .. product_id, 'stock')
if not stock or tonumber(stock) < quantity then
  return {err = 'Insufficient stock'}
end

-- æ‰£å‡åº“å­˜
redis.call('HINCRBY', 'product:' .. product_id, 'stock', -quantity)

-- è®°å½•è®¢å•
local order_key = 'order:' .. user_id .. ':' .. redis.call('TIME')[1]
redis.call('HMSET', order_key, 'product_id', product_id, 'quantity', quantity, 'timestamp', redis.call('TIME')[1])

return 'Order placed successfully'
EOF
)
  
  # æ‰§è¡ŒLuaè„šæœ¬
  redis-cli EVAL "$lua_script" 1 "user:1001" "product:2001" "2"
}

# 5. è¿‡æœŸæ—¶é—´ä¼˜åŒ–
optimize_expiration_strategy() {
  echo "ä¼˜åŒ–é”®è¿‡æœŸç­–ç•¥..."
  
  # åˆç†è®¾ç½®è¿‡æœŸæ—¶é—´
  redis-cli SETEX "session:user:1001" 3600 "session_data"
  redis-cli SETEX "cache:api_response" 300 "cached_response"
  
  # ä½¿ç”¨éšæœºè¿‡æœŸé¿å…é›ªå´©æ•ˆåº”
  local ttl=$((3600 + RANDOM % 3600))
  redis-cli SETEX "cache:expensive_query" $ttl "cached_result"
}

# 6. é›†ç¾¤ä¼˜åŒ–
optimize_cluster_performance() {
  echo "ä¼˜åŒ–Redisé›†ç¾¤æ€§èƒ½..."
  
  # ä½¿ç”¨hash tagsç¡®ä¿ç›¸å…³é”®åœ¨åŒä¸€slot
  redis-cli SET "{user:1001}:profile" "profile_data"
  redis-cli SET "{user:1001}:preferences" "preference_data"
  
  # æ‰¹é‡æ“ä½œä¼˜åŒ–
  redis-cli MGET "{user:1001}:profile" "{user:1001}:preferences"
}

# 7. ç›‘æ§å’Œåˆ†æ
setup_performance_monitoring() {
  echo "è®¾ç½®æ€§èƒ½ç›‘æ§..."
  
  # å¯ç”¨æ…¢æŸ¥è¯¢æ—¥å¿—
  redis-cli CONFIG SET slowlog-log-slower-than 10000  # 10ms
  redis-cli CONFIG SET slowlog-max-len 128
  
  # ç›‘æ§å…³é”®æŒ‡æ ‡
  redis-cli INFO stats
  redis-cli INFO memory
  redis-cli INFO cpu
  redis-cli INFO commandstats
  
  # åˆ†ææ…¢æŸ¥è¯¢
  redis-cli SLOWLOG GET 10
}

# 8. åŸºå‡†æµ‹è¯•
run_performance_benchmark() {
  echo "è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•..."
  
  # ä½¿ç”¨redis-benchmarkè¿›è¡Œæµ‹è¯•
  redis-benchmark -h localhost -p 6379 -n 100000 -c 50 -t get,set,lpush,lpop
  
  # æµ‹è¯•ä¸åŒæ•°æ®ç±»å‹çš„æ€§èƒ½
  echo "æµ‹è¯•å­—ç¬¦ä¸²æ“ä½œæ€§èƒ½:"
  redis-benchmark -t set,get -n 100000 -d 256
  
  echo "æµ‹è¯•å“ˆå¸Œæ“ä½œæ€§èƒ½:"
  redis-benchmark -t hset,hget -n 100000
  
  echo "æµ‹è¯•åˆ—è¡¨æ“ä½œæ€§èƒ½:"
  redis-benchmark -t lpush,lpop -n 100000
}

# 9. é…ç½®ä¼˜åŒ–
optimize_redis_configuration() {
  echo "ä¼˜åŒ–Redisé…ç½®å‚æ•°..."
  
  # å†…å­˜ç›¸å…³é…ç½®
  redis-cli CONFIG SET tcp-keepalive 300
  redis-cli CONFIG SET timeout 0
  redis-cli CONFIG SET tcp-backlog 511
  
  # æŒä¹…åŒ–é…ç½®ä¼˜åŒ–
  redis-cli CONFIG SET save "900 1 300 10 60 10000"
  redis-cli CONFIG SET appendfsync everysec
  
  # ç½‘ç»œé…ç½®ä¼˜åŒ–
  redis-cli CONFIG SET tcp-nodelay yes
}

# ä¸»æ‰§è¡Œå‡½æ•°
main() {
  case "$1" in
    optimize-all)
      optimize_key_structure
      optimize_memory_usage
      optimize_with_pipelining
      optimize_with_lua_scripts
      optimize_expiration_strategy
      optimize_cluster_performance
      setup_performance_monitoring
      run_performance_benchmark
      optimize_redis_configuration
      echo "RedisæŸ¥è¯¢ä¼˜åŒ–å®Œæˆ"
      ;;
    benchmark)
      run_performance_benchmark
      ;;
    monitor)
      setup_performance_monitoring
      ;;
    *)
      echo "Usage: $0 {optimize-all|benchmark|monitor}"
      exit 1
      ;;
  esac
}

main "$@"
```

## ğŸ¯ ç»Ÿä¸€æŸ¥è¯¢ä¼˜åŒ–å¹³å°

### PythonæŸ¥è¯¢ä¼˜åŒ–åˆ†æå™¨
```python
#!/usr/bin/env python3
"""
ä¼ä¸šçº§ç»Ÿä¸€æŸ¥è¯¢ä¼˜åŒ–åˆ†æå¹³å°
æ”¯æŒå¤šæ•°æ®åº“æŸ¥è¯¢ä¼˜åŒ–çš„æ™ºèƒ½åˆ†æå’Œå»ºè®®
"""

import re
import json
import sqlite3
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass
from enum import Enum

class DatabaseType(Enum):
    MYSQL = "mysql"
    POSTGRESQL = "postgresql"
    MONGODB = "mongodb"
    REDIS = "redis"

@dataclass
class QueryAnalysis:
    """æŸ¥è¯¢åˆ†æç»“æœ"""
    query_text: str
    database_type: DatabaseType
    complexity_score: float
    optimization_suggestions: List[str]
    estimated_performance_gain: float
    execution_plan_issues: List[str]

class QueryOptimizer:
    """ç»Ÿä¸€æŸ¥è¯¢ä¼˜åŒ–å™¨"""
    
    def __init__(self):
        self.optimization_rules = self._load_optimization_rules()
        self.performance_baselines = self._load_baselines()
    
    def _load_optimization_rules(self) -> Dict:
        """åŠ è½½ä¼˜åŒ–è§„åˆ™"""
        return {
            'anti_patterns': [
                'SELECT *',
                'COUNT(*) with WHERE clause',
                'LIKE with leading wildcard',
                'Functions in WHERE clause',
                'Multiple subqueries'
            ],
            'best_practices': [
                'Use specific column names instead of *',
                'Add appropriate indexes',
                'Limit result sets',
                'Use EXPLAIN to analyze queries',
                'Consider query rewriting'
            ]
        }
    
    def _load_baselines(self) -> Dict:
        """åŠ è½½æ€§èƒ½åŸºçº¿"""
        return {
            'mysql': {'avg_query_time': 0.1, 'max_result_set': 1000},
            'postgresql': {'avg_query_time': 0.08, 'max_result_set': 1500},
            'mongodb': {'avg_query_time': 0.05, 'max_result_set': 2000},
            'redis': {'avg_operation_time': 0.001, 'max_batch_size': 10000}
        }
    
    def analyze_query(self, query: str, db_type: DatabaseType) -> QueryAnalysis:
        """åˆ†ææŸ¥è¯¢å¹¶æä¾›ä¼˜åŒ–å»ºè®®"""
        analysis = QueryAnalysis(
            query_text=query,
            database_type=db_type,
            complexity_score=0.0,
            optimization_suggestions=[],
            estimated_performance_gain=0.0,
            execution_plan_issues=[]
        )
        
        # è®¡ç®—å¤æ‚åº¦åˆ†æ•°
        analysis.complexity_score = self._calculate_complexity(query, db_type)
        
        # è¯†åˆ«åæ¨¡å¼
        anti_patterns_found = self._detect_anti_patterns(query)
        if anti_patterns_found:
            analysis.optimization_suggestions.extend(anti_patterns_found)
        
        # ç”Ÿæˆä¼˜åŒ–å»ºè®®
        suggestions = self._generate_optimization_suggestions(query, db_type)
        analysis.optimization_suggestions.extend(suggestions)
        
        # ä¼°ç®—æ€§èƒ½æå‡
        analysis.estimated_performance_gain = self._estimate_performance_gain(
            analysis.complexity_score, len(analysis.optimization_suggestions)
        )
        
        return analysis
    
    def _calculate_complexity(self, query: str, db_type: DatabaseType) -> float:
        """è®¡ç®—æŸ¥è¯¢å¤æ‚åº¦"""
        score = 0.0
        
        # åŸºç¡€å¤æ‚åº¦è®¡ç®—
        if 'JOIN' in query.upper():
            score += 0.3
        if 'UNION' in query.upper():
            score += 0.2
        if 'SUBQUERY' in query.upper() or query.count('(') > 3:
            score += 0.25
        if 'GROUP BY' in query.upper():
            score += 0.15
        if 'ORDER BY' in query.upper():
            score += 0.1
        
        # æ•°æ®åº“ç‰¹å®šè°ƒæ•´
        if db_type == DatabaseType.MYSQL:
            if 'DISTINCT' in query.upper():
                score += 0.1
        elif db_type == DatabaseType.POSTGRESQL:
            if 'WINDOW' in query.upper() or 'OVER' in query.upper():
                score += 0.2
        
        return min(score, 1.0)
    
    def _detect_anti_patterns(self, query: str) -> List[str]:
        """æ£€æµ‹æŸ¥è¯¢åæ¨¡å¼"""
        suggestions = []
        
        query_upper = query.upper()
        
        if 'SELECT *' in query_upper:
            suggestions.append("é¿å…ä½¿ç”¨SELECT *ï¼Œæ˜ç¡®æŒ‡å®šéœ€è¦çš„åˆ—")
        
        if 'COUNT(*)' in query_upper and 'WHERE' in query_upper:
            suggestions.append("å¯¹å¸¦æœ‰WHEREæ¡ä»¶çš„COUNT(*)è€ƒè™‘ä½¿ç”¨ç´¢å¼•ä¼˜åŒ–")
        
        if 'LIKE \'%\' in query_upper:
            suggestions.append("é¿å…å‰å¯¼é€šé…ç¬¦çš„LIKEæŸ¥è¯¢ï¼Œè¿™ä¼šå¯¼è‡´å…¨è¡¨æ‰«æ")
        
        # æ£€æµ‹å‡½æ•°åœ¨WHEREå­å¥ä¸­
        functions = ['UPPER(', 'LOWER(', 'DATE(', 'YEAR(', 'MONTH(']
        for func in functions:
            if func in query_upper:
                suggestions.append(f"é¿å…åœ¨WHEREå­å¥ä¸­ä½¿ç”¨{func.rstrip('(')}å‡½æ•°")
        
        return suggestions
    
    def _generate_optimization_suggestions(self, query: str, db_type: DatabaseType) -> List[str]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        suggestions = []
        
        # é€šç”¨å»ºè®®
        suggestions.append("ä½¿ç”¨EXPLAINåˆ†ææŸ¥è¯¢æ‰§è¡Œè®¡åˆ’")
        suggestions.append("ä¸ºç»å¸¸æŸ¥è¯¢çš„åˆ—æ·»åŠ é€‚å½“çš„ç´¢å¼•")
        
        # æ•°æ®åº“ç‰¹å®šå»ºè®®
        if db_type == DatabaseType.MYSQL:
            suggestions.append("è€ƒè™‘ä½¿ç”¨æŸ¥è¯¢ç¼“å­˜ï¼ˆé€‚ç”¨äºè¯»å¤šå†™å°‘åœºæ™¯ï¼‰")
            suggestions.append("ä¼˜åŒ–JOINé¡ºåºï¼Œå°†é€‰æ‹©æ€§é«˜çš„è¡¨æ”¾åœ¨å‰é¢")
        elif db_type == DatabaseType.POSTGRESQL:
            suggestions.append("è€ƒè™‘ä½¿ç”¨ç‰©åŒ–è§†å›¾é¢„è®¡ç®—å¤æ‚æŸ¥è¯¢")
            suggestions.append("ä½¿ç”¨CTEæ”¹å–„å¤æ‚æŸ¥è¯¢çš„å¯è¯»æ€§å’Œæ€§èƒ½")
        elif db_type == DatabaseType.MONGODB:
            suggestions.append("ä½¿ç”¨æŠ•å½±å‡å°‘è¿”å›çš„å­—æ®µæ•°é‡")
            suggestions.append("è€ƒè™‘ä½¿ç”¨èšåˆç®¡é“æ›¿ä»£å¤šä¸ªæŸ¥è¯¢")
        elif db_type == DatabaseType.REDIS:
            suggestions.append("ä½¿ç”¨å“ˆå¸Œç»“æ„åˆå¹¶ç›¸å…³é”®å€¼")
            suggestions.append("åˆç†ä½¿ç”¨ç®¡é“æ‰¹é‡æ“ä½œ")
        
        return suggestions
    
    def _estimate_performance_gain(self, complexity: float, suggestion_count: int) -> float:
        """ä¼°ç®—æ€§èƒ½æå‡ç™¾åˆ†æ¯”"""
        base_gain = complexity * 30  # åŸºç¡€æå‡
        suggestion_bonus = suggestion_count * 5  # å»ºè®®å¥–åŠ±
        return min(base_gain + suggestion_bonus, 90)  # æœ€å¤§90%æå‡

# ä½¿ç”¨ç¤ºä¾‹
def main():
    optimizer = QueryOptimizer()
    
    # æµ‹è¯•æŸ¥è¯¢
    test_queries = [
        ("SELECT * FROM users WHERE created_at > '2024-01-01'", DatabaseType.MYSQL),
        ("SELECT u.name, COUNT(o.id) FROM users u LEFT JOIN orders o ON u.id = o.user_id GROUP BY u.id", DatabaseType.POSTGRESQL),
        ("db.users.find({createdAt: {$gt: new Date('2024-01-01')}})", DatabaseType.MONGODB),
        ("GET user:1001:profile", DatabaseType.REDIS)
    ]
    
    for query, db_type in test_queries:
        analysis = optimizer.analyze_query(query, db_type)
        print(f"\n{db_type.value.upper()} æŸ¥è¯¢åˆ†æ:")
        print(f"å¤æ‚åº¦åˆ†æ•°: {analysis.complexity_score:.2f}")
        print(f"é¢„è®¡æ€§èƒ½æå‡: {analysis.estimated_performance_gain:.1f}%")
        print("ä¼˜åŒ–å»ºè®®:")
        for suggestion in analysis.optimization_suggestions:
            print(f"  - {suggestion}")

if __name__ == "__main__":
    main()
```

## ğŸ§ª æ€§èƒ½ä¼˜åŒ–éªŒè¯æµ‹è¯•

### è‡ªåŠ¨åŒ–ä¼˜åŒ–æ•ˆæœæµ‹è¯•
```bash
#!/bin/bash
# æŸ¥è¯¢ä¼˜åŒ–æ•ˆæœéªŒè¯æµ‹è¯•å¥—ä»¶

TEST_RESULTS=()

# MySQLæŸ¥è¯¢ä¼˜åŒ–æµ‹è¯•
test_mysql_query_optimization() {
  echo "=== MySQLæŸ¥è¯¢ä¼˜åŒ–æµ‹è¯• ==="
  
  # åˆ›å»ºæµ‹è¯•è¡¨å’Œæ•°æ®
  mysql -u root -p << EOF
DROP DATABASE IF EXISTS optimization_test;
CREATE DATABASE optimization_test;
USE optimization_test;

CREATE TABLE users (
  id INT PRIMARY KEY AUTO_INCREMENT,
  name VARCHAR(100),
  email VARCHAR(100),
  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE orders (
  id INT PRIMARY KEY AUTO_INCREMENT,
  user_id INT,
  order_date DATE,
  status VARCHAR(20),
  total_amount DECIMAL(10,2),
  FOREIGN KEY (user_id) REFERENCES users(id)
);

-- æ’å…¥æµ‹è¯•æ•°æ®
INSERT INTO users (name, email, created_at) 
SELECT CONCAT('User', seq), CONCAT('user', seq, '@example.com'), DATE_SUB(NOW(), INTERVAL FLOOR(RAND() * 365) DAY)
FROM seq_1_to_10000;

INSERT INTO orders (user_id, order_date, status, total_amount)
SELECT FLOOR(RAND() * 10000) + 1, DATE_SUB(NOW(), INTERVAL FLOOR(RAND() * 365) DAY), 
       ELT(FLOOR(RAND() * 3) + 1, 'pending', 'completed', 'cancelled'), RAND() * 1000
FROM seq_1_to_50000;
EOF
  
  # æµ‹è¯•åŸå§‹æŸ¥è¯¢æ€§èƒ½
  echo "æµ‹è¯•åŸå§‹æŸ¥è¯¢æ€§èƒ½..."
  local start_time=$(date +%s%N)
  mysql -u root -p optimization_test << EOF > /dev/null
SELECT u.name, COUNT(o.id) as order_count, SUM(o.total_amount) as total_spent
FROM users u
LEFT JOIN orders o ON u.id = o.user_id
WHERE u.created_at > DATE_SUB(NOW(), INTERVAL 1 YEAR)
GROUP BY u.id, u.name
HAVING order_count > 5
ORDER BY total_spent DESC
LIMIT 100;
EOF
  local end_time=$(date +%s%N)
  local original_time=$((($end_time - $start_time) / 1000000))
  
  # åˆ›å»ºç´¢å¼•
  mysql -u root -p optimization_test << EOF
CREATE INDEX idx_users_created_at ON users(created_at);
CREATE INDEX idx_orders_user_date ON orders(user_id, order_date);
EOF
  
  # æµ‹è¯•ä¼˜åŒ–åæŸ¥è¯¢æ€§èƒ½
  echo "æµ‹è¯•ä¼˜åŒ–åæŸ¥è¯¢æ€§èƒ½..."
  start_time=$(date +%s%N)
  mysql -u root -p optimization_test << EOF > /dev/null
SELECT u.name, user_stats.order_count, user_stats.total_spent
FROM users u
JOIN (
  SELECT user_id, COUNT(*) as order_count, SUM(total_amount) as total_spent
  FROM orders 
  WHERE order_date > DATE_SUB(NOW(), INTERVAL 1 YEAR)
  GROUP BY user_id
  HAVING COUNT(*) > 5
) user_stats ON u.id = user_stats.user_id
WHERE u.created_at > DATE_SUB(NOW(), INTERVAL 1 YEAR)
ORDER BY user_stats.total_spent DESC
LIMIT 100;
EOF
  end_time=$(date +%s%N)
  local optimized_time=$((($end_time - $start_time) / 1000000))
  
  local improvement_pct=$((($original_time - $optimized_time) * 100 / $original_time))
  
  if [ $improvement_pct -gt 30 ]; then
    TEST_RESULTS+=("MySQLæŸ¥è¯¢ä¼˜åŒ–æµ‹è¯•: é€šè¿‡ (æ€§èƒ½æå‡${improvement_pct}%)")
    echo "âœ… MySQLæŸ¥è¯¢ä¼˜åŒ–æˆåŠŸï¼Œæ€§èƒ½æå‡${improvement_pct}%"
  else
    TEST_RESULTS+=("MySQLæŸ¥è¯¢ä¼˜åŒ–æµ‹è¯•: å¤±è´¥ (æ€§èƒ½æå‡${improvement_pct}%)")
    echo "âŒ MySQLæŸ¥è¯¢ä¼˜åŒ–æ•ˆæœä¸ä½³ï¼Œä»…æå‡${improvement_pct}%"
  fi
}

# PostgreSQLæŸ¥è¯¢ä¼˜åŒ–æµ‹è¯•
test_postgresql_query_optimization() {
  echo "=== PostgreSQLæŸ¥è¯¢ä¼˜åŒ–æµ‹è¯• ==="
  
  # åˆ›å»ºæµ‹è¯•ç¯å¢ƒ
  psql -U postgres << EOF
DROP DATABASE IF EXISTS optimization_test;
CREATE DATABASE optimization_test;
\c optimization_test

CREATE TABLE users (
  id SERIAL PRIMARY KEY,
  name VARCHAR(100),
  email VARCHAR(100),
  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE orders (
  id SERIAL PRIMARY KEY,
  user_id INTEGER REFERENCES users(id),
  order_date DATE,
  status VARCHAR(20),
  total_amount DECIMAL(10,2)
);

-- æ’å…¥æµ‹è¯•æ•°æ®
INSERT INTO users (name, email, created_at)
SELECT 'User' || seq, 'user' || seq || '@example.com', CURRENT_DATE - (RANDOM() * 365)::INTEGER
FROM generate_series(1, 10000) seq;

INSERT INTO orders (user_id, order_date, status, total_amount)
SELECT (RANDOM() * 10000)::INTEGER + 1, CURRENT_DATE - (RANDOM() * 365)::INTEGER,
       CASE (RANDOM() * 3)::INTEGER 
         WHEN 0 THEN 'pending'
         WHEN 1 THEN 'completed'
         ELSE 'cancelled'
       END,
       RANDOM() * 1000
FROM generate_series(1, 50000);
EOF
  
  # æµ‹è¯•åŸå§‹æŸ¥è¯¢æ€§èƒ½
  echo "æµ‹è¯•åŸå§‹æŸ¥è¯¢æ€§èƒ½..."
  local original_time=$(psql -U postgres -d optimization_test -t -A -c "
    EXPLAIN (ANALYZE, TIMING)
    SELECT u.name, COUNT(o.id) as order_count, SUM(o.total_amount) as total_spent
    FROM users u
    LEFT JOIN orders o ON u.id = o.user_id
    WHERE u.created_at > CURRENT_DATE - INTERVAL '1 year'
    GROUP BY u.id, u.name
    HAVING COUNT(o.id) > 5
    ORDER BY total_spent DESC
    LIMIT 100;
  " | grep 'Execution Time' | awk '{print $3}')
  
  # åˆ›å»ºç´¢å¼•
  psql -U postgres -d optimization_test << EOF
CREATE INDEX idx_users_created_at ON users(created_at);
CREATE INDEX idx_orders_user_date_status ON orders(user_id, order_date, status);
EOF
  
  # æµ‹è¯•ä¼˜åŒ–åæŸ¥è¯¢æ€§èƒ½
  echo "æµ‹è¯•ä¼˜åŒ–åæŸ¥è¯¢æ€§èƒ½..."
  local optimized_time=$(psql -U postgres -d optimization_test -t -A -c "
    EXPLAIN (ANALYZE, TIMING)
    WITH user_order_stats AS (
      SELECT user_id, COUNT(*) as order_count, SUM(total_amount) as total_spent
      FROM orders
      WHERE order_date > CURRENT_DATE - INTERVAL '1 year'
      GROUP BY user_id
      HAVING COUNT(*) > 5
    )
    SELECT u.name, uos.order_count, uos.total_spent
    FROM users u
    JOIN user_order_stats uos ON u.id = uos.user_id
    WHERE u.created_at > CURRENT_DATE - INTERVAL '1 year'
    ORDER BY uos.total_spent DESC
    LIMIT 100;
  " | grep 'Execution Time' | awk '{print $3}')
  
  local improvement_pct=$(echo "scale=2; (($original_time - $optimized_time) / $original_time) * 100" | bc)
  
  if (( $(echo "$improvement_pct > 30" | bc -l) )); then
    TEST_RESULTS+=("PostgreSQLæŸ¥è¯¢ä¼˜åŒ–æµ‹è¯•: é€šè¿‡ (æ€§èƒ½æå‡${improvement_pct}%)")
    echo "âœ… PostgreSQLæŸ¥è¯¢ä¼˜åŒ–æˆåŠŸï¼Œæ€§èƒ½æå‡${improvement_pct}%"
  else
    TEST_RESULTS+=("PostgreSQLæŸ¥è¯¢ä¼˜åŒ–æµ‹è¯•: å¤±è´¥ (æ€§èƒ½æå‡${improvement_pct}%)")
    echo "âŒ PostgreSQLæŸ¥è¯¢ä¼˜åŒ–æ•ˆæœä¸ä½³ï¼Œä»…æå‡${improvement_pct}%"
  fi
}

# ç”Ÿæˆä¼˜åŒ–æµ‹è¯•æŠ¥å‘Š
generate_optimization_test_report() {
  echo "=== æŸ¥è¯¢ä¼˜åŒ–æµ‹è¯•ç»¼åˆæŠ¥å‘Š ==="
  
  local total_tests=${#TEST_RESULTS[@]}
  local passed_tests=0
  
  for result in "${TEST_RESULTS[@]}"; do
    echo "$result"
    if [[ $result == *"é€šè¿‡"* ]]; then
      ((passed_tests++))
    fi
  done
  
  echo ""
  echo "æµ‹è¯•æ€»ç»“:"
  echo "æ€»æµ‹è¯•é¡¹: $total_tests"
  echo "é€šè¿‡é¡¹: $passed_tests"
  echo "é€šè¿‡ç‡: $((passed_tests * 100 / total_tests))%"
  
  # ä¿å­˜æŠ¥å‘Š
  local report_file="/tmp/query_optimization_report_$(date +%Y%m%d_%H%M%S).txt"
  printf "%s\n" "${TEST_RESULTS[@]}" > "$report_file"
  echo "è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: $report_file"
}

# æ‰§è¡Œæ‰€æœ‰æµ‹è¯•
test_mysql_query_optimization
test_postgresql_query_optimization
generate_optimization_test_report
```

## ğŸ“š æœ€ä½³å®è·µæ€»ç»“

### æŸ¥è¯¢ä¼˜åŒ–æ ¸å¿ƒåŸåˆ™
1. **é¢„é˜²ä¼˜äºæ²»ç–—**: åœ¨è®¾è®¡é˜¶æ®µå°±è€ƒè™‘æ€§èƒ½å› ç´ 
2. **æµ‹é‡é©±åŠ¨ä¼˜åŒ–**: åŸºäºå®é™…æ€§èƒ½æ•°æ®è¿›è¡Œä¼˜åŒ–å†³ç­–
3. **æ¸è¿›å¼ä¼˜åŒ–**: ä»æœ€å½±å“æ€§èƒ½çš„åœ°æ–¹å¼€å§‹ä¼˜åŒ–
4. **æˆæœ¬æ•ˆç›Šå¹³è¡¡**: è€ƒè™‘ä¼˜åŒ–æŠ•å…¥ä¸æ”¶ç›Šçš„æ¯”ä¾‹
5. **æŒç»­ç›‘æ§**: ä¼˜åŒ–æ˜¯ä¸€ä¸ªæŒç»­çš„è¿‡ç¨‹ï¼Œéœ€è¦ä¸æ–­ç›‘æ§å’Œè°ƒæ•´

### å¸¸è§ä¼˜åŒ–ç­–ç•¥ä¼˜å…ˆçº§
1. **ç´¢å¼•ä¼˜åŒ–** (æœ€é«˜ä¼˜å…ˆçº§) - è§£å†³å¤§éƒ¨åˆ†æ€§èƒ½é—®é¢˜
2. **æŸ¥è¯¢é‡å†™** (é«˜ä¼˜å…ˆçº§) - æ”¹å˜æŸ¥è¯¢ç»“æ„æå‡æ•ˆç‡
3. **ç»Ÿè®¡ä¿¡æ¯æ›´æ–°** (ä¸­ç­‰ä¼˜å…ˆçº§) - ç¡®ä¿ä¼˜åŒ–å™¨åšå‡ºæ­£ç¡®å†³ç­–
4. **é…ç½®å‚æ•°è°ƒæ•´** (ä¸­ç­‰ä¼˜å…ˆçº§) - ä¼˜åŒ–ç³»ç»Ÿèµ„æºé…ç½®
5. **æ¶æ„é‡æ„** (ä½ä¼˜å…ˆçº§) - é‡å¤§ç»“æ„è°ƒæ•´

### æ€§èƒ½ç›‘æ§è¦ç‚¹
- **å“åº”æ—¶é—´**: å…³æ³¨95thå’Œ99thç™¾åˆ†ä½å“åº”æ—¶é—´
- **ååé‡**: æ¯ç§’å¤„ç†çš„æŸ¥è¯¢æ•°é‡
- **èµ„æºåˆ©ç”¨ç‡**: CPUã€å†…å­˜ã€ç£ç›˜I/Oä½¿ç”¨æƒ…å†µ
- **ç¼“å­˜å‘½ä¸­ç‡**: å„çº§ç¼“å­˜çš„æ•ˆç‡æŒ‡æ ‡
- **é”™è¯¯ç‡**: æŸ¥è¯¢å¤±è´¥å’Œè¶…æ—¶çš„æ¯”ä¾‹

---
> **ğŸ’¡ æç¤º**: æŸ¥è¯¢ä¼˜åŒ–éœ€è¦ç†è®ºçŸ¥è¯†å’Œå®è·µç»éªŒç›¸ç»“åˆï¼Œå»ºè®®åœ¨æµ‹è¯•ç¯å¢ƒä¸­å……åˆ†éªŒè¯åå†åº”ç”¨åˆ°ç”Ÿäº§ç¯å¢ƒã€‚