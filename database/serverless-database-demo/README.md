# Serverlessæ•°æ®åº“å®è·µå®Œæ•´æŒ‡å—

## ğŸ¯ æ¦‚è¿°

Serverlessæ•°æ®åº“ä½œä¸ºæ— æœåŠ¡å™¨è®¡ç®—çš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼Œé€šè¿‡æŒ‰éœ€ä»˜è´¹ã€è‡ªåŠ¨æ‰©ç¼©å®¹å’Œäº‹ä»¶é©±åŠ¨çš„ç‰¹æ€§ï¼Œä¸ºç°ä»£åº”ç”¨æä¾›äº†æè‡´çš„å¼¹æ€§å’Œæˆæœ¬æ•ˆç›Šã€‚æœ¬æŒ‡å—æ·±å…¥è§£æAWS Aurora Serverlessã€Azure Cosmos DB Serverlesså’ŒGoogle Cloud Firestoreç­‰ä¸»æµServerlessæ•°æ®åº“æœåŠ¡ï¼Œå¸®åŠ©ä¼ä¸šæ„å»ºé«˜æ•ˆã€ç»æµçš„æ•°æ®åº“è§£å†³æ–¹æ¡ˆã€‚

## ğŸ“‹ ç›®å½•

1. [Serverlessæ•°æ®åº“åŸºç¡€ç†è®º](#1-serverlessæ•°æ®åº“åŸºç¡€ç†è®º)
2. [ä¸»æµServerlessæ•°æ®åº“æœåŠ¡](#2-ä¸»æµserverlessæ•°æ®åº“æœåŠ¡)
3. [æ¶æ„è®¾è®¡ä¸æœ€ä½³å®è·µ](#3-æ¶æ„è®¾è®¡ä¸æœ€ä½³å®è·µ)
4. [æˆæœ¬ä¼˜åŒ–ä¸æ€§èƒ½è°ƒä¼˜](#4-æˆæœ¬ä¼˜åŒ–ä¸æ€§èƒ½è°ƒä¼˜)
5. [ç›‘æ§å‘Šè­¦ä¸æ•…éšœæ’æŸ¥](#5-ç›‘æ§å‘Šè­¦ä¸æ•…éšœæ’æŸ¥)
6. [å®é™…åº”ç”¨æ¡ˆä¾‹](#6-å®é™…åº”ç”¨æ¡ˆä¾‹)

---

## 1. Serverlessæ•°æ®åº“åŸºç¡€ç†è®º

### 1.1 Serverlessæ•°æ®åº“æ ¸å¿ƒæ¦‚å¿µ

#### Serverlessæ•°æ®åº“æ¶æ„æ¼”è¿›
```mermaid
graph TD
    A[ä¼ ç»Ÿæ•°æ®åº“] --> B[äº‘æ•°æ®åº“]
    B --> C[æ‰˜ç®¡æ•°æ®åº“]
    C --> D[è‡ªåŠ¨æ‰©ç¼©å®¹æ•°æ®åº“]
    D --> E[Serverlessæ•°æ®åº“]
    
    subgraph "ä¼ ç»Ÿæ•°æ®åº“æ—¶ä»£"
        A1[è‡ªå»ºæœåŠ¡å™¨]
        A2[æ‰‹åŠ¨è¿ç»´]
        A3[å›ºå®šèµ„æºé…ç½®]
    end
    
    subgraph "äº‘æ•°æ®åº“æ—¶ä»£"
        B1[IaaSéƒ¨ç½²]
        B2[åŠè‡ªåŠ¨åŒ–è¿ç»´]
        B3[é¢„é…ç½®èµ„æº]
    end
    
    subgraph "æ‰˜ç®¡æ•°æ®åº“æ—¶ä»£"
        C1[å®Œå…¨æ‰˜ç®¡]
        C2[è‡ªåŠ¨åŒ–å¤‡ä»½]
        C3[é¢„è®¾è§„æ ¼]
    end
    
    subgraph "è‡ªåŠ¨æ‰©ç¼©å®¹æ—¶ä»£"
        D1[è‡ªåŠ¨ä¼¸ç¼©]
        D2[æŒ‰é‡è®¡è´¹]
        D3[é¢„é…ç½®èŒƒå›´]
    end
    
    subgraph "Serverlessæ—¶ä»£"
        E1[æŒ‰éœ€åˆ†é…]
        E2[æ¯«ç§’çº§è®¡è´¹]
        E3[é›¶ç®¡ç†è¿ç»´]
    end
    
    A --> A1
    A --> A2
    A --> A3
    B --> B1
    B --> B2
    B --> B3
    C --> C1
    C --> C2
    C --> C3
    D --> D1
    D --> D2
    D --> D3
    E --> E1
    E --> E2
    E --> E3
```

#### Serverlessæ•°æ®åº“ç‰¹æ€§åˆ†æ
```python
# serverless_database_analyzer.py
from typing import Dict, List, Any
from dataclasses import dataclass
from enum import Enum

class DatabaseType(Enum):
    RELATIONAL = "relational"
    DOCUMENT = "document"
    KEY_VALUE = "key_value"
    WIDE_COLUMN = "wide_column"
    GRAPH = "graph"
    TIME_SERIES = "time_series"

@dataclass
class ServerlessFeature:
    name: str
    description: str
    benefit: str
    limitation: str
   é€‚ç”¨åœºæ™¯: List[str]

@dataclass
class ProviderComparison:
    provider: str
    service_name: str
    database_type: DatabaseType
    auto_scaling: bool
    pay_per_use: bool
    cold_start_time: str
    max_connections: int
    storage_limits: str
    pricing_model: str

class ServerlessDatabaseAnalyzer:
    def __init__(self):
        self.features = self._initialize_features()
        self.providers = self._initialize_providers()
    
    def _initialize_features(self) -> List[ServerlessFeature]:
        """åˆå§‹åŒ–Serverlessæ•°æ®åº“æ ¸å¿ƒç‰¹æ€§"""
        return [
            ServerlessFeature(
                name="è‡ªåŠ¨æ‰©ç¼©å®¹",
                description="æ ¹æ®è´Ÿè½½è‡ªåŠ¨è°ƒæ•´è®¡ç®—èµ„æº",
                benefit="æ— éœ€é¢„é…ç½®å®¹é‡ï¼Œèµ„æºåˆ©ç”¨ç‡æœ€å¤§åŒ–",
                limitation="å¯èƒ½å­˜åœ¨å†·å¯åŠ¨å»¶è¿Ÿ",
                é€‚ç”¨åœºæ™¯=["çªå‘æµé‡", "ä¸å¯é¢„æµ‹çš„å·¥ä½œè´Ÿè½½", "æˆæœ¬æ•æ„Ÿåº”ç”¨"]
            ),
            ServerlessFeature(
                name="æŒ‰éœ€ä»˜è´¹",
                description="åªä¸ºå®é™…ä½¿ç”¨çš„èµ„æºä»˜è´¹",
                benefit="æˆæœ¬é€æ˜ï¼Œæ— é—²ç½®èµ„æºæµªè´¹",
                limitation="é«˜æŒç»­è´Ÿè½½å¯èƒ½æ¯”é¢„ç•™å®ä¾‹è´µ",
                é€‚ç”¨åœºæ™¯=["é—´æ­‡æ€§å·¥ä½œè´Ÿè½½", "å¼€å‘æµ‹è¯•ç¯å¢ƒ", "åŸå‹éªŒè¯"]
            ),
            ServerlessFeature(
                name="é›¶ç®¡ç†è¿ç»´",
                description="å®Œå…¨æ‰˜ç®¡çš„æœåŠ¡ï¼Œæ— éœ€ç®¡ç†åº•å±‚åŸºç¡€è®¾æ–½",
                benefit="ä¸“æ³¨ä¸šåŠ¡é€»è¾‘ï¼Œé™ä½è¿ç»´å¤æ‚åº¦",
                limitation="å®šåˆ¶åŒ–èƒ½åŠ›å—é™",
                é€‚ç”¨åœºæ™¯=["å¿«é€Ÿäº§å“è¿­ä»£", "åˆåˆ›å…¬å¸", "æŠ€æœ¯å›¢é˜Ÿè¾ƒå°çš„ç»„ç»‡"]
            ),
            ServerlessFeature(
                name="äº‹ä»¶é©±åŠ¨é›†æˆ",
                description="ä¸å‡½æ•°è®¡ç®—ã€æ¶ˆæ¯é˜Ÿåˆ—ç­‰æœåŠ¡æ— ç¼é›†æˆ",
                benefit="æ„å»ºå“åº”å¼ã€æ¾è€¦åˆçš„ç³»ç»Ÿæ¶æ„",
                limitation="è°ƒè¯•å’Œç›‘æ§å¤æ‚åº¦å¢åŠ ",
                é€‚ç”¨åœºæ™¯=["å®æ—¶æ•°æ®å¤„ç†", "å¾®æœåŠ¡æ¶æ„", "IoTåº”ç”¨"]
            ),
            ServerlessFeature(
                name="å…¨çƒåˆ†å¸ƒ",
                description="å†…ç½®çš„å¤šåŒºåŸŸå¤åˆ¶å’Œå°±è¿‘è®¿é—®èƒ½åŠ›",
                benefit="ä½å»¶è¿Ÿå…¨çƒè®¿é—®ï¼Œé«˜å¯ç”¨æ€§ä¿éšœ",
                limitation="æ•°æ®åŒæ­¥å¯èƒ½å¸¦æ¥ä¸€è‡´æ€§æŒ‘æˆ˜",
                é€‚ç”¨åœºæ™¯=["å…¨çƒåŒ–åº”ç”¨", "å†…å®¹åˆ†å‘", "å¤šåœ°åŠå…¬ç³»ç»Ÿ"]
            )
        ]
    
    def _initialize_providers(self) -> List[ProviderComparison]:
        """åˆå§‹åŒ–ä¸»æµæä¾›å•†å¯¹æ¯”"""
        return [
            ProviderComparison(
                provider="AWS",
                service_name="Aurora Serverless v2",
                database_type=DatabaseType.RELATIONAL,
                auto_scaling=True,
                pay_per_use=True,
                cold_start_time="å‡ ç§’åˆ°å‡ åç§’",
                max_connections=200000,
                storage_limits="128TB",
                pricing_model="ACU*å°æ—¶ + å­˜å‚¨GB*æœˆ"
            ),
            ProviderComparison(
                provider="Azure",
                service_name="Cosmos DB Serverless",
                database_type=DatabaseType.DOCUMENT,
                auto_scaling=True,
                pay_per_use=True,
                cold_start_time="å‡ ä¹å³æ—¶",
                max_connections=æ— é™åˆ¶,
                storage_limits="æ— é™åˆ¶",
                pricing_model="æ¯ä¸‡æ¬¡RUæ¶ˆè€—"
            ),
            ProviderComparison(
                provider="Google Cloud",
                service_name="Firestore",
                database_type=DatabaseType.DOCUMENT,
                auto_scaling=True,
                pay_per_use=True,
                cold_start_time="å‡ ä¹å³æ—¶",
                max_connections=1000000,
                storage_limits="æ— é™åˆ¶",
                pricing_model="è¯»å†™æ“ä½œæ•° + å­˜å‚¨GB"
            ),
            ProviderComparison(
                provider="é˜¿é‡Œäº‘",
                service_name="PolarDB Serverless",
                database_type=DatabaseType.RELATIONAL,
                auto_scaling=True,
                pay_per_use=True,
                cold_start_time="å‡ ç§’",
                max_connections=100000,
                storage_limits="64TB",
                pricing_model="CU*å°æ—¶ + å­˜å‚¨GB*æœˆ"
            )
        ]
    
    def analyze_workload_suitability(self, workload_profile: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æå·¥ä½œè´Ÿè½½å¯¹Serverlessçš„é€‚ç”¨æ€§"""
        suitability_score = 0
        recommendations = []
        risk_factors = []
        
        # å·¥ä½œè´Ÿè½½ç‰¹å¾åˆ†æ
        workload_characteristics = {
            'burstiness': workload_profile.get('burstiness', 0.5),  # çªå‘æ€§ 0-1
            'predictability': workload_profile.get('predictability', 0.5),  # å¯é¢„æµ‹æ€§ 0-1
            'duration': workload_profile.get('average_duration', 300),  # å¹³å‡æŒç»­æ—¶é—´(ç§’)
            'concurrency': workload_profile.get('peak_concurrency', 100),  # å³°å€¼å¹¶å‘æ•°
            'consistency_requirement': workload_profile.get('consistency_requirement', 'eventual'),  # ä¸€è‡´æ€§è¦æ±‚
            'latency_sensitivity': workload_profile.get('latency_sensitivity', 'medium')  # å»¶è¿Ÿæ•æ„Ÿåº¦
        }
        
        # é€‚ç”¨æ€§è¯„åˆ†é€»è¾‘
        if workload_characteristics['burstiness'] > 0.7:
            suitability_score += 30
            recommendations.append("é«˜çªå‘æ€§å·¥ä½œè´Ÿè½½éå¸¸é€‚åˆServerlessæ¶æ„")
        elif workload_characteristics['burstiness'] < 0.3:
            suitability_score -= 20
            risk_factors.append("ä½çªå‘æ€§å·¥ä½œè´Ÿè½½å¯èƒ½äº§ç”Ÿè¾ƒé«˜æˆæœ¬")
        
        if workload_characteristics['predictability'] < 0.3:
            suitability_score += 25
            recommendations.append("ä¸å¯é¢„æµ‹çš„å·¥ä½œè´Ÿè½½æ˜¯Serverlessçš„ç†æƒ³åœºæ™¯")
        elif workload_characteristics['predictability'] > 0.8:
            suitability_score -= 15
            risk_factors.append("é«˜åº¦å¯é¢„æµ‹çš„å·¥ä½œè´Ÿè½½æ›´é€‚åˆé¢„ç•™å®ä¾‹")
        
        if workload_characteristics['duration'] < 900:  # 15åˆ†é’Ÿä»¥ä¸‹
            suitability_score += 20
            recommendations.append("çŸ­æ—¶ä»»åŠ¡éå¸¸é€‚åˆæŒ‰éœ€ä»˜è´¹æ¨¡å¼")
        else:
            suitability_score -= 10
            risk_factors.append("é•¿æ—¶é—´è¿è¡Œçš„ä»»åŠ¡å¯èƒ½äº§ç”Ÿè¾ƒé«˜æˆæœ¬")
        
        if workload_characteristics['concurrency'] < 1000:
            suitability_score += 15
            recommendations.append("ä¸­ä½å¹¶å‘åœºæ™¯Serverlessè¡¨ç°ä¼˜å¼‚")
        else:
            risk_factors.append("é«˜å¹¶å‘åœºæ™¯éœ€è¦æ³¨æ„å†·å¯åŠ¨å’Œè¿æ¥é™åˆ¶")
        
        if workload_characteristics['consistency_requirement'] == 'eventual':
            suitability_score += 10
        else:
            risk_factors.append("å¼ºä¸€è‡´æ€§è¦æ±‚å¯èƒ½é™åˆ¶Serverlessé€‰æ‹©")
        
        # ç”Ÿæˆåˆ†æç»“æœ
        overall_suitability = "éå¸¸é€‚åˆ" if suitability_score > 60 else \
                            "é€‚åˆ" if suitability_score > 30 else \
                            "éœ€è¦è°¨æ…è¯„ä¼°" if suitability_score > 0 else "ä¸å¤ªé€‚åˆ"
        
        return {
            'suitability_score': max(0, suitability_score),
            'overall_suitability': overall_suitability,
            'recommendations': recommendations,
            'risk_factors': risk_factors,
            'workload_characteristics': workload_characteristics,
            'detailed_analysis': self._generate_detailed_analysis(workload_characteristics)
        }
    
    def _generate_detailed_analysis(self, characteristics: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆè¯¦ç»†åˆ†æ"""
        analysis = {}
        
        # æˆæœ¬åˆ†æ
        if characteristics['burstiness'] > 0.7 and characteristics['predictability'] < 0.3:
            analysis['cost_benefit'] = "æ˜¾è‘—èŠ‚çœæˆæœ¬ï¼Œé¿å…èµ„æºé—²ç½®"
        elif characteristics['duration'] > 900:
            analysis['cost_benefit'] = "é•¿æ—¶é—´è¿è¡Œå¯èƒ½æˆæœ¬è¾ƒé«˜ï¼Œéœ€è¯¦ç»†æµ‹ç®—"
        else:
            analysis['cost_benefit'] = "æˆæœ¬æ•ˆç›Šä¸­ç­‰ï¼Œéœ€è¦ä¸é¢„ç•™å®ä¾‹å¯¹æ¯”"
        
        # æ€§èƒ½åˆ†æ
        if characteristics['latency_sensitivity'] == 'high':
            analysis['performance_considerations'] = "éœ€è¦æ³¨æ„å†·å¯åŠ¨å»¶è¿Ÿå½±å“"
        else:
            analysis['performance_considerations'] = "æ€§èƒ½è¡¨ç°è‰¯å¥½ï¼Œå»¶è¿Ÿå½±å“å¯æ¥å—"
        
        # æ¶æ„å»ºè®®
        if characteristics['concurrency'] > 1000:
            analysis['architectural_recommendations'] = [
                "è€ƒè™‘ä½¿ç”¨è¿æ¥æ± ",
                "å®ç°è¯·æ±‚æ’é˜Ÿæœºåˆ¶",
                "è®¾è®¡ä¼˜é›…é™çº§ç­–ç•¥"
            ]
        else:
            analysis['architectural_recommendations'] = [
                "ç›´æ¥ä½¿ç”¨ServerlessæœåŠ¡",
                "å…³æ³¨APIè°ƒç”¨é¢‘ç‡",
                "åˆç†è®¾è®¡æ•°æ®è®¿é—®æ¨¡å¼"
            ]
        
        return analysis
    
    def compare_provider_suitability(self, workload_profile: Dict[str, Any]) -> List[Dict[str, Any]]:
        """æ¯”è¾ƒä¸åŒæä¾›å•†çš„é€‚ç”¨æ€§"""
        comparisons = []
        
        for provider in self.providers:
            score = self._calculate_provider_score(provider, workload_profile)
            comparisons.append({
                'provider': provider.provider,
                'service': provider.service_name,
                'suitability_score': score,
                'strengths': self._get_provider_strengths(provider),
                'weaknesses': self._get_provider_weaknesses(provider),
                'pricing_estimate': self._estimate_pricing(provider, workload_profile)
            })
        
        # æŒ‰å¾—åˆ†æ’åº
        comparisons.sort(key=lambda x: x['suitability_score'], reverse=True)
        return comparisons
    
    def _calculate_provider_score(self, provider: ProviderComparison, 
                                workload_profile: Dict[str, Any]) -> int:
        """è®¡ç®—æä¾›å•†é€‚ç”¨æ€§å¾—åˆ†"""
        score = 0
        
        # æ•°æ®åº“ç±»å‹åŒ¹é…åº¦
        required_type = workload_profile.get('database_type', 'document')
        if provider.database_type.value == required_type:
            score += 30
        elif provider.database_type in [DatabaseType.DOCUMENT, DatabaseType.KEY_VALUE]:
            score += 20  # æ–‡æ¡£å’Œé”®å€¼æ•°æ®åº“é€šç”¨æ€§è¾ƒå¼º
        
        # è‡ªåŠ¨æ‰©ç¼©å®¹èƒ½åŠ›
        if provider.auto_scaling:
            score += 15
        
        # æŒ‰éœ€ä»˜è´¹æ¨¡å¼
        if provider.pay_per_use:
            score += 15
        
        # å¹¶å‘å¤„ç†èƒ½åŠ›
        required_concurrency = workload_profile.get('peak_concurrency', 100)
        if provider.max_connections >= required_concurrency * 2:
            score += 20
        elif provider.max_connections >= required_concurrency:
            score += 10
        else:
            score -= 10  # å®¹é‡ä¸è¶³
        
        # å­˜å‚¨é™åˆ¶
        storage_needs = workload_profile.get('storage_gb', 100)
        if "æ— é™åˆ¶" in provider.storage_limits or int(provider.storage_limits.replace('TB', '')) * 1024 > storage_needs:
            score += 10
        else:
            score -= 20  # å­˜å‚¨ä¸è¶³
        
        return max(0, score)
    
    def _get_provider_strengths(self, provider: ProviderComparison) -> List[str]:
        """è·å–æä¾›å•†ä¼˜åŠ¿"""
        strengths = []
        if provider.provider == "AWS":
            strengths.extend(["ç”Ÿæ€å®Œå–„", "åŠŸèƒ½ä¸°å¯Œ", "ä¼ä¸šçº§æ”¯æŒ"])
        elif provider.provider == "Azure":
            strengths.extend(["ä¼ä¸šé›†æˆå¥½", "åˆè§„è®¤è¯å¤š", ".NETç”Ÿæ€å¼º"])
        elif provider.provider == "Google Cloud":
            strengths.extend(["æ€§èƒ½ä¼˜å¼‚", "å…¨çƒç½‘ç»œå¥½", "æœºå™¨å­¦ä¹ é›†æˆ"])
        elif provider.provider == "é˜¿é‡Œäº‘":
            strengths.extend(["æœ¬åœŸåŒ–å¥½", "ä»·æ ¼ä¼˜åŠ¿", "ä¸­æ–‡æ”¯æŒ"])
        return strengths
    
    def _get_provider_weaknesses(self, provider: ProviderComparison) -> List[str]:
        """è·å–æä¾›å•†åŠ£åŠ¿"""
        weaknesses = []
        if provider.cold_start_time != "å‡ ä¹å³æ—¶":
            weaknesses.append("å†·å¯åŠ¨å»¶è¿Ÿè¾ƒé•¿")
        if provider.provider in ["AWS", "é˜¿é‡Œäº‘"]:
            weaknesses.append("å­¦ä¹ æ›²çº¿è¾ƒé™¡")
        return weaknesses
    
    def _estimate_pricing(self, provider: ProviderComparison, 
                         workload_profile: Dict[str, Any]) -> Dict[str, float]:
        """ä¼°ç®—å®šä»·"""
        # ç®€åŒ–çš„å®šä»·ä¼°ç®—é€»è¾‘
        monthly_requests = workload_profile.get('monthly_requests', 1000000)
        average_request_duration = workload_profile.get('average_duration', 100)  # æ¯«ç§’
        storage_gb = workload_profile.get('storage_gb', 100)
        
        estimates = {}
        
        if provider.provider == "AWS":
            # Aurora Serverless v2 ä¼°ç®—
            acu_hours = (monthly_requests * average_request_duration / 1000 / 3600) * 0.5  # å‡è®¾50%åˆ©ç”¨ç‡
            compute_cost = acu_hours * 0.12  # $0.12 per ACU-hour
            storage_cost = storage_gb * 0.10  # $0.10 per GB-month
            estimates['monthly_cost'] = compute_cost + storage_cost
            estimates['cost_per_million_requests'] = (compute_cost / monthly_requests) * 1000000
            
        elif provider.provider == "Azure":
            # Cosmos DB Serverless ä¼°ç®—
            ru_consumption = monthly_requests * 10  # å‡è®¾æ¯æ¬¡è¯·æ±‚æ¶ˆè€—10 RU
            compute_cost = (ru_consumption / 10000) * 0.000016  # $0.000016 per RU
            storage_cost = storage_gb * 0.25  # $0.25 per GB-month
            estimates['monthly_cost'] = compute_cost + storage_cost
            estimates['cost_per_million_requests'] = (compute_cost / monthly_requests) * 1000000
            
        elif provider.provider == "Google Cloud":
            # Firestore ä¼°ç®—
            reads = monthly_requests * 0.7  # å‡è®¾70%æ˜¯è¯»æ“ä½œ
            writes = monthly_requests * 0.3  # 30%æ˜¯å†™æ“ä½œ
            read_cost = (reads / 100000) * 0.06  # $0.06 per 100,000 document reads
            write_cost = (writes / 100000) * 0.18  # $0.18 per 100,000 document writes
            storage_cost = storage_gb * 0.18  # $0.18 per GB-month
            estimates['monthly_cost'] = read_cost + write_cost + storage_cost
            estimates['cost_per_million_requests'] = ((read_cost + write_cost) / monthly_requests) * 1000000
            
        return estimates

# ä½¿ç”¨ç¤ºä¾‹
analyzer = ServerlessDatabaseAnalyzer()

# åˆ†æå·¥ä½œè´Ÿè½½é€‚ç”¨æ€§
workload_profile = {
    'burstiness': 0.8,
    'predictability': 0.2,
    'average_duration': 50,  # 50æ¯«ç§’
    'peak_concurrency': 500,
    'consistency_requirement': 'eventual',
    'latency_sensitivity': 'medium',
    'database_type': 'document',
    'monthly_requests': 2000000,
    'storage_gb': 200
}

# å·¥ä½œè´Ÿè½½é€‚ç”¨æ€§åˆ†æ
suitability = analyzer.analyze_workload_suitability(workload_profile)
print("å·¥ä½œè´Ÿè½½é€‚ç”¨æ€§åˆ†æ:")
print(f"é€‚ç”¨æ€§è¯„åˆ†: {suitability['suitability_score']}")
print(f"æ€»ä½“è¯„ä»·: {suitability['overall_suitability']}")
print(f"æ¨èå»ºè®®: {suitability['recommendations']}")
print(f"é£é™©å› ç´ : {suitability['risk_factors']}")

# æä¾›å•†å¯¹æ¯”åˆ†æ
provider_comparisons = analyzer.compare_provider_suitability(workload_profile)
print("\næä¾›å•†å¯¹æ¯”åˆ†æ:")
for comparison in provider_comparisons[:3]:  # æ˜¾ç¤ºå‰3å
    print(f"\n{comparison['provider']} {comparison['service']}:")
    print(f"  é€‚ç”¨æ€§å¾—åˆ†: {comparison['suitability_score']}")
    print(f"  æœˆåº¦é¢„ä¼°æˆæœ¬: ${comparison['pricing_estimate']['monthly_cost']:.2f}")
    print(f"  æ¯ç™¾ä¸‡è¯·æ±‚æˆæœ¬: ${comparison['pricing_estimate']['cost_per_million_requests']:.4f}")
```

### 1.2 Serverlessæ•°æ®åº“æ¶æ„æ¨¡å¼

#### å…¸å‹Serverlessåº”ç”¨æ¶æ„
```yaml
# serverless-architecture-patterns.yaml
architecture_patterns:
  event_driven_architecture:
    description: "åŸºäºäº‹ä»¶é©±åŠ¨çš„Serverlessæ¶æ„"
    components:
      - trigger_source: "API Gateway / HTTPè§¦å‘å™¨"
      - function_layer: "Function Compute / Lambdaå‡½æ•°"
      - database_layer: "Serverlessæ•°æ®åº“"
      - event_bus: "æ¶ˆæ¯é˜Ÿåˆ— / äº‹ä»¶æ€»çº¿"
    
    data_flow:
      - step: "1. ç”¨æˆ·è¯·æ±‚åˆ°è¾¾APIç½‘å…³"
      - step: "2. è§¦å‘ç›¸åº”çš„å‡½æ•°è®¡ç®—"
      - step: "3. å‡½æ•°è®¿é—®Serverlessæ•°æ®åº“"
      - step: "4. æ•°æ®åº“è‡ªåŠ¨æ‰©ç¼©å®¹å¤„ç†è¯·æ±‚"
      - step: "5. å“åº”è¿”å›ç»™ç”¨æˆ·"
    
    advantages:
      - "é›¶ç®¡ç†è¿ç»´"
      - "æŒ‰éœ€ä»˜è´¹ï¼Œæˆæœ¬ä¼˜åŒ–"
      - "è‡ªåŠ¨å¼¹æ€§æ‰©ç¼©å®¹"
      - "é«˜å¯ç”¨æ€§å’Œå®¹é”™æ€§"
    
    use_cases:
      - "Webåº”ç”¨åç«¯"
      - "ç§»åŠ¨åº”ç”¨API"
      - "IoTæ•°æ®å¤„ç†"
      - "å®æ—¶æ•°æ®æµå¤„ç†"

  microservices_architecture:
    description: "åŸºäºå¾®æœåŠ¡çš„Serverlessæ¶æ„"
    components:
      - service_mesh: "æœåŠ¡ç½‘æ ¼"
      - api_gateway: "ç»Ÿä¸€APIå…¥å£"
      - function_services: "å¤šä¸ªç‹¬ç«‹çš„å‡½æ•°æœåŠ¡"
      - serverless_databases: "å¤šä¸ªä¸“ç”¨æ•°æ®åº“å®ä¾‹"
      - message_queue: "å¼‚æ­¥é€šä¿¡é˜Ÿåˆ—"
    
    service_decomposition:
      - user_service:
          database: "ç”¨æˆ·ä¿¡æ¯æ•°æ®åº“"
          functions: ["ç”¨æˆ·æ³¨å†Œ", "ç”¨æˆ·ç™»å½•", "ç”¨æˆ·ä¿¡æ¯ç®¡ç†"]
      - order_service:
          database: "è®¢å•æ•°æ®åº“"
          functions: ["åˆ›å»ºè®¢å•", "æŸ¥è¯¢è®¢å•", "è®¢å•çŠ¶æ€æ›´æ–°"]
      - payment_service:
          database: "æ”¯ä»˜æ•°æ®åº“"
          functions: ["å¤„ç†æ”¯ä»˜", "é€€æ¬¾å¤„ç†", "å¯¹è´¦æœåŠ¡"]
    
    communication_patterns:
      - synchronous: "APIè°ƒç”¨ï¼Œç›´æ¥å“åº”"
      - asynchronous: "æ¶ˆæ¯é˜Ÿåˆ—ï¼Œå¼‚æ­¥å¤„ç†"
      - event_based: "äº‹ä»¶å‘å¸ƒè®¢é˜…æ¨¡å¼"

  data_pipeline_architecture:
    description: "æ•°æ®ç®¡é“å¤„ç†æ¶æ„"
    pipeline_stages:
      - ingestion: "æ•°æ®æ‘„å…¥å±‚"
        components: ["IoTè®¾å¤‡", "ç§»åŠ¨åº”ç”¨", "Webè¡¨å•"]
        processing: "æ•°æ®éªŒè¯å’Œåˆæ­¥å¤„ç†"
      
      - processing: "æ•°æ®å¤„ç†å±‚"
        components: ["æµå¤„ç†å‡½æ•°", "æ‰¹å¤„ç†ä½œä¸š"]
        processing: "æ•°æ®è½¬æ¢ã€æ¸…æ´—ã€èšåˆ"
      
      - storage: "æ•°æ®å­˜å‚¨å±‚"
        components: ["å®æ—¶æ•°æ®åº“", "æ•°æ®ä»“åº“", "å¯¹è±¡å­˜å‚¨"]
        processing: "ç»“æ„åŒ–å’Œéç»“æ„åŒ–æ•°æ®å­˜å‚¨"
      
      - analytics: "æ•°æ®åˆ†æå±‚"
        components: ["BIå·¥å…·", "æœºå™¨å­¦ä¹ æ¨¡å‹", "æŠ¥è¡¨ç³»ç»Ÿ"]
        processing: "æ•°æ®æ´å¯Ÿå’Œæ™ºèƒ½åˆ†æ"
    
    scaling_characteristics:
      - auto_scaling: "æ ¹æ®æ•°æ®æµé‡è‡ªåŠ¨è°ƒæ•´"
      - burst_handling: "å¤„ç†æ•°æ®å³°å€¼å’Œçªå‘æµé‡"
      - cost_optimization: "åªä¸ºå¤„ç†çš„æ•°æ®ä»˜è´¹"
```

## 2. ä¸»æµServerlessæ•°æ®åº“æœåŠ¡

### 2.1 AWS Aurora Serverless

#### Aurora Serverless v2é…ç½®ç¤ºä¾‹
```yaml
# aurora-serverless-v2-template.yaml
AWSTemplateFormatVersion: '2010-09-09'
Description: 'Aurora Serverless v2 Database Cluster'

Parameters:
  DBUsername:
    Type: String
    Default: admin
    Description: æ•°æ®åº“ç”¨æˆ·å
  
  DBPassword:
    Type: String
    NoEcho: true
    Description: æ•°æ®åº“å¯†ç 
  
  MinCapacity:
    Type: Number
    Default: 0.5
    Description: æœ€å°ACUå®¹é‡
  
  MaxCapacity:
    Type: Number
    Default: 32
    Description: æœ€å¤§ACUå®¹é‡

Resources:
  # Aurora Serverless v2é›†ç¾¤
  AuroraServerlessCluster:
    Type: AWS::RDS::DBCluster
    Properties:
      Engine: aurora-mysql
      EngineVersion: '8.0.mysql_aurora.3.02.0'
      DatabaseName: serverlessdb
      MasterUsername: !Ref DBUsername
      MasterUserPassword: !Ref DBPassword
      ServerlessV2ScalingConfiguration:
        MinCapacity: !Ref MinCapacity
        MaxCapacity: !Ref MaxCapacity
      BackupRetentionPeriod: 7
      StorageEncrypted: true
      DeletionProtection: false
      EnableHttpEndpoint: true  # å¯ç”¨HTTP APIç«¯ç‚¹
      
  # æ•°æ®åº“å®ä¾‹
  AuroraServerlessInstance:
    Type: AWS::RDS::DBInstance
    Properties:
      DBInstanceClass: db.serverless
      Engine: aurora-mysql
      DBClusterIdentifier: !Ref AuroraServerlessCluster
      PubliclyAccessible: false

  # IAMè§’è‰²ç”¨äºHTTP APIè®¿é—®
  AuroraAccessRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: 'lambda.amazonaws.com'
            Action: 'sts:AssumeRole'
      Policies:
        - PolicyName: AuroraServerlessAccess
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - 'rds-data:ExecuteStatement'
                  - 'rds-data:BatchExecuteStatement'
                  - 'rds-data:BeginTransaction'
                  - 'rds-data:CommitTransaction'
                  - 'rds-data:RollbackTransaction'
                Resource: !Sub 'arn:aws:rds:${AWS::Region}:${AWS::AccountId}:cluster:${AuroraServerlessCluster}'

Outputs:
  ClusterEndpoint:
    Description: Aurora Serverlessé›†ç¾¤ç«¯ç‚¹
    Value: !GetAtt AuroraServerlessCluster.Endpoint.Address
  
  HttpEndpoint:
    Description: HTTP APIç«¯ç‚¹
    Value: !Sub 'https://rds-data.${AWS::Region}.amazonaws.com'
  
  AccessRoleArn:
    Description: è®¿é—®è§’è‰²ARN
    Value: !GetAtt AuroraAccessRole.Arn
```

#### Pythonåº”ç”¨é›†æˆç¤ºä¾‹
```python
# aurora_serverless_client.py
import boto3
import json
from typing import Dict, List, Any, Optional
import time

class AuroraServerlessClient:
    def __init__(self, cluster_arn: str, secret_arn: str, database: str, region: str = 'us-east-1'):
        self.cluster_arn = cluster_arn
        self.secret_arn = secret_arn
        self.database = database
        self.region = region
        self.rds_data_client = boto3.client('rds-data', region_name=region)
    
    def execute_statement(self, sql: str, parameters: Optional[List[Dict]] = None) -> Dict[str, Any]:
        """æ‰§è¡ŒSQLè¯­å¥"""
        try:
            response = self.rds_data_client.execute_statement(
                resourceArn=self.cluster_arn,
                secretArn=self.secret_arn,
                database=self.database,
                sql=sql,
                parameters=parameters or []
            )
            return response
        except Exception as e:
            print(f"SQLæ‰§è¡Œå¤±è´¥: {str(e)}")
            raise
    
    def batch_execute(self, sql: str, parameter_sets: List[List[Dict]]) -> Dict[str, Any]:
        """æ‰¹é‡æ‰§è¡ŒSQLè¯­å¥"""
        try:
            response = self.rds_data_client.batch_execute_statement(
                resourceArn=self.cluster_arn,
                secretArn=self.secret_arn,
                database=self.database,
                sql=sql,
                parameterSets=parameter_sets
            )
            return response
        except Exception as e:
            print(f"æ‰¹é‡æ‰§è¡Œå¤±è´¥: {str(e)}")
            raise
    
    def begin_transaction(self) -> str:
        """å¼€å§‹äº‹åŠ¡"""
        response = self.rds_data_client.begin_transaction(
            resourceArn=self.cluster_arn,
            secretArn=self.secret_arn,
            database=self.database
        )
        return response['transactionId']
    
    def commit_transaction(self, transaction_id: str) -> Dict[str, Any]:
        """æäº¤äº‹åŠ¡"""
        response = self.rds_data_client.commit_transaction(
            resourceArn=self.cluster_arn,
            secretArn=self.secret_arn,
            transactionId=transaction_id
        )
        return response
    
    def rollback_transaction(self, transaction_id: str) -> Dict[str, Any]:
        """å›æ»šäº‹åŠ¡"""
        response = self.rds_data_client.rollback_transaction(
            resourceArn=self.cluster_arn,
            secretArn=self.secret_arn,
            transactionId=transaction_id
        )
        return response

# Lambdaå‡½æ•°ç¤ºä¾‹
def lambda_handler(event, context):
    """å¤„ç†APIè¯·æ±‚çš„Lambdaå‡½æ•°"""
    
    # åˆå§‹åŒ–Aurora Serverlesså®¢æˆ·ç«¯
    client = AuroraServerlessClient(
        cluster_arn='arn:aws:rds:us-east-1:123456789012:cluster:my-serverless-cluster',
        secret_arn='arn:aws:secretsmanager:us-east-1:123456789012:secret:my-db-secret',
        database='serverlessdb'
    )
    
    # è§£æè¯·æ±‚
    http_method = event['httpMethod']
    path = event['path']
    body = json.loads(event.get('body', '{}')) if event.get('body') else {}
    
    try:
        if http_method == 'GET' and path == '/users':
            # æŸ¥è¯¢ç”¨æˆ·åˆ—è¡¨
            result = client.execute_statement(
                "SELECT id, name, email FROM users WHERE active = :active",
                [{'name': 'active', 'value': {'booleanValue': True}}]
            )
            users = [
                {
                    'id': record[0]['longValue'],
                    'name': record[1]['stringValue'],
                    'email': record[2]['stringValue']
                }
                for record in result.get('records', [])
            ]
            return {
                'statusCode': 200,
                'body': json.dumps(users)
            }
        
        elif http_method == 'POST' and path == '/users':
            # åˆ›å»ºæ–°ç”¨æˆ·
            transaction_id = client.begin_transaction()
            
            try:
                # æ’å…¥ç”¨æˆ·
                client.execute_statement(
                    "INSERT INTO users (name, email, created_at) VALUES (:name, :email, NOW())",
                    [
                        {'name': 'name', 'value': {'stringValue': body['name']}},
                        {'name': 'email', 'value': {'stringValue': body['email']}}
                    ]
                )
                
                # è·å–æ’å…¥çš„ç”¨æˆ·ID
                result = client.execute_statement("SELECT LAST_INSERT_ID()")
                user_id = result['records'][0][0]['longValue']
                
                client.commit_transaction(transaction_id)
                
                return {
                    'statusCode': 201,
                    'body': json.dumps({'id': user_id, 'message': 'ç”¨æˆ·åˆ›å»ºæˆåŠŸ'})
                }
                
            except Exception as e:
                client.rollback_transaction(transaction_id)
                raise e
        
        else:
            return {
                'statusCode': 404,
                'body': json.dumps({'error': 'æœªæ‰¾åˆ°çš„è·¯ç”±'})
            }
            
    except Exception as e:
        return {
            'statusCode': 500,
            'body': json.dumps({'error': str(e)})
        }

# æ‰¹é‡æ•°æ®å¤„ç†ç¤ºä¾‹
def process_batch_data(event, context):
    """æ‰¹é‡å¤„ç†æ•°æ®çš„Lambdaå‡½æ•°"""
    
    client = AuroraServerlessClient(
        cluster_arn='arn:aws:rds:us-east-1:123456789012:cluster:my-serverless-cluster',
        secret_arn='arn:aws:secretsmanager:us-east-1:123456789012:secret:my-db-secret',
        database='serverlessdb'
    )
    
    # ä»SQSè·å–æ‰¹é‡æ•°æ®
    records = event['Records']
    
    # å‡†å¤‡æ‰¹é‡æ’å…¥å‚æ•°
    parameter_sets = []
    for record in records:
        body = json.loads(record['body'])
        parameter_sets.append([
            {'name': 'name', 'value': {'stringValue': body['name']}},
            {'name': 'email', 'value': {'stringValue': body['email']}},
            {'name': 'data', 'value': {'stringValue': json.dumps(body['data'])}}
        ])
    
    # æ‰¹é‡æ’å…¥
    client.batch_execute(
        "INSERT INTO user_events (name, email, event_data, created_at) VALUES (:name, :email, :data, NOW())",
        parameter_sets
    )
    
    return {'statusCode': 200, 'processedRecords': len(records)}
```

### 2.2 Azure Cosmos DB Serverless

#### Cosmos DB Serverlessé…ç½®
```json
{
  "$schema": "https://schema.management.azure.com/schemas/2019-04-01/deploymentTemplate.json#",
  "contentVersion": "1.0.0.0",
  "parameters": {
    "accountName": {
      "type": "string",
      "defaultValue": "[concat('cosmos-', uniqueString(resourceGroup().id))]",
      "metadata": {
        "description": "Cosmos DB account name"
      }
    },
    "databaseName": {
      "type": "string",
      "defaultValue": "serverless-db",
      "metadata": {
        "description": "Database name"
      }
    }
  },
  "resources": [
    {
      "type": "Microsoft.DocumentDB/databaseAccounts",
      "apiVersion": "2021-04-15",
      "name": "[parameters('accountName')]",
      "location": "[resourceGroup().location]",
      "properties": {
        "databaseAccountOfferType": "Standard",
        "consistencyPolicy": {
          "defaultConsistencyLevel": "Session"
        },
        "locations": [
          {
            "locationName": "[resourceGroup().location]",
            "failoverPriority": 0
          }
        ],
        "capabilities": [
          {
            "name": "EnableServerless"
          }
        ]
      }
    },
    {
      "type": "Microsoft.DocumentDB/databaseAccounts/sqlDatabases",
      "apiVersion": "2021-04-15",
      "name": "[concat(parameters('accountName'), '/', parameters('databaseName'))]",
      "dependsOn": [
        "[resourceId('Microsoft.DocumentDB/databaseAccounts', parameters('accountName'))]"
      ],
      "properties": {
        "resource": {
          "id": "[parameters('databaseName')]"
        }
      }
    },
    {
      "type": "Microsoft.DocumentDB/databaseAccounts/sqlDatabases/containers",
      "apiVersion": "2021-04-15",
      "name": "[concat(parameters('accountName'), '/', parameters('databaseName'), '/users')]",
      "dependsOn": [
        "[resourceId('Microsoft.DocumentDB/databaseAccounts/sqlDatabases', parameters('accountName'), parameters('databaseName'))]"
      ],
      "properties": {
        "resource": {
          "id": "users",
          "partitionKey": {
            "paths": ["/userId"],
            "kind": "Hash"
          },
          "indexingPolicy": {
            "indexingMode": "consistent",
            "automatic": true,
            "includedPaths": [
              {
                "path": "/*"
              }
            ],
            "excludedPaths": [
              {
                "path": "/_etag/?"
              }
            ]
          }
        }
      }
    }
  ],
  "outputs": {
    "connectionString": {
      "type": "string",
      "value": "[listConnectionStrings(resourceId('Microsoft.DocumentDB/databaseAccounts', parameters('accountName')), '2021-04-15').connectionStrings[0].connectionString]"
    }
  }
}
```

#### Azure Functionsé›†æˆç¤ºä¾‹
```python
# cosmos_db_functions.py
import azure.functions as func
import azure.cosmos.cosmos_client as cosmos_client
import azure.cosmos.exceptions as exceptions
import json
import os
import logging

class CosmosDBHandler:
    def __init__(self):
        self.endpoint = os.environ['COSMOS_DB_ENDPOINT']
        self.key = os.environ['COSMOS_DB_KEY']
        self.database_id = os.environ['COSMOS_DB_DATABASE']
        self.container_id = os.environ['COSMOS_DB_CONTAINER']
        
        self.client = cosmos_client.CosmosClient(self.endpoint, {'masterKey': self.key})
        self.database = self.client.get_database_client(self.database_id)
        self.container = self.database.get_container_client(self.container_id)
    
    def create_item(self, item: dict) -> dict:
        """åˆ›å»ºæ–‡æ¡£"""
        try:
            created_item = self.container.create_item(body=item)
            return created_item
        except exceptions.CosmosHttpResponseError as e:
            logging.error(f'åˆ›å»ºæ–‡æ¡£å¤±è´¥: {e.message}')
            raise
    
    def get_item(self, item_id: str, partition_key: str) -> dict:
        """è·å–æ–‡æ¡£"""
        try:
            item = self.container.read_item(item=item_id, partition_key=partition_key)
            return item
        except exceptions.CosmosResourceNotFoundError:
            return None
        except exceptions.CosmosHttpResponseError as e:
            logging.error(f'è·å–æ–‡æ¡£å¤±è´¥: {e.message}')
            raise
    
    def query_items(self, query: str, parameters: list = None) -> list:
        """æŸ¥è¯¢æ–‡æ¡£"""
        try:
            items = list(self.container.query_items(
                query=query,
                parameters=parameters,
                enable_cross_partition_query=True
            ))
            return items
        except exceptions.CosmosHttpResponseError as e:
            logging.error(f'æŸ¥è¯¢æ–‡æ¡£å¤±è´¥: {e.message}')
            raise
    
    def update_item(self, item_id: str, partition_key: str, updates: dict) -> dict:
        """æ›´æ–°æ–‡æ¡£"""
        try:
            # å…ˆè·å–ç°æœ‰æ–‡æ¡£
            existing_item = self.get_item(item_id, partition_key)
            if not existing_item:
                return None
            
            # åº”ç”¨æ›´æ–°
            existing_item.update(updates)
            
            # ä¿å­˜æ›´æ–°åçš„æ–‡æ¡£
            updated_item = self.container.upsert_item(body=existing_item)
            return updated_item
        except exceptions.CosmosHttpResponseError as e:
            logging.error(f'æ›´æ–°æ–‡æ¡£å¤±è´¥: {e.message}')
            raise

# HTTPè§¦å‘å™¨å‡½æ•°
def main(req: func.HttpRequest) -> func.HttpResponse:
    logging.info('Python HTTP trigger function processed a request.')
    
    # åˆå§‹åŒ–Cosmos DBå¤„ç†å™¨
    handler = CosmosDBHandler()
    
    try:
        if req.method == 'GET':
            # è·å–ç”¨æˆ·åˆ—è¡¨
            users = handler.query_items(
                "SELECT * FROM c WHERE c.type = @type",
                [{"name": "@type", "value": "user"}]
            )
            return func.HttpResponse(
                json.dumps(users),
                status_code=200,
                mimetype="application/json"
            )
        
        elif req.method == 'POST':
            # åˆ›å»ºæ–°ç”¨æˆ·
            req_body = req.get_json()
            
            user_document = {
                "id": req_body.get('userId'),
                "userId": req_body.get('userId'),
                "name": req_body.get('name'),
                "email": req_body.get('email'),
                "type": "user",
                "createdAt": func.datetime.utcnow().isoformat()
            }
            
            created_user = handler.create_item(user_document)
            
            return func.HttpResponse(
                json.dumps(created_user),
                status_code=201,
                mimetype="application/json"
            )
        
        elif req.method == 'PUT':
            # æ›´æ–°ç”¨æˆ·
            user_id = req.route_params.get('userId')
            if not user_id:
                return func.HttpResponse(
                    json.dumps({"error": "ç¼ºå°‘ç”¨æˆ·ID"}),
                    status_code=400,
                    mimetype="application/json"
                )
            
            req_body = req.get_json()
            updates = {k: v for k, v in req_body.items() if k != 'id' and k != 'userId'}
            
            updated_user = handler.update_item(user_id, user_id, updates)
            
            if updated_user:
                return func.HttpResponse(
                    json.dumps(updated_user),
                    status_code=200,
                    mimetype="application/json"
                )
            else:
                return func.HttpResponse(
                    json.dumps({"error": "ç”¨æˆ·ä¸å­˜åœ¨"}),
                    status_code=404,
                    mimetype="application/json"
                )
        
        else:
            return func.HttpResponse(
                json.dumps({"error": "ä¸æ”¯æŒçš„HTTPæ–¹æ³•"}),
                status_code=405,
                mimetype="application/json"
            )
            
    except Exception as e:
        logging.error(f'å¤„ç†è¯·æ±‚æ—¶å‡ºé”™: {str(e)}')
        return func.HttpResponse(
            json.dumps({"error": str(e)}),
            status_code=500,
            mimetype="application/json"
        )

# å®šæ—¶è§¦å‘å™¨å‡½æ•° - æ•°æ®æ¸…ç†
def cleanup_expired_data(mytimer: func.TimerRequest) -> None:
    utc_timestamp = func.datetime.utcnow().replace(tzinfo=None).isoformat()
    
    if mytimer.past_due:
        logging.info('å®šæ—¶å™¨è¿‡å»æ—¶é—´ï¼Œç«‹å³æ‰§è¡Œ')
    
    logging.info('Python timer trigger function ran at %s', utc_timestamp)
    
    # åˆå§‹åŒ–å¤„ç†å™¨
    handler = CosmosDBHandler()
    
    # æŸ¥è¯¢è¿‡æœŸæ•°æ®
    expired_items = handler.query_items(
        "SELECT * FROM c WHERE c.expiryDate < @currentDate",
        [{"name": "@currentDate", "value": utc_timestamp}]
    )
    
    # åˆ é™¤è¿‡æœŸæ•°æ®
    deleted_count = 0
    for item in expired_items:
        try:
            handler.container.delete_item(
                item=item['id'],
                partition_key=item['userId']
            )
            deleted_count += 1
        except Exception as e:
            logging.error(f'åˆ é™¤è¿‡æœŸé¡¹å¤±è´¥ {item["id"]}: {str(e)}')
    
    logging.info(f'æ¸…ç†äº† {deleted_count} ä¸ªè¿‡æœŸé¡¹')
```

### 2.3 Google Cloud Firestore

#### Firestoreé…ç½®å’Œåˆå§‹åŒ–
```python
# firestore_client.py
from google.cloud import firestore
from google.cloud.firestore_v1.base_query import FieldFilter
import json
from typing import Dict, List, Any, Optional
import logging

class FirestoreClient:
    def __init__(self, project_id: str = None):
        """åˆå§‹åŒ–Firestoreå®¢æˆ·ç«¯"""
        if project_id:
            self.client = firestore.Client(project=project_id)
        else:
            # ä½¿ç”¨é»˜è®¤é¡¹ç›®
            self.client = firestore.Client()
    
    def add_document(self, collection_name: str, data: Dict[str, Any], 
                    document_id: Optional[str] = None) -> str:
        """æ·»åŠ æ–‡æ¡£"""
        try:
            collection_ref = self.client.collection(collection_name)
            
            if document_id:
                doc_ref = collection_ref.document(document_id)
                doc_ref.set(data)
                return document_id
            else:
                doc_ref = collection_ref.add(data)
                return doc_ref[1].id
                
        except Exception as e:
            logging.error(f"æ·»åŠ æ–‡æ¡£å¤±è´¥: {str(e)}")
            raise
    
    def get_document(self, collection_name: str, document_id: str) -> Optional[Dict[str, Any]]:
        """è·å–æ–‡æ¡£"""
        try:
            doc_ref = self.client.collection(collection_name).document(document_id)
            doc = doc_ref.get()
            
            if doc.exists:
                return doc.to_dict()
            else:
                return None
                
        except Exception as e:
            logging.error(f"è·å–æ–‡æ¡£å¤±è´¥: {str(e)}")
            raise
    
    def update_document(self, collection_name: str, document_id: str, 
                       updates: Dict[str, Any]) -> bool:
        """æ›´æ–°æ–‡æ¡£"""
        try:
            doc_ref = self.client.collection(collection_name).document(document_id)
            doc_ref.update(updates)
            return True
        except Exception as e:
            logging.error(f"æ›´æ–°æ–‡æ¡£å¤±è´¥: {str(e)}")
            return False
    
    def delete_document(self, collection_name: str, document_id: str) -> bool:
        """åˆ é™¤æ–‡æ¡£"""
        try:
            doc_ref = self.client.collection(collection_name).document(document_id)
            doc_ref.delete()
            return True
        except Exception as e:
            logging.error(f"åˆ é™¤æ–‡æ¡£å¤±è´¥: {str(e)}")
            return False
    
    def query_documents(self, collection_name: str, 
                       filters: List[tuple] = None,
                       order_by: Optional[str] = None,
                       limit: Optional[int] = None) -> List[Dict[str, Any]]:
        """æŸ¥è¯¢æ–‡æ¡£"""
        try:
            collection_ref = self.client.collection(collection_name)
            query = collection_ref
            
            # åº”ç”¨è¿‡æ»¤å™¨
            if filters:
                for field, operator, value in filters:
                    query = query.where(field, operator, value)
            
            # æ’åº
            if order_by:
                query = query.order_by(order_by)
            
            # é™åˆ¶ç»“æœæ•°é‡
            if limit:
                query = query.limit(limit)
            
            # æ‰§è¡ŒæŸ¥è¯¢
            docs = query.stream()
            
            results = []
            for doc in docs:
                doc_dict = doc.to_dict()
                doc_dict['id'] = doc.id
                results.append(doc_dict)
            
            return results
            
        except Exception as e:
            logging.error(f"æŸ¥è¯¢æ–‡æ¡£å¤±è´¥: {str(e)}")
            raise
    
    def batch_write(self, operations: List[Dict[str, Any]]) -> List[str]:
        """æ‰¹é‡å†™å…¥æ“ä½œ"""
        try:
            batch = self.client.batch()
            created_ids = []
            
            for operation in operations:
                op_type = operation['type']
                collection = operation['collection']
                data = operation['data']
                
                collection_ref = self.client.collection(collection)
                
                if op_type == 'create':
                    if 'id' in operation:
                        doc_ref = collection_ref.document(operation['id'])
                        batch.set(doc_ref, data)
                        created_ids.append(operation['id'])
                    else:
                        doc_ref = collection_ref.document()
                        batch.set(doc_ref, data)
                        created_ids.append(doc_ref.id)
                
                elif op_type == 'update':
                    doc_ref = collection_ref.document(operation['id'])
                    batch.update(doc_ref, data)
                    created_ids.append(operation['id'])
                
                elif op_type == 'delete':
                    doc_ref = collection_ref.document(operation['id'])
                    batch.delete(doc_ref)
                    created_ids.append(operation['id'])
            
            # æ‰§è¡Œæ‰¹é‡æ“ä½œ
            batch.commit()
            return created_ids
            
        except Exception as e:
            logging.error(f"æ‰¹é‡å†™å…¥å¤±è´¥: {str(e)}")
            raise

# Cloud Functionsé›†æˆç¤ºä¾‹
def handle_firestore_request(request):
    """å¤„ç†Firestoreç›¸å…³è¯·æ±‚çš„Cloud Function"""
    
    # åˆå§‹åŒ–Firestoreå®¢æˆ·ç«¯
    db = FirestoreClient()
    
    # è§£æè¯·æ±‚
    request_json = request.get_json(silent=True)
    request_args = request.args
    
    if request.method == 'GET':
        # æŸ¥è¯¢ç”¨æˆ·æ•°æ®
        user_id = request_args.get('userId')
        
        if user_id:
            # è·å–ç‰¹å®šç”¨æˆ·
            user = db.get_document('users', user_id)
            if user:
                return {'user': user}, 200
            else:
                return {'error': 'ç”¨æˆ·ä¸å­˜åœ¨'}, 404
        else:
            # è·å–ç”¨æˆ·åˆ—è¡¨
            users = db.query_documents(
                'users',
                filters=[('active', '==', True)],
                order_by='createdAt',
                limit=100
            )
            return {'users': users}, 200
    
    elif request.method == 'POST':
        # åˆ›å»ºæ–°ç”¨æˆ·
        if not request_json:
            return {'error': 'è¯·æ±‚ä½“ä¸èƒ½ä¸ºç©º'}, 400
        
        user_data = {
            'name': request_json.get('name'),
            'email': request_json.get('email'),
            'active': True,
            'createdAt': firestore.SERVER_TIMESTAMP,
            'profile': request_json.get('profile', {})
        }
        
        user_id = db.add_document('users', user_data)
        return {'message': 'ç”¨æˆ·åˆ›å»ºæˆåŠŸ', 'userId': user_id}, 201
    
    elif request.method == 'PUT':
        # æ›´æ–°ç”¨æˆ·ä¿¡æ¯
        user_id = request_args.get('userId')
        if not user_id:
            return {'error': 'ç¼ºå°‘ç”¨æˆ·ID'}, 400
        
        if not request_json:
            return {'error': 'è¯·æ±‚ä½“ä¸èƒ½ä¸ºç©º'}, 400
        
        # ç§»é™¤ä¸å¯æ›´æ–°çš„å­—æ®µ
        update_data = {k: v for k, v in request_json.items() 
                      if k not in ['id', 'createdAt', 'userId']}
        
        if update_data:
            update_data['updatedAt'] = firestore.SERVER_TIMESTAMP
            success = db.update_document('users', user_id, update_data)
            
            if success:
                return {'message': 'ç”¨æˆ·æ›´æ–°æˆåŠŸ'}, 200
            else:
                return {'error': 'ç”¨æˆ·æ›´æ–°å¤±è´¥'}, 500
        else:
            return {'error': 'æ²¡æœ‰æœ‰æ•ˆçš„æ›´æ–°æ•°æ®'}, 400
    
    elif request.method == 'DELETE':
        # åˆ é™¤ç”¨æˆ·
        user_id = request_args.get('userId')
        if not user_id:
            return {'error': 'ç¼ºå°‘ç”¨æˆ·ID'}, 400
        
        success = db.delete_document('users', user_id)
        if success:
            return {'message': 'ç”¨æˆ·åˆ é™¤æˆåŠŸ'}, 200
        else:
            return {'error': 'ç”¨æˆ·åˆ é™¤å¤±è´¥'}, 500
    
    else:
        return {'error': 'ä¸æ”¯æŒçš„HTTPæ–¹æ³•'}, 405

# å®æ—¶ç›‘å¬å‡½æ•°
def listen_to_user_changes(data, context):
    """ç›‘å¬ç”¨æˆ·é›†åˆçš„å˜åŒ–"""
    
    trigger_resource = context.resource
    logging.info(f'å¤„ç†æ¥è‡ª {trigger_resource} çš„äº‹ä»¶')
    
    # è·å–å˜æ›´ç±»å‹
    if context.event_type == 'google.firestore.document.create':
        logging.info('æ£€æµ‹åˆ°æ–°æ–‡æ¡£åˆ›å»º')
        # å¤„ç†æ–°ç”¨æˆ·æ³¨å†Œé€»è¾‘
        handle_new_user(data['value'])
        
    elif context.event_type == 'google.firestore.document.update':
        logging.info('æ£€æµ‹åˆ°æ–‡æ¡£æ›´æ–°')
        # å¤„ç†ç”¨æˆ·ä¿¡æ¯æ›´æ–°é€»è¾‘
        handle_user_update(data['value'], data.get('oldValue'))
        
    elif context.event_type == 'google.firestore.document.delete':
        logging.info('æ£€æµ‹åˆ°æ–‡æ¡£åˆ é™¤')
        # å¤„ç†ç”¨æˆ·åˆ é™¤é€»è¾‘
        handle_user_deletion(data.get('oldValue'))

def handle_new_user(user_data):
    """å¤„ç†æ–°ç”¨æˆ·æ³¨å†Œ"""
    db = FirestoreClient()
    
    # å‘é€æ¬¢è¿é‚®ä»¶
    user_email = user_data.get('fields', {}).get('email', {}).get('stringValue')
    user_name = user_data.get('fields', {}).get('name', {}).get('stringValue')
    
    if user_email and user_name:
        # è¿™é‡Œå¯ä»¥é›†æˆé‚®ä»¶æœåŠ¡å‘é€æ¬¢è¿é‚®ä»¶
        logging.info(f'å‡†å¤‡å‘ {user_name} <{user_email}> å‘é€æ¬¢è¿é‚®ä»¶')
        
        # åˆ›å»ºç”¨æˆ·ç»Ÿè®¡æ–‡æ¡£
        stats_data = {
            'userId': user_data.get('name').split('/')[-1],  # ä»æ–‡æ¡£è·¯å¾„æå–ç”¨æˆ·ID
            'registrationDate': firestore.SERVER_TIMESTAMP,
            'loginCount': 0,
            'lastActivity': firestore.SERVER_TIMESTAMP
        }
        
        db.add_document('user_stats', stats_data)

def handle_user_update(new_data, old_data):
    """å¤„ç†ç”¨æˆ·ä¿¡æ¯æ›´æ–°"""
    # æ¯”è¾ƒå˜æ›´å†…å®¹
    if old_data and new_data:
        old_fields = old_data.get('fields', {})
        new_fields = new_data.get('fields', {})
        
        # æ£€æŸ¥é‚®ç®±å˜æ›´
        old_email = old_fields.get('email', {}).get('stringValue')
        new_email = new_fields.get('email', {}).get('stringValue')
        
        if old_email != new_email and new_email:
            logging.info(f'ç”¨æˆ·é‚®ç®±ä» {old_email} å˜æ›´ä¸º {new_email}')
            # å¯ä»¥åœ¨è¿™é‡Œè§¦å‘é‚®ç®±éªŒè¯æµç¨‹

def handle_user_deletion(old_data):
    """å¤„ç†ç”¨æˆ·åˆ é™¤"""
    if old_data:
        user_id = old_data.get('name').split('/')[-1]
        logging.info(f'ç”¨æˆ· {user_id} å·²è¢«åˆ é™¤')
        # å¯ä»¥åœ¨è¿™é‡Œæ‰§è¡Œæ¸…ç†æ“ä½œ
```

## 3. æ¶æ„è®¾è®¡ä¸æœ€ä½³å®è·µ

### 3.1 Serverlessæ•°æ®åº“è®¾è®¡åŸåˆ™

#### æ•°æ®å»ºæ¨¡æœ€ä½³å®è·µ
```python
# serverless_data_modeling.py
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
from enum import Enum
import json

class DataModelPattern(Enum):
    DENORMALIZED = "denormalized"
    NORMALIZED = "normalized"
    HYBRID = "hybrid"

@dataclass
class CollectionDesign:
    name: str
    partition_key: str
    indexes: List[str]
    ttl_enabled: bool
    ttl_field: Optional[str]

class ServerlessDataModeler:
    def __init__(self, database_type: str):
        self.database_type = database_type  # 'document', 'key_value', 'relational'
        self.collections = {}
    
    def design_user_centric_model(self) -> Dict[str, CollectionDesign]:
        """è®¾è®¡ä»¥ç”¨æˆ·ä¸ºä¸­å¿ƒçš„æ•°æ®æ¨¡å‹"""
        
        if self.database_type == 'document':
            return self._design_document_user_model()
        elif self.database_type == 'key_value':
            return self._design_keyvalue_user_model()
        else:
            return self._design_relational_user_model()
    
    def _design_document_user_model(self) -> Dict[str, CollectionDesign]:
        """æ–‡æ¡£æ•°æ®åº“ç”¨æˆ·æ¨¡å‹è®¾è®¡"""
        collections = {
            'users': CollectionDesign(
                name='users',
                partition_key='userId',
                indexes=['email', 'createdAt', 'status'],
                ttl_enabled=False,
                ttl_field=None
            ),
            
            'user_sessions': CollectionDesign(
                name='user_sessions',
                partition_key='sessionId',
                indexes=['userId', 'createdAt', 'expiresAt'],
                ttl_enabled=True,
                ttl_field='expiresAt'
            ),
            
            'user_preferences': CollectionDesign(
                name='user_preferences',
                partition_key='userId',
                indexes=['category', 'updatedAt'],
                ttl_enabled=False,
                ttl_field=None
            ),
            
            'user_activities': CollectionDesign(
                name='user_activities',
                partition_key='activityId',
                indexes=['userId', 'timestamp', 'type'],
                ttl_enabled=True,
                ttl_field='timestamp'  # ä¿ç•™90å¤©æ´»åŠ¨è®°å½•
            )
        }
        
        return collections
    
    def _design_keyvalue_user_model(self) -> Dict[str, CollectionDesign]:
        """é”®å€¼æ•°æ®åº“ç”¨æˆ·æ¨¡å‹è®¾è®¡"""
        collections = {
            'user_profiles': CollectionDesign(
                name='user_profiles',
                partition_key='userId',
                indexes=['email'],
                ttl_enabled=False,
                ttl_field=None
            ),
            
            'user_settings': CollectionDesign(
                name='user_settings',
                partition_key='userId:setting_key',
                indexes=[],
                ttl_enabled=False,
                ttl_field=None
            ),
            
            'session_tokens': CollectionDesign(
                name='session_tokens',
                partition_key='token',
                indexes=['userId'],
                ttl_enabled=True,
                ttl_field='expiry'
            )
        }
        
        return collections
    
    def _design_relational_user_model(self) -> Dict[str, CollectionDesign]:
        """å…³ç³»å‹æ•°æ®åº“ç”¨æˆ·æ¨¡å‹è®¾è®¡"""
        collections = {
            'users': CollectionDesign(
                name='users',
                partition_key='id',
                indexes=['email', 'created_at', 'status'],
                ttl_enabled=False,
                ttl_field=None
            ),
            
            'user_sessions': CollectionDesign(
                name='user_sessions',
                partition_key='id',
                indexes=['user_id', 'created_at', 'expires_at'],
                ttl_enabled=True,
                ttl_field='expires_at'
            ),
            
            'user_preferences': CollectionDesign(
                name='user_preferences',
                partition_key='id',
                indexes=['user_id', 'category'],
                ttl_enabled=False,
                ttl_field=None
            )
        }
        
        return collections
    
    def optimize_for_serverless(self, collections: Dict[str, CollectionDesign]) -> Dict[str, Any]:
        """é’ˆå¯¹Serverlessä¼˜åŒ–æ•°æ®æ¨¡å‹"""
        optimizations = {
            'denormalization_strategy': self._suggest_denormalization(collections),
            'indexing_recommendations': self._suggest_indexes(collections),
            'partitioning_strategy': self._suggest_partitioning(collections),
            'cost_optimization': self._suggest_cost_optimization(collections)
        }
        
        return optimizations
    
    def _suggest_denormalization(self, collections: Dict[str, CollectionDesign]) -> List[str]:
        """å»ºè®®åè§„èŒƒåŒ–ç­–ç•¥"""
        suggestions = []
        
        if self.database_type == 'document':
            suggestions.extend([
                "å°†é¢‘ç¹è®¿é—®çš„ç”¨æˆ·ä¿¡æ¯åµŒå…¥åˆ°ç›¸å…³æ–‡æ¡£ä¸­",
                "é¢„è®¡ç®—èšåˆæ•°æ®å‡å°‘æŸ¥è¯¢å¤æ‚åº¦",
                "å¤åˆ¶å¼•ç”¨æ•°æ®é¿å…JOINæ“ä½œ"
            ])
        elif self.database_type == 'key_value':
            suggestions.extend([
                "ä½¿ç”¨å¤åˆé”®å­˜å‚¨å…³è”æ•°æ®",
                "åœ¨å€¼ä¸­å­˜å‚¨å†—ä½™ä¿¡æ¯æé«˜è¯»å–æ•ˆç‡",
                "é¿å…è·¨å¤šä¸ªé”®çš„å¤æ‚æŸ¥è¯¢"
            ])
        
        return suggestions
    
    def _suggest_indexes(self, collections: Dict[str, CollectionDesign]) -> Dict[str, List[str]]:
        """å»ºè®®ç´¢å¼•ç­–ç•¥"""
        index_suggestions = {}
        
        for collection_name, design in collections.items():
            # åŸºæœ¬ç´¢å¼•å»ºè®®
            suggested_indexes = design.indexes.copy()
            
            # æ ¹æ®æŸ¥è¯¢æ¨¡å¼æ·»åŠ æ›´å¤šç´¢å¼•
            if collection_name == 'users':
                suggested_indexes.extend(['status', 'last_login'])
            elif collection_name == 'user_activities':
                suggested_indexes.extend(['userId_type', 'timestamp_type'])
            
            index_suggestions[collection_name] = suggested_indexes
        
        return index_suggestions
    
    def _suggest_partitioning(self, collections: Dict[str, CollectionDesign]) -> Dict[str, str]:
        """å»ºè®®åˆ†åŒºç­–ç•¥"""
        partitioning_suggestions = {}
        
        for collection_name, design in collections.items():
            if self.database_type == 'document':
                # æ–‡æ¡£æ•°æ®åº“åˆ†åŒºå»ºè®®
                if collection_name == 'users':
                    partitioning_suggestions[collection_name] = "æŒ‰userIdå“ˆå¸Œåˆ†åŒº"
                elif collection_name == 'user_activities':
                    partitioning_suggestions[collection_name] = "æŒ‰userIdèŒƒå›´åˆ†åŒºï¼Œé…åˆæ—¶é—´æˆ³"
            elif self.database_type == 'key_value':
                # é”®å€¼æ•°æ®åº“åˆ†åŒºå»ºè®®
                partitioning_suggestions[collection_name] = f"æŒ‰{design.partition_key}åˆ†åŒº"
        
        return partitioning_suggestions
    
    def _suggest_cost_optimization(self, collections: Dict[str, CollectionDesign]) -> List[str]:
        """å»ºè®®æˆæœ¬ä¼˜åŒ–ç­–ç•¥"""
        suggestions = [
            "å¯ç”¨TTLè‡ªåŠ¨æ¸…ç†è¿‡æœŸæ•°æ®",
            "åˆç†è®¾ç½®è¯»å†™å®¹é‡å•ä½",
            "ä½¿ç”¨æ‰¹é‡æ“ä½œå‡å°‘è¯·æ±‚æ¬¡æ•°",
            "ç¼“å­˜çƒ­ç‚¹æ•°æ®å‡å°‘æ•°æ®åº“è®¿é—®",
            "å‹ç¼©å¤§å­—æ®µæ•°æ®"
        ]
        
        return suggestions
    
    def generate_model_documentation(self, collections: Dict[str, CollectionDesign], 
                                   optimizations: Dict[str, Any]) -> str:
        """ç”Ÿæˆæ•°æ®æ¨¡å‹æ–‡æ¡£"""
        doc = "# Serverlessæ•°æ®åº“æ•°æ®æ¨¡å‹è®¾è®¡\n\n"
        
        doc += "## é›†åˆè®¾è®¡\n\n"
        for name, design in collections.items():
            doc += f"### {name}\n"
            doc += f"- **åˆ†åŒºé”®**: {design.partition_key}\n"
            doc += f"- **ç´¢å¼•**: {', '.join(design.indexes)}\n"
            doc += f"- **TTLå¯ç”¨**: {'æ˜¯' if design.ttl_enabled else 'å¦'}\n"
            if design.ttl_field:
                doc += f"- **TTLå­—æ®µ**: {design.ttl_field}\n"
            doc += "\n"
        
        doc += "## ä¼˜åŒ–å»ºè®®\n\n"
        
        doc += "### åè§„èŒƒåŒ–ç­–ç•¥\n"
        for suggestion in optimizations['denormalization_strategy']:
            doc += f"- {suggestion}\n"
        doc += "\n"
        
        doc += "### ç´¢å¼•å»ºè®®\n"
        for collection, indexes in optimizations['indexing_recommendations'].items():
            doc += f"- **{collection}**: {', '.join(indexes)}\n"
        doc += "\n"
        
        doc += "### åˆ†åŒºç­–ç•¥\n"
        for collection, strategy in optimizations['partitioning_strategy'].items():
            doc += f"- **{collection}**: {strategy}\n"
        doc += "\n"
        
        doc += "### æˆæœ¬ä¼˜åŒ–\n"
        for suggestion in optimizations['cost_optimization']:
            doc += f"- {suggestion}\n"
        
        return doc

# ä½¿ç”¨ç¤ºä¾‹
modeler = ServerlessDataModeler('document')

# è®¾è®¡ç”¨æˆ·ä¸­å¿ƒæ¨¡å‹
collections = modeler.design_user_centric_model()
print("é›†åˆè®¾è®¡:")
for name, design in collections.items():
    print(f"{name}: åˆ†åŒºé”®={design.partition_key}, ç´¢å¼•={design.indexes}")

# ç”Ÿæˆä¼˜åŒ–å»ºè®®
optimizations = modeler.optimize_for_serverless(collections)
print("\nä¼˜åŒ–å»ºè®®:")
print(json.dumps(optimizations, indent=2, ensure_ascii=False))

# ç”Ÿæˆæ–‡æ¡£
documentation = modeler.generate_model_documentation(collections, optimizations)
print("\nç”Ÿæˆçš„æ–‡æ¡£:")
print(documentation[:500] + "..." if len(documentation) > 500 else documentation)
```

### 3.2 åº”ç”¨æ¶æ„æ¨¡å¼

#### å¾®æœåŠ¡æ¶æ„è®¾è®¡
```yaml
# serverless-microservices-architecture.yaml
microservices_architecture:
  api_gateway:
    service: "API Gateway"
    responsibilities:
      - "è¯·æ±‚è·¯ç”±å’Œè´Ÿè½½å‡è¡¡"
      - "èº«ä»½è®¤è¯å’Œæˆæƒ"
      - "è¯·æ±‚éªŒè¯å’Œé™æµ"
      - "APIæ–‡æ¡£å’Œç›‘æ§"
    integration_points:
      - function_compute: "è·¯ç”±åˆ°ç›¸åº”å‡½æ•°"
      - serverless_database: "ç›´æ¥æ•°æ®åº“è®¿é—®"
  
  user_service:
    functions:
      - register_user:
          trigger: "HTTP POST /users"
          database_operations:
            - "usersé›†åˆ: åˆ›å»ºç”¨æˆ·æ–‡æ¡£"
            - "user_statsé›†åˆ: åˆ›å»ºç»Ÿè®¡æ–‡æ¡£"
            - "audit_logsé›†åˆ: è®°å½•æ³¨å†Œäº‹ä»¶"
      
      - authenticate_user:
          trigger: "HTTP POST /auth/login"
          database_operations:
            - "usersé›†åˆ: æŸ¥è¯¢ç”¨æˆ·å‡­æ®"
            - "user_sessionsé›†åˆ: åˆ›å»ºä¼šè¯"
            - "user_activitiesé›†åˆ: è®°å½•ç™»å½•æ´»åŠ¨"
      
      - get_user_profile:
          trigger: "HTTP GET /users/{userId}"
          database_operations:
            - "usersé›†åˆ: è·å–ç”¨æˆ·åŸºæœ¬ä¿¡æ¯"
            - "user_preferencesé›†åˆ: è·å–ç”¨æˆ·åå¥½è®¾ç½®"
  
  order_service:
    functions:
      - create_order:
          trigger: "HTTP POST /orders"
          database_operations:
            - "ordersé›†åˆ: åˆ›å»ºè®¢å•æ–‡æ¡£"
            - "inventoryé›†åˆ: æ›´æ–°åº“å­˜"
            - "user_activitiesé›†åˆ: è®°å½•ä¸‹å•æ´»åŠ¨"
      
      - process_payment:
          trigger: "EventBridgeäº‹ä»¶"
          database_operations:
            - "paymentsé›†åˆ: åˆ›å»ºæ”¯ä»˜è®°å½•"
            - "ordersé›†åˆ: æ›´æ–°è®¢å•çŠ¶æ€"
            - "notificationsé›†åˆ: åˆ›å»ºé€šçŸ¥è®°å½•"
  
  notification_service:
    functions:
      - send_email:
          trigger: "SNSä¸»é¢˜: email_notifications"
          database_operations:
            - "notificationsé›†åˆ: æ›´æ–°å‘é€çŠ¶æ€"
            - "email_templatesé›†åˆ: è·å–æ¨¡æ¿"
      
      - send_push_notification:
          trigger: "SNSä¸»é¢˜: push_notifications"
          database_operations:
            - "device_tokensé›†åˆ: è·å–è®¾å¤‡ä»¤ç‰Œ"
            - "notificationsé›†åˆ: è®°å½•æ¨é€å†å²"
  
  data_processing_service:
    functions:
      - process_user_analytics:
          trigger: "Scheduled Event (æ¯å¤©å‡Œæ™¨2ç‚¹)"
          database_operations:
            - "user_activitiesé›†åˆ: èšåˆç”¨æˆ·è¡Œä¸ºæ•°æ®"
            - "analyticsé›†åˆ: å­˜å‚¨åˆ†æç»“æœ"
            - "reportsé›†åˆ: ç”ŸæˆæŠ¥å‘Šæ–‡æ¡£"
      
      - cleanup_expired_data:
          trigger: "Scheduled Event (æ¯å°æ—¶)"
          database_operations:
            - "user_sessionsé›†åˆ: åˆ é™¤è¿‡æœŸä¼šè¯"
            - "temporary_dataé›†åˆ: æ¸…ç†ä¸´æ—¶æ•°æ®"
  
  shared_components:
    authentication_layer:
      service: "Cognito / Auth0"
      responsibilities:
        - "ç”¨æˆ·èº«ä»½ç®¡ç†"
        - "JWTä»¤ç‰Œç”Ÿæˆå’ŒéªŒè¯"
        - "ç¤¾äº¤ç™»å½•é›†æˆ"
    
    caching_layer:
      service: "Redis / DynamoDB Accelerator"
      responsibilities:
        - "çƒ­ç‚¹æ•°æ®ç¼“å­˜"
        - "ä¼šè¯å­˜å‚¨"
        - "é€Ÿç‡é™åˆ¶"
    
    messaging_system:
      service: "EventBridge / SNS + SQS"
      responsibilities:
        - "æœåŠ¡é—´å¼‚æ­¥é€šä¿¡"
        - "äº‹ä»¶é©±åŠ¨æ¶æ„"
        - "æ¶ˆæ¯é˜Ÿåˆ—ç®¡ç†"
    
    monitoring_system:
      service: "CloudWatch / Datadog"
      responsibilities:
        - "æ€§èƒ½ç›‘æ§"
        - "é”™è¯¯è¿½è¸ª"
        - "æ—¥å¿—èšåˆ"
        - "å‘Šè­¦é€šçŸ¥"
  
  database_design:
    collections_structure:
      users:
        fields: ["userId", "email", "name", "profile", "preferences", "createdAt"]
        partition_key: "userId"
        indexes: ["email", "createdAt"]
      
      orders:
        fields: ["orderId", "userId", "items", "total", "status", "createdAt"]
        partition_key: "orderId"
        indexes: ["userId", "status", "createdAt"]
      
      user_activities:
        fields: ["activityId", "userId", "type", "data", "timestamp"]
        partition_key: "activityId"
        indexes: ["userId", "type", "timestamp"]
        ttl: "90å¤©"
      
      notifications:
        fields: ["notificationId", "userId", "type", "content", "status", "createdAt"]
        partition_key: "notificationId"
        indexes: ["userId", "status", "createdAt"]
```

## 4. æˆæœ¬ä¼˜åŒ–ä¸æ€§èƒ½è°ƒä¼˜

### 4.1 æˆæœ¬åˆ†æå’Œä¼˜åŒ–ç­–ç•¥

#### Serverlessæ•°æ®åº“æˆæœ¬è®¡ç®—å™¨
```python
# serverless_cost_calculator.py
from typing import Dict, List, Any
from dataclasses import dataclass
import json

@dataclass
class CostComponent:
    name: str
    unit: str
    rate: float
    quantity: float
    total_cost: float

class ServerlessCostCalculator:
    def __init__(self):
        # å„äº‘æœåŠ¡å•†çš„å®šä»·ä¿¡æ¯ï¼ˆç®€åŒ–ç‰ˆï¼‰
        self.pricing = {
            'aws': {
                'aurora_serverless_v2': {
                    'acu_hour': 0.12,  # $0.12 per ACU-hour
                    'storage_gb_month': 0.10,  # $0.10 per GB-month
                    'io_request': 0.0002  # $0.0002 per IO request (ä¼°ç®—)
                }
            },
            'azure': {
                'cosmos_db_serverless': {
                    'ru_hour': 0.000016,  # $0.000016 per RU
                    'storage_gb_month': 0.25  # $0.25 per GB-month
                }
            },
            'gcp': {
                'firestore': {
                    'document_read': 0.0000006,  # $0.0000006 per document read
                    'document_write': 0.0000018,  # $0.0000018 per document write
                    'document_delete': 0.0000002,  # $0.0000002 per document delete
                    'storage_gb_month': 0.18  # $0.18 per GB-month
                }
            }
        }
    
    def calculate_workload_cost(self, provider: str, service: str, 
                              workload_profile: Dict[str, Any]) -> Dict[str, Any]:
        """è®¡ç®—å·¥ä½œè´Ÿè½½æˆæœ¬"""
        
        if provider == 'aws' and service == 'aurora_serverless_v2':
            return self._calculate_aurora_cost(workload_profile)
        elif provider == 'azure' and service == 'cosmos_db_serverless':
            return self._calculate_cosmos_cost(workload_profile)
        elif provider == 'gcp' and service == 'firestore':
            return self._calculate_firestore_cost(workload_profile)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ç»„åˆ: {provider} {service}")
    
    def _calculate_aurora_cost(self, profile: Dict[str, Any]) -> Dict[str, Any]:
        """è®¡ç®—Aurora Serverlessæˆæœ¬"""
        monthly_requests = profile.get('monthly_requests', 1000000)
        avg_request_duration = profile.get('avg_request_duration_ms', 100)
        storage_gb = profile.get('storage_gb', 100)
        peak_concurrent = profile.get('peak_concurrent_requests', 100)
        
        # è®¡ç®—ACUä½¿ç”¨é‡ï¼ˆç®€åŒ–ä¼°ç®—ï¼‰
        # å‡è®¾æ¯ä¸ªå¹¶å‘è¯·æ±‚éœ€è¦0.5 ACUï¼Œå¹³å‡åˆ©ç”¨ç‡50%
        acu_needed = peak_concurrent * 0.5
        hours_per_month = 730  # å¹³å‡æ¯æœˆå°æ—¶æ•°
        acu_hours = acu_needed * hours_per_month * 0.5  # 50%åˆ©ç”¨ç‡
        
        compute_cost = acu_hours * self.pricing['aws']['aurora_serverless_v2']['acu_hour']
        storage_cost = storage_gb * self.pricing['aws']['aurora_serverless_v2']['storage_gb_month']
        
        # IOæˆæœ¬ä¼°ç®—ï¼ˆå‡è®¾æ¯æ¬¡è¯·æ±‚10ä¸ªIOæ“ä½œï¼‰
        io_operations = monthly_requests * 10
        io_cost = io_operations * self.pricing['aws']['aurora_serverless_v2']['io_request']
        
        total_cost = compute_cost + storage_cost + io_cost
        
        cost_breakdown = [
            CostComponent('è®¡ç®—æˆæœ¬', 'ACU-hours', 
                         self.pricing['aws']['aurora_serverless_v2']['acu_hour'],
                         acu_hours, compute_cost),
            CostComponent('å­˜å‚¨æˆæœ¬', 'GB-month',
                         self.pricing['aws']['aurora_serverless_v2']['storage_gb_month'],
                         storage_gb, storage_cost),
            CostComponent('IOæˆæœ¬', 'IOæ“ä½œ',
                         self.pricing['aws']['aurora_serverless_v2']['io_request'],
                         io_operations, io_cost)
        ]
        
        return {
            'total_monthly_cost': round(total_cost, 2),
            'cost_breakdown': cost_breakdown,
            'cost_per_thousand_requests': round((total_cost / monthly_requests) * 1000, 4),
            'daily_cost': round(total_cost / 30, 2),
            'recommendations': self._generate_aurora_recommendations(profile, total_cost)
        }
    
    def _calculate_cosmos_cost(self, profile: Dict[str, Any]) -> Dict[str, Any]:
        """è®¡ç®—Cosmos DBæˆæœ¬"""
        monthly_requests = profile.get('monthly_requests', 1000000)
        read_ratio = profile.get('read_ratio', 0.7)  # è¯»æ“ä½œæ¯”ä¾‹
        write_ratio = profile.get('write_ratio', 0.3)  # å†™æ“ä½œæ¯”ä¾‹
        storage_gb = profile.get('storage_gb', 100)
        
        # RUæ¶ˆè€—ä¼°ç®—ï¼ˆå‡è®¾å¹³å‡æ¯æ¬¡æ“ä½œæ¶ˆè€—10 RUï¼‰
        read_ru = monthly_requests * read_ratio * 10
        write_ru = monthly_requests * write_ratio * 20  # å†™æ“ä½œé€šå¸¸æ¶ˆè€—æ›´å¤šRU
        total_ru = read_ru + write_ru
        
        compute_cost = (total_ru / 1000000) * self.pricing['azure']['cosmos_db_serverless']['ru_hour'] * 730
        storage_cost = storage_gb * self.pricing['azure']['cosmos_db_serverless']['storage_gb_month']
        total_cost = compute_cost + storage_cost
        
        cost_breakdown = [
            CostComponent('RUè®¡ç®—æˆæœ¬', 'RU-hours',
                         self.pricing['azure']['cosmos_db_serverless']['ru_hour'],
                         total_ru / 1000000 * 730, compute_cost),
            CostComponent('å­˜å‚¨æˆæœ¬', 'GB-month',
                         self.pricing['azure']['cosmos_db_serverless']['storage_gb_month'],
                         storage_gb, storage_cost)
        ]
        
        return {
            'total_monthly_cost': round(total_cost, 2),
            'cost_breakdown': cost_breakdown,
            'cost_per_thousand_requests': round((total_cost / monthly_requests) * 1000, 4),
            'daily_cost': round(total_cost / 30, 2),
            'recommendations': self._generate_cosmos_recommendations(profile, total_cost)
        }
    
    def _calculate_firestore_cost(self, profile: Dict[str, Any]) -> Dict[str, Any]:
        """è®¡ç®—Firestoreæˆæœ¬"""
        monthly_requests = profile.get('monthly_requests', 1000000)
        read_ratio = profile.get('read_ratio', 0.7)
        write_ratio = profile.get('write_ratio', 0.25)
        delete_ratio = profile.get('delete_ratio', 0.05)
        storage_gb = profile.get('storage_gb', 100)
        
        read_operations = monthly_requests * read_ratio
        write_operations = monthly_requests * write_ratio
        delete_operations = monthly_requests * delete_ratio
        
        read_cost = read_operations * self.pricing['gcp']['firestore']['document_read']
        write_cost = write_operations * self.pricing['gcp']['firestore']['document_write']
        delete_cost = delete_operations * self.pricing['gcp']['firestore']['document_delete']
        storage_cost = storage_gb * self.pricing['gcp']['firestore']['storage_gb_month']
        
        total_cost = read_cost + write_cost + delete_cost + storage_cost
        
        cost_breakdown = [
            CostComponent('è¯»æ“ä½œæˆæœ¬', 'æ“ä½œæ•°',
                         self.pricing['gcp']['firestore']['document_read'],
                         read_operations, read_cost),
            CostComponent('å†™æ“ä½œæˆæœ¬', 'æ“ä½œæ•°',
                         self.pricing['gcp']['firestore']['document_write'],
                         write_operations, write_cost),
            CostComponent('åˆ é™¤æ“ä½œæˆæœ¬', 'æ“ä½œæ•°',
                         self.pricing['gcp']['firestore']['document_delete'],
                         delete_operations, delete_cost),
            CostComponent('å­˜å‚¨æˆæœ¬', 'GB-month',
                         self.pricing['gcp']['firestore']['storage_gb_month'],
                         storage_gb, storage_cost)
        ]
        
        return {
            'total_monthly_cost': round(total_cost, 2),
            'cost_breakdown': cost_breakdown,
            'cost_per_thousand_requests': round((total_cost / monthly_requests) * 1000, 4),
            'daily_cost': round(total_cost / 30, 2),
            'recommendations': self._generate_firestore_recommendations(profile, total_cost)
        }
    
    def _generate_aurora_recommendations(self, profile: Dict[str, Any], total_cost: float) -> List[str]:
        """ç”ŸæˆAuroraä¼˜åŒ–å»ºè®®"""
        recommendations = []
        
        if profile.get('avg_request_duration_ms', 100) > 200:
            recommendations.append("ä¼˜åŒ–SQLæŸ¥è¯¢ï¼Œå‡å°‘è¯·æ±‚å¤„ç†æ—¶é—´")
        
        if profile.get('storage_gb', 100) > 500:
            recommendations.append("è€ƒè™‘æ•°æ®å½’æ¡£ç­–ç•¥ï¼Œå‡å°‘çƒ­æ•°æ®å­˜å‚¨")
        
        if total_cost > 1000:
            recommendations.append("è¯„ä¼°é¢„ç•™å®ä¾‹æ˜¯å¦æ›´ç»æµ")
        
        return recommendations
    
    def _generate_cosmos_recommendations(self, profile: Dict[str, Any], total_cost: float) -> List[str]:
        """ç”ŸæˆCosmos DBä¼˜åŒ–å»ºè®®"""
        recommendations = []
        
        if profile.get('read_ratio', 0.7) > 0.9:
            recommendations.append("è€ƒè™‘ä½¿ç”¨åªè¯»å‰¯æœ¬æ¥é™ä½RUæ¶ˆè€—")
        
        if total_cost > 500:
            recommendations.append("è¯„ä¼°é¢„é…ååé‡æ¨¡å¼æ˜¯å¦æ›´åˆé€‚")
        
        return recommendations
    
    def _generate_firestore_recommendations(self, profile: Dict[str, Any], total_cost: float) -> List[str]:
        """ç”ŸæˆFirestoreä¼˜åŒ–å»ºè®®"""
        recommendations = []
        
        if profile.get('read_ratio', 0.7) > 0.8:
            recommendations.append("ä½¿ç”¨ç¼“å­˜å‡å°‘é‡å¤è¯»å–æ“ä½œ")
        
        if profile.get('write_ratio', 0.25) > 0.4:
            recommendations.append("è€ƒè™‘æ‰¹é‡å†™å…¥æ“ä½œæ¥ä¼˜åŒ–æˆæœ¬")
        
        return recommendations
    
    def compare_providers(self, workload_profile: Dict[str, Any]) -> Dict[str, Any]:
        """æ¯”è¾ƒä¸åŒæä¾›å•†çš„æˆæœ¬"""
        comparisons = {}
        
        providers = [
            ('aws', 'aurora_serverless_v2'),
            ('azure', 'cosmos_db_serverless'),
            ('gcp', 'firestore')
        ]
        
        for provider, service in providers:
            try:
                cost_details = self.calculate_workload_cost(provider, service, workload_profile)
                comparisons[f"{provider}_{service}"] = {
                    'total_cost': cost_details['total_monthly_cost'],
                    'cost_per_1000_requests': cost_details['cost_per_thousand_requests'],
                    'recommendations': cost_details['recommendations']
                }
            except Exception as e:
                comparisons[f"{provider}_{service}"] = {
                    'error': str(e)
                }
        
        # æ‰¾å‡ºæœ€å…·æˆæœ¬æ•ˆç›Šçš„é€‰é¡¹
        valid_options = {k: v for k, v in comparisons.items() if 'error' not in v}
        if valid_options:
            best_option = min(valid_options.items(), key=lambda x: x[1]['total_cost'])
            comparisons['best_option'] = best_option[0]
            comparisons['cost_savings'] = {
                option: round(((valid_options[best_option[0]]['total_cost'] - details['total_cost']) 
                              / valid_options[best_option[0]]['total_cost']) * 100, 2)
                for option, details in valid_options.items()
            }
        
        return comparisons

# ä½¿ç”¨ç¤ºä¾‹
calculator = ServerlessCostCalculator()

# å®šä¹‰å·¥ä½œè´Ÿè½½é…ç½®
workload_profile = {
    'monthly_requests': 2000000,
    'avg_request_duration_ms': 80,
    'storage_gb': 150,
    'peak_concurrent_requests': 200,
    'read_ratio': 0.7,
    'write_ratio': 0.25,
    'delete_ratio': 0.05
}

# è®¡ç®—å„æä¾›å•†æˆæœ¬
cost_comparisons = calculator.compare_providers(workload_profile)

print("æˆæœ¬æ¯”è¾ƒç»“æœ:")
print(json.dumps(cost_comparisons, indent=2, ensure_ascii=False))

# è¯¦ç»†è®¡ç®—æŸä¸ªæä¾›å•†çš„æˆæœ¬
aws_cost = calculator.calculate_workload_cost('aws', 'aurora_serverless_v2', workload_profile)
print(f"\nAWS Aurora Serverlessè¯¦ç»†æˆæœ¬:")
print(f"æœˆåº¦æ€»æˆæœ¬: ${aws_cost['total_monthly_cost']}")
print(f"åƒæ¬¡è¯·æ±‚æˆæœ¬: ${aws_cost['cost_per_thousand_requests']}")
print("æˆæœ¬æ„æˆ:")
for component in aws_cost['cost_breakdown']:
    print(f"  {component.name}: ${component.total_cost:.2f}")
```

### 4.2 æ€§èƒ½è°ƒä¼˜ç­–ç•¥

#### æ€§èƒ½ç›‘æ§å’Œä¼˜åŒ–å·¥å…·
```python
# performance_optimizer.py
import time
import statistics
from typing import Dict, List, Any, Callable
from dataclasses import dataclass
import json

@dataclass
class PerformanceMetric:
    name: str
    value: float
    unit: str
    timestamp: float
    threshold: float

class ServerlessPerformanceOptimizer:
    def __init__(self):
        self.metrics_history = []
        self.optimization_rules = self._initialize_optimization_rules()
    
    def _initialize_optimization_rules(self) -> Dict[str, Callable]:
        """åˆå§‹åŒ–ä¼˜åŒ–è§„åˆ™"""
        return {
            'high_latency': self._optimize_high_latency,
            'high_cost': self._optimize_high_cost,
            'low_throughput': self._optimize_low_throughput,
            'high_error_rate': self._optimize_high_error_rate
        }
    
    def collect_metrics(self, function_name: str, execution_data: Dict[str, Any]) -> List[PerformanceMetric]:
        """æ”¶é›†æ€§èƒ½æŒ‡æ ‡"""
        metrics = []
        current_time = time.time()
        
        # æ‰§è¡Œæ—¶é—´
        duration = execution_data.get('duration_ms', 0)
        metrics.append(PerformanceMetric(
            name='execution_duration',
            value=duration,
            unit='milliseconds',
            timestamp=current_time,
            threshold=1000  # 1ç§’é˜ˆå€¼
        ))
        
        # å†…å­˜ä½¿ç”¨
        memory_used = execution_data.get('memory_used_mb', 0)
        memory_limit = execution_data.get('memory_limit_mb', 128)
        memory_utilization = (memory_used / memory_limit) * 100
        metrics.append(PerformanceMetric(
            name='memory_utilization',
            value=memory_utilization,
            unit='percentage',
            timestamp=current_time,
            threshold=80  # 80%é˜ˆå€¼
        ))
        
        # å†·å¯åŠ¨æ£€æµ‹
        cold_start = execution_data.get('cold_start', False)
        metrics.append(PerformanceMetric(
            name='cold_start',
            value=1 if cold_start else 0,
            unit='boolean',
            timestamp=current_time,
            threshold=0.1  # 10%å†·å¯åŠ¨ç‡é˜ˆå€¼
        ))
        
        # é”™è¯¯ç‡
        error_occurred = execution_data.get('error', False)
        metrics.append(PerformanceMetric(
            name='error_occurrence',
            value=1 if error_occurred else 0,
            unit='boolean',
            timestamp=current_time,
            threshold=0.01  # 1%é”™è¯¯ç‡é˜ˆå€¼
        ))
        
        # å­˜å‚¨I/O
        io_operations = execution_data.get('io_operations', 0)
        metrics.append(PerformanceMetric(
            name='io_operations',
            value=io_operations,
            unit='count',
            timestamp=current_time,
            threshold=100  # 100æ¬¡I/Oæ“ä½œé˜ˆå€¼
        ))
        
        # å°†æŒ‡æ ‡æ·»åŠ åˆ°å†å²è®°å½•
        self.metrics_history.extend(metrics)
        
        return metrics
    
    def analyze_performance_trends(self, window_hours: int = 24) -> Dict[str, Any]:
        """åˆ†ææ€§èƒ½è¶‹åŠ¿"""
        cutoff_time = time.time() - (window_hours * 3600)
        recent_metrics = [m for m in self.metrics_history if m.timestamp >= cutoff_time]
        
        if not recent_metrics:
            return {'error': 'æ²¡æœ‰è¶³å¤Ÿçš„å†å²æ•°æ®'}
        
        # æŒ‰æŒ‡æ ‡åç§°åˆ†ç»„
        metrics_by_name = {}
        for metric in recent_metrics:
            if metric.name not in metrics_by_name:
                metrics_by_name[metric.name] = []
            metrics_by_name[metric.name].append(metric.value)
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        trends = {}
        issues_detected = []
        
        for name, values in metrics_by_name.items():
            if not values:
                continue
                
            avg_value = statistics.mean(values)
            max_value = max(values)
            min_value = min(values)
            
            trends[name] = {
                'average': round(avg_value, 2),
                'maximum': max_value,
                'minimum': min_value,
                'count': len(values)
            }
            
            # æ£€æµ‹æ€§èƒ½é—®é¢˜
            sample_metric = next(m for m in recent_metrics if m.name == name)
            if avg_value > sample_metric.threshold:
                issues_detected.append({
                    'metric': name,
                    'current_average': round(avg_value, 2),
                    'threshold': sample_metric.threshold,
                    'severity': 'high' if avg_value > sample_metric.threshold * 1.5 else 'medium'
                })
        
        return {
            'trends': trends,
            'issues_detected': issues_detected,
            'analysis_period_hours': window_hours,
            'total_metrics_collected': len(recent_metrics)
        }
    
    def generate_optimization_recommendations(self, trends_analysis: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []
        issues = trends_analysis.get('issues_detected', [])
        
        for issue in issues:
            metric_name = issue['metric']
            severity = issue['severity']
            
            if metric_name in self.optimization_rules:
                rule_function = self.optimization_rules[metric_name]
                rule_recommendations = rule_function(issue)
                recommendations.extend(rule_recommendations)
            else:
                # é»˜è®¤ä¼˜åŒ–å»ºè®®
                recommendations.append({
                    'issue': metric_name,
                    'severity': severity,
                    'recommendation': f'ä¼˜åŒ–{metric_name}æ€§èƒ½',
                    'implementation': 'éœ€è¦å…·ä½“åˆ†æ',
                    'expected_improvement': 'å¾…è¯„ä¼°'
                })
        
        return recommendations
    
    def _optimize_high_latency(self, issue: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ä¼˜åŒ–é«˜å»¶è¿Ÿé—®é¢˜"""
        return [
            {
                'issue': 'execution_duration',
                'severity': issue['severity'],
                'recommendation': 'ä¼˜åŒ–å‡½æ•°ä»£ç æ‰§è¡Œæ•ˆç‡',
                'implementation': [
                    'åˆ†æä»£ç çƒ­ç‚¹ï¼Œä¼˜åŒ–ç®—æ³•å¤æ‚åº¦',
                    'å‡å°‘ä¸å¿…è¦çš„è®¡ç®—å’Œå¾ªç¯',
                    'ä½¿ç”¨æ›´é«˜æ•ˆçš„æ•°æ®ç»“æ„'
                ],
                'expected_improvement': '20-50% æ‰§è¡Œæ—¶é—´å‡å°‘'
            },
            {
                'issue': 'execution_duration',
                'severity': issue['severity'],
                'recommendation': 'ä¼˜åŒ–æ•°æ®åº“è®¿é—®',
                'implementation': [
                    'æ·»åŠ æ•°æ®åº“æŸ¥è¯¢ç´¢å¼•',
                    'ä½¿ç”¨è¿æ¥æ± å‡å°‘è¿æ¥å¼€é”€',
                    'æ‰¹é‡å¤„ç†æ•°æ®åº“æ“ä½œ'
                ],
                'expected_improvement': '30-70% æ•°æ®åº“è®¿é—®æ—¶é—´å‡å°‘'
            },
            {
                'issue': 'execution_duration',
                'severity': issue['severity'],
                'recommendation': 'å®æ–½ç¼“å­˜ç­–ç•¥',
                'implementation': [
                    'ç¼“å­˜é¢‘ç¹è®¿é—®çš„æ•°æ®',
                    'ä½¿ç”¨Redisæˆ–å†…å­˜ç¼“å­˜',
                    'è®¾ç½®åˆé€‚çš„ç¼“å­˜è¿‡æœŸæ—¶é—´'
                ],
                'expected_improvement': '50-90% é‡å¤è¯·æ±‚å“åº”æ—¶é—´å‡å°‘'
            }
        ]
    
    def _optimize_high_cost(self, issue: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ä¼˜åŒ–é«˜æˆæœ¬é—®é¢˜"""
        return [
            {
                'issue': 'memory_utilization',
                'severity': issue['severity'],
                'recommendation': 'ä¼˜åŒ–å†…å­˜ä½¿ç”¨',
                'implementation': [
                    'å‡å°‘å…¨å±€å˜é‡å’Œå¤§å¯¹è±¡',
                    'åŠæ—¶é‡Šæ”¾ä¸éœ€è¦çš„å¯¹è±¡',
                    'ä½¿ç”¨æµå¼å¤„ç†å¤§æ•°æ®'
                ],
                'expected_improvement': 'é™ä½å†…å­˜é…ç½®çº§åˆ«ï¼ŒèŠ‚çœ20-40%æˆæœ¬'
            },
            {
                'issue': 'io_operations',
                'severity': issue['severity'],
                'recommendation': 'å‡å°‘I/Oæ“ä½œ',
                'implementation': [
                    'æ‰¹é‡å¤„ç†æ•°æ®åº“æ“ä½œ',
                    'ä½¿ç”¨æ•°æ®åº“è¿æ¥æ± ',
                    'ä¼˜åŒ–æŸ¥è¯¢å‡å°‘è¿”å›æ•°æ®é‡'
                ],
                'expected_improvement': 'å‡å°‘30-60% I/Oç›¸å…³æˆæœ¬'
            }
        ]
    
    def _optimize_low_throughput(self, issue: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ä¼˜åŒ–ä½ååé‡é—®é¢˜"""
        return [
            {
                'issue': 'cold_start',
                'severity': issue['severity'],
                'recommendation': 'å‡å°‘å†·å¯åŠ¨å½±å“',
                'implementation': [
                    'é…ç½®é¢„ç½®å¹¶å‘',
                    'ä¼˜åŒ–å‡½æ•°åŒ…å¤§å°',
                    'ä½¿ç”¨æ›´å¿«çš„è¿è¡Œæ—¶'
                ],
                'expected_improvement': 'å‡å°‘80-95%å†·å¯åŠ¨å»¶è¿Ÿ'
            }
        ]
    
    def _optimize_high_error_rate(self, issue: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ä¼˜åŒ–é«˜é”™è¯¯ç‡é—®é¢˜"""
        return [
            {
                'issue': 'error_occurrence',
                'severity': issue['severity'],
                'recommendation': 'æé«˜ä»£ç å¥å£®æ€§',
                'implementation': [
                    'æ·»åŠ è¾“å…¥éªŒè¯å’Œé”™è¯¯å¤„ç†',
                    'å®ç°é‡è¯•æœºåˆ¶',
                    'æ·»åŠ ç›‘æ§å’Œå‘Šè­¦'
                ],
                'expected_improvement': 'é”™è¯¯ç‡é™ä½è‡³0.1%ä»¥ä¸‹'
            }
        ]
    
    def simulate_performance_improvement(self, current_metrics: Dict[str, Any], 
                                       optimizations: List[Dict[str, Any]]) -> Dict[str, Any]:
        """æ¨¡æ‹Ÿæ€§èƒ½æ”¹è¿›æ•ˆæœ"""
        improved_metrics = current_metrics.copy()
        
        # æ ¹æ®ä¼˜åŒ–å»ºè®®è°ƒæ•´æŒ‡æ ‡
        for optimization in optimizations:
            improvement_factor = 0.3  # é»˜è®¤30%æ”¹å–„
            
            if 'expected_improvement' in optimization:
                # è§£æé¢„æœŸæ”¹å–„ç™¾åˆ†æ¯”
                improvement_text = optimization['expected_improvement']
                if '%' in improvement_text:
                    try:
                        improvement_factor = float(improvement_text.split('-')[0].replace('%', '')) / 100
                        improvement_factor = min(0.9, improvement_factor)  # æœ€å¤š90%æ”¹å–„
                    except:
                        pass
            
            # åº”ç”¨æ”¹å–„åˆ°ç›¸å…³æŒ‡æ ‡
            issue = optimization['issue']
            if issue == 'execution_duration' and 'execution_duration' in improved_metrics:
                improved_metrics['execution_duration'] *= (1 - improvement_factor)
            elif issue == 'memory_utilization' and 'memory_utilization' in improved_metrics:
                improved_metrics['memory_utilization'] *= (1 - improvement_factor * 0.5)  # å†…å­˜æ”¹å–„é€šå¸¸è¾ƒå°
            elif issue == 'io_operations' and 'io_operations' in improved_metrics:
                improved_metrics['io_operations'] *= (1 - improvement_factor)
            elif issue == 'cold_start' and 'cold_start_rate' in improved_metrics:
                improved_metrics['cold_start_rate'] *= (1 - improvement_factor)
        
        return improved_metrics

# ä½¿ç”¨ç¤ºä¾‹
optimizer = ServerlessPerformanceOptimizer()

# æ¨¡æ‹Ÿæ”¶é›†æ€§èƒ½æ•°æ®
sample_executions = [
    {
        'duration_ms': 1200,
        'memory_used_mb': 85,
        'memory_limit_mb': 128,
        'cold_start': True,
        'error': False,
        'io_operations': 150
    },
    {
        'duration_ms': 800,
        'memory_used_mb': 95,
        'memory_limit_mb': 128,
        'cold_start': False,
        'error': False,
        'io_operations': 120
    },
    {
        'duration_ms': 2500,
        'memory_used_mb': 110,
        'memory_limit_mb': 128,
        'cold_start': True,
        'error': True,
        'io_operations': 200
    }
]

# æ”¶é›†æŒ‡æ ‡
for execution in sample_executions:
    metrics = optimizer.collect_metrics('user_processing_function', execution)
    print(f"æ”¶é›†åˆ° {len(metrics)} ä¸ªæŒ‡æ ‡")

# åˆ†ææ€§èƒ½è¶‹åŠ¿
trends = optimizer.analyze_performance_trends(window_hours=1)
print("\næ€§èƒ½è¶‹åŠ¿åˆ†æ:")
print(json.dumps(trends, indent=2, ensure_ascii=False))

# ç”Ÿæˆä¼˜åŒ–å»ºè®®
if 'issues_detected' in trends:
    recommendations = optimizer.generate_optimization_recommendations(trends)
    print("\nä¼˜åŒ–å»ºè®®:")
    for rec in recommendations:
        print(f"- {rec['recommendation']}")
        print(f"  å®æ–½æ–¹æ¡ˆ: {rec['implementation']}")
        print(f"  é¢„æœŸæ”¹å–„: {rec['expected_improvement']}")
```

## 5. ç›‘æ§å‘Šè­¦ä¸æ•…éšœæ’æŸ¥

### 5.1 ç›‘æ§ä½“ç³»æ­å»º

#### Serverlessç›‘æ§ä»ªè¡¨æ¿é…ç½®
```json
{
  "dashboard": {
    "id": null,
    "title": "Serverlessæ•°æ®åº“ç›‘æ§æ€»è§ˆ",
    "timezone": "browser",
    "schemaVersion": 16,
    "version": 0,
    "refresh": "30s",
    "panels": [
      {
        "id": 1,
        "type": "graph",
        "title": "å‡½æ•°æ‰§è¡Œæ€§èƒ½",
        "gridPos": {
          "h": 8,
          "w": 12,
          "x": 0,
          "y": 0
        },
        "targets": [
          {
            "expr": "aws_lambda_duration_sum / aws_lambda_duration_count",
            "legendFormat": "å¹³å‡æ‰§è¡Œæ—¶é—´ (ms)",
            "refId": "A"
          },
          {
            "expr": "aws_lambda_invocations_sum",
            "legendFormat": "è°ƒç”¨æ¬¡æ•°",
            "refId": "B"
          }
        ],
        "alert": {
          "conditions": [
            {
              "evaluator": {
                "params": [1000],
                "type": "gt"
              },
              "operator": {
                "type": "and"
              },
              "query": {
                "params": ["A", "5m", "now"]
              },
              "reducer": {
                "params": [],
                "type": "avg"
              },
              "type": "query"
            }
          ],
          "executionErrorState": "alerting",
          "frequency": "1m",
          "handler": 1,
          "name": "å‡½æ•°æ‰§è¡Œæ—¶é—´è¿‡é•¿å‘Šè­¦",
          "noDataState": "no_data",
          "notifications": []
        }
      },
      {
        "id": 2,
        "type": "stat",
        "title": "æ•°æ®åº“æ€§èƒ½æŒ‡æ ‡",
        "gridPos": {
          "h": 8,
          "w": 12,
          "x": 12,
          "y": 0
        },
        "targets": [
          {
            "expr": "aws_dynamodb_consumed_read_capacity_units",
            "legendFormat": "è¯»å–å®¹é‡å•ä½",
            "refId": "A"
          },
          {
            "expr": "aws_dynamodb_consumed_write_capacity_units",
            "legendFormat": "å†™å…¥å®¹é‡å•ä½",
            "refId": "B"
          },
          {
            "expr": "aws_dynamodb_successful_request_latency_average",
            "legendFormat": "å¹³å‡å»¶è¿Ÿ (ms)",
            "refId": "C"
          }
        ]
      },
      {
        "id": 3,
        "type": "graph",
        "title": "é”™è¯¯ç‡ç›‘æ§",
        "gridPos": {
          "h": 8,
          "w": 12,
          "x": 0,
          "y": 8
        },
        "targets": [
          {
            "expr": "aws_lambda_errors_sum / aws_lambda_invocations_sum * 100",
            "legendFormat": "é”™è¯¯ç‡ (%)",
            "refId": "A"
          }
        ],
        "alert": {
          "conditions": [
            {
              "evaluator": {
                "params": [5],
                "type": "gt"
              },
              "operator": {
                "type": "and"
              },
              "query": {
                "params": ["A", "5m", "now"]
              },
              "reducer": {
                "params": [],
                "type": "avg"
              },
              "type": "query"
            }
          ],
          "name": "å‡½æ•°é”™è¯¯ç‡è¿‡é«˜å‘Šè­¦"
        }
      },
      {
        "id": 4,
        "type": "table",
        "title": "å†·å¯åŠ¨åˆ†æ",
        "gridPos": {
          "h": 8,
          "w": 12,
          "x": 12,
          "y": 8
        },
        "targets": [
          {
            "expr": "aws_lambda_init_duration_sum / aws_lambda_init_duration_count",
            "legendFormat": "å¹³å‡å†·å¯åŠ¨æ—¶é—´ (ms)",
            "refId": "A"
          }
        ]
      }
    ]
  }
}
```

### 5.2 æ•…éšœè¯Šæ–­å·¥å…·

#### Serverlessæ•…éšœæ’æŸ¥åŠ©æ‰‹
```python
# troubleshooting_assistant.py
import json
import traceback
from typing import Dict, List, Any, Optional
from datetime import datetime, timedelta
import re

class ServerlessTroubleshootingAssistant:
    def __init__(self):
        self.known_issues = self._initialize_known_issues()
        self.diagnosis_history = []
    
    def _initialize_known_issues(self) -> Dict[str, Dict[str, Any]]:
        """åˆå§‹åŒ–å·²çŸ¥é—®é¢˜åº“"""
        return {
            'timeout_error': {
                'patterns': [
                    r'Task timed out after \d+\.\d+ seconds',
                    r'Execution timeout',
                    r'Function timeout exceeded'
                ],
                'causes': [
                    'å‡½æ•°æ‰§è¡Œæ—¶é—´è¶…è¿‡é…ç½®çš„è¶…æ—¶æ—¶é—´',
                    'æ•°æ®åº“æŸ¥è¯¢è¿‡äºå¤æ‚',
                    'å¤–éƒ¨APIè°ƒç”¨å“åº”ç¼“æ…¢',
                    'å¾ªç¯æˆ–é€’å½’æ“ä½œè¿‡å¤š'
                ],
                'solutions': [
                    'å¢åŠ å‡½æ•°è¶…æ—¶æ—¶é—´é…ç½®',
                    'ä¼˜åŒ–æ•°æ®åº“æŸ¥è¯¢ï¼Œæ·»åŠ ç´¢å¼•',
                    'å®ç°å¼‚æ­¥å¤„ç†æˆ–åˆ†é¡µ',
                    'ä½¿ç”¨ç¼“å­˜å‡å°‘é‡å¤è®¡ç®—'
                ],
                'severity': 'high'
            },
            
            'memory_error': {
                'patterns': [
                    r'Out of memory',
                    r'Memory limit exceeded',
                    r'Process exited before completing request'
                ],
                'causes': [
                    'å‡½æ•°å†…å­˜é…ç½®ä¸è¶³',
                    'å†…å­˜æ³„æ¼é—®é¢˜',
                    'å¤„ç†å¤§æ•°æ®é›†æ—¶å†…å­˜æº¢å‡º',
                    'å…¨å±€å˜é‡å ç”¨è¿‡å¤šå†…å­˜'
                ],
                'solutions': [
                    'å¢åŠ å‡½æ•°å†…å­˜é…ç½®',
                    'ä¼˜åŒ–å†…å­˜ä½¿ç”¨ï¼ŒåŠæ—¶é‡Šæ”¾å¯¹è±¡',
                    'ä½¿ç”¨æµå¼å¤„ç†å¤§æ•°æ®',
                    'é¿å…å…¨å±€å˜é‡å­˜å‚¨å¤§é‡æ•°æ®'
                ],
                'severity': 'high'
            },
            
            'cold_start': {
                'patterns': [
                    r'Cold start detected',
                    r'Init duration',
                    r'First invocation delay'
                ],
                'causes': [
                    'å‡½æ•°é•¿æ—¶é—´æœªè¢«è°ƒç”¨',
                    'å‡½æ•°åŒ…ä½“ç§¯è¿‡å¤§',
                    'å¤æ‚çš„åˆå§‹åŒ–é€»è¾‘',
                    'ä¾èµ–åº“åŠ è½½æ—¶é—´é•¿'
                ],
                'solutions': [
                    'é…ç½®é¢„ç½®å¹¶å‘',
                    'ä¼˜åŒ–å‡½æ•°åŒ…å¤§å°ï¼Œç§»é™¤ä¸å¿…è¦çš„ä¾èµ–',
                    'ç®€åŒ–åˆå§‹åŒ–é€»è¾‘',
                    'ä½¿ç”¨æ›´å¿«çš„è¿è¡Œæ—¶ç¯å¢ƒ'
                ],
                'severity': 'medium'
            },
            
            'database_connection': {
                'patterns': [
                    r'Connection timeout',
                    r'Connection refused',
                    r'Database connection failed'
                ],
                'causes': [
                    'æ•°æ®åº“è¿æ¥æ± é…ç½®ä¸å½“',
                    'ç½‘ç»œé—®é¢˜å¯¼è‡´è¿æ¥å¤±è´¥',
                    'æ•°æ®åº“å®ä¾‹èµ„æºä¸è¶³',
                    'è¿æ¥å­—ç¬¦ä¸²é…ç½®é”™è¯¯'
                ],
                'solutions': [
                    'ä¼˜åŒ–æ•°æ®åº“è¿æ¥æ± é…ç½®',
                    'å¢åŠ è¿æ¥é‡è¯•æœºåˆ¶',
                    'ç›‘æ§æ•°æ®åº“èµ„æºä½¿ç”¨æƒ…å†µ',
                    'éªŒè¯è¿æ¥å­—ç¬¦ä¸²å’Œç½‘ç»œé…ç½®'
                ],
                'severity': 'high'
            }
        }
    
    def diagnose_issue(self, error_info: Dict[str, Any]) -> Dict[str, Any]:
        """è¯Šæ–­é—®é¢˜"""
        diagnosis = {
            'timestamp': datetime.now().isoformat(),
            'error_message': error_info.get('error_message', ''),
            'function_name': error_info.get('function_name', 'unknown'),
            'matched_patterns': [],
            'likely_causes': [],
            'recommended_solutions': [],
            'severity': 'unknown',
            'confidence': 0.0
        }
        
        error_message = error_info.get('error_message', '')
        
        # åŒ¹é…å·²çŸ¥é—®é¢˜æ¨¡å¼
        for issue_name, issue_info in self.known_issues.items():
            for pattern in issue_info['patterns']:
                if re.search(pattern, error_message, re.IGNORECASE):
                    diagnosis['matched_patterns'].append(issue_name)
                    diagnosis['likely_causes'].extend(issue_info['causes'])
                    diagnosis['recommended_solutions'].extend(issue_info['solutions'])
                    diagnosis['severity'] = issue_info['severity']
                    diagnosis['confidence'] += 0.3
        
        # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°å·²çŸ¥æ¨¡å¼ï¼Œè¿›è¡Œé€šç”¨åˆ†æ
        if not diagnosis['matched_patterns']:
            generic_analysis = self._generic_analysis(error_info)
            diagnosis.update(generic_analysis)
        
        # å»é‡å’Œæ•´ç†ç»“æœ
        diagnosis['likely_causes'] = list(set(diagnosis['likely_causes']))
        diagnosis['recommended_solutions'] = list(set(diagnosis['recommended_solutions']))
        diagnosis['confidence'] = min(1.0, diagnosis['confidence'])
        
        # è®°å½•è¯Šæ–­å†å²
        self.diagnosis_history.append(diagnosis)
        
        return diagnosis
    
    def _generic_analysis(self, error_info: Dict[str, Any]) -> Dict[str, Any]:
        """é€šç”¨é—®é¢˜åˆ†æ"""
        error_message = error_info.get('error_message', '').lower()
        
        analysis = {
            'matched_patterns': ['generic'],
            'likely_causes': [],
            'recommended_solutions': [],
            'severity': 'medium',
            'confidence': 0.1
        }
        
        # åŸºäºé”™è¯¯æ¶ˆæ¯å†…å®¹çš„å¯å‘å¼åˆ†æ
        if 'timeout' in error_message or 'timed out' in error_message:
            analysis['likely_causes'].append('å¯èƒ½å­˜åœ¨æ€§èƒ½ç“¶é¢ˆæˆ–è¶…æ—¶é…ç½®ä¸è¶³')
            analysis['recommended_solutions'].append('æ£€æŸ¥å‡½æ•°è¶…æ—¶é…ç½®å’Œæ€§èƒ½ç›‘æ§')
            analysis['severity'] = 'high'
            analysis['confidence'] += 0.2
        
        if 'memory' in error_message or 'heap' in error_message:
            analysis['likely_causes'].append('å¯èƒ½å­˜åœ¨å†…å­˜ä½¿ç”¨é—®é¢˜')
            analysis['recommended_solutions'].append('æ£€æŸ¥å†…å­˜ä½¿ç”¨æƒ…å†µå’Œé…ç½®')
            analysis['severity'] = 'high'
            analysis['confidence'] += 0.2
        
        if 'connection' in error_message or 'network' in error_message:
            analysis['likely_causes'].append('å¯èƒ½å­˜åœ¨ç½‘ç»œè¿æ¥é—®é¢˜')
            analysis['recommended_solutions'].append('æ£€æŸ¥ç½‘ç»œé…ç½®å’Œè¿æ¥çŠ¶æ€')
            analysis['confidence'] += 0.15
        
        return analysis
    
    def get_troubleshooting_guide(self, issue_type: str) -> Dict[str, Any]:
        """è·å–æ•…éšœæ’é™¤æŒ‡å—"""
        guides = {
            'timeout_error': {
                'title': 'å‡½æ•°è¶…æ—¶é—®é¢˜æ’é™¤æŒ‡å—',
                'steps': [
                    {
                        'step': 1,
                        'action': 'æ£€æŸ¥å‡½æ•°è¶…æ—¶é…ç½®',
                        'details': 'åœ¨äº‘æ§åˆ¶å°æˆ–é…ç½®æ–‡ä»¶ä¸­æŸ¥çœ‹å½“å‰è¶…æ—¶è®¾ç½®'
                    },
                    {
                        'step': 2,
                        'action': 'åˆ†ææ‰§è¡Œæ—¥å¿—',
                        'details': 'æŸ¥çœ‹CloudWatchæ—¥å¿—äº†è§£å‡½æ•°åœ¨ä½•å¤„è€—æ—¶è¾ƒå¤š'
                    },
                    {
                        'step': 3,
                        'action': 'ä¼˜åŒ–ä»£ç æ€§èƒ½',
                        'details': 'è¯†åˆ«å¹¶ä¼˜åŒ–æ‰§è¡Œæ—¶é—´è¾ƒé•¿çš„ä»£ç æ®µ'
                    },
                    {
                        'step': 4,
                        'action': 'è€ƒè™‘æ¶æ„è°ƒæ•´',
                        'details': 'å¯¹äºé•¿æ—¶é—´è¿è¡Œçš„ä»»åŠ¡ï¼Œè€ƒè™‘ä½¿ç”¨å¼‚æ­¥å¤„ç†'
                    }
                ]
            },
            
            'memory_error': {
                'title': 'å†…å­˜ä¸è¶³é—®é¢˜æ’é™¤æŒ‡å—',
                'steps': [
                    {
                        'step': 1,
                        'action': 'æ£€æŸ¥å†…å­˜é…ç½®',
                        'details': 'ç¡®è®¤å½“å‰å†…å­˜é…ç½®æ˜¯å¦æ»¡è¶³åº”ç”¨éœ€æ±‚'
                    },
                    {
                        'step': 2,
                        'action': 'åˆ†æå†…å­˜ä½¿ç”¨æ¨¡å¼',
                        'details': 'ä½¿ç”¨å†…å­˜åˆ†æå·¥å…·è¯†åˆ«å†…å­˜ä½¿ç”¨çƒ­ç‚¹'
                    },
                    {
                        'step': 3,
                        'action': 'ä¼˜åŒ–æ•°æ®å¤„ç†',
                        'details': 'å®ç°æµå¼å¤„ç†ï¼Œé¿å…ä¸€æ¬¡æ€§åŠ è½½å¤§é‡æ•°æ®'
                    },
                    {
                        'step': 4,
                        'action': 'æ¸…ç†èµ„æº',
                        'details': 'ç¡®ä¿åŠæ—¶é‡Šæ”¾ä¸å†ä½¿ç”¨çš„å¯¹è±¡å’Œè¿æ¥'
                    }
                ]
            }
        }
        
        return guides.get(issue_type, {
            'title': 'é€šç”¨æ•…éšœæ’é™¤æŒ‡å—',
            'steps': [
                {
                    'step': 1,
                    'action': 'æ£€æŸ¥é”™è¯¯æ—¥å¿—',
                    'details': 'è¯¦ç»†æŸ¥çœ‹é”™è¯¯ä¿¡æ¯å’Œå †æ ˆè·Ÿè¸ª'
                },
                {
                    'step': 2,
                    'action': 'éªŒè¯é…ç½®',
                    'details': 'æ£€æŸ¥ç›¸å…³æœåŠ¡çš„é…ç½®æ˜¯å¦æ­£ç¡®'
                },
                {
                    'step': 3,
                    'action': 'æµ‹è¯•ç¯å¢ƒéªŒè¯',
                    'details': 'åœ¨æµ‹è¯•ç¯å¢ƒä¸­é‡ç°é—®é¢˜'
                },
                {
                    'step': 4,
                    'action': 'è”ç³»æŠ€æœ¯æ”¯æŒ',
                    'details': 'å¦‚é—®é¢˜æŒç»­å­˜åœ¨ï¼Œå¯»æ±‚ä¸“ä¸šæŠ€æœ¯æ”¯æŒ'
                }
            ]
        })
    
    def generate_incident_report(self, diagnosis: Dict[str, Any]) -> str:
        """ç”Ÿæˆäº‹æ•…æŠ¥å‘Š"""
        report = f"""
# Serverlessåº”ç”¨äº‹æ•…æŠ¥å‘Š

## åŸºæœ¬ä¿¡æ¯
- **æ—¶é—´**: {diagnosis['timestamp']}
- **å‡½æ•°åç§°**: {diagnosis['function_name']}
- **ä¸¥é‡ç¨‹åº¦**: {diagnosis['severity']}
- **è¯Šæ–­ç½®ä¿¡åº¦**: {diagnosis['confidence']:.2%}

## é”™è¯¯è¯¦æƒ…
```
{diagnosis['error_message']}
```

## é—®é¢˜åˆ†æ
### åŒ¹é…çš„å·²çŸ¥é—®é¢˜æ¨¡å¼
{', '.join(diagnosis['matched_patterns']) if diagnosis['matched_patterns'] else 'æœªåŒ¹é…åˆ°å·²çŸ¥æ¨¡å¼'}

### å¯èƒ½çš„æ ¹æœ¬åŸå› 
"""
        for cause in diagnosis['likely_causes']:
            report += f"- {cause}\n"
        
        report += "\n### å»ºè®®çš„è§£å†³æ–¹æ¡ˆ\n"
        for solution in diagnosis['recommended_solutions']:
            report += f"- {solution}\n"
        
        # æ·»åŠ ç›¸å…³çš„å†å²è¯Šæ–­ä¿¡æ¯
        if len(self.diagnosis_history) > 1:
            recent_history = self.diagnosis_history[-5:]  # æœ€è¿‘5æ¬¡è¯Šæ–­
            report += f"\n## è¿‘æœŸç±»ä¼¼é—®é¢˜å†å²\n"
            for i, hist in enumerate(recent_history[:-1], 1):  # æ’é™¤å½“å‰è¿™æ¬¡
                report += f"{i}. {hist['timestamp'][:19]} - {hist['function_name']} - {hist['severity']}\n"
        
        return report
    
    def suggest_preventive_measures(self, diagnosis_history: List[Dict[str, Any]]) -> List[str]:
        """å»ºè®®é¢„é˜²æªæ–½"""
        preventive_measures = []
        
        # åˆ†æå†å²é—®é¢˜æ¨¡å¼
        issue_types = {}
        for diagnosis in diagnosis_history:
            for pattern in diagnosis['matched_patterns']:
                issue_types[pattern] = issue_types.get(pattern, 0) + 1
        
        # åŸºäºé«˜é¢‘é—®é¢˜æä¾›å»ºè®®
        if issue_types.get('timeout_error', 0) > 2:
            preventive_measures.append("å®æ–½ä¸»åŠ¨æ€§èƒ½ç›‘æ§å’Œå‘Šè­¦")
        
        if issue_types.get('memory_error', 0) > 2:
            preventive_measures.append("å»ºç«‹å†…å­˜ä½¿ç”¨åŸºçº¿å’Œå¼‚å¸¸æ£€æµ‹")
        
        if issue_types.get('cold_start', 0) > 3:
            preventive_measures.append("é…ç½®é¢„ç½®å¹¶å‘å’Œä¼˜åŒ–å‡½æ•°åŒ…å¤§å°")
        
        # é€šç”¨é¢„é˜²æªæ–½
        preventive_measures.extend([
            "å»ºç«‹å®Œå–„çš„æ—¥å¿—è®°å½•å’Œç›‘æ§ä½“ç³»",
            "å®æ–½è“ç»¿éƒ¨ç½²å’Œå›æ»šæœºåˆ¶",
            "å®šæœŸè¿›è¡Œæ€§èƒ½å‹åŠ›æµ‹è¯•",
            "å»ºç«‹æ•…éšœæ¼”ç»ƒå’Œåº”æ€¥å“åº”æµç¨‹"
        ])
        
        return preventive_measures

# ä½¿ç”¨ç¤ºä¾‹
assistant = ServerlessTroubleshootingAssistant()

# æ¨¡æ‹Ÿé”™è¯¯ä¿¡æ¯
error_info = {
    'error_message': 'Task timed out after 30.03 seconds',
    'function_name': 'process_user_data',
    'timestamp': datetime.now().isoformat()
}

# è¿›è¡Œè¯Šæ–­
diagnosis = assistant.diagnose_issue(error_info)
print("è¯Šæ–­ç»“æœ:")
print(json.dumps(diagnosis, indent=2, ensure_ascii=False))

# è·å–æ•…éšœæ’é™¤æŒ‡å—
guide = assistant.get_troubleshooting_guide('timeout_error')
print(f"\næ•…éšœæ’é™¤æŒ‡å—: {guide['title']}")
for step in guide['steps']:
    print(f"æ­¥éª¤ {step['step']}: {step['action']}")
    print(f"  è¯¦æƒ…: {step['details']}")

# ç”Ÿæˆäº‹æ•…æŠ¥å‘Š
report = assistant.generate_incident_report(diagnosis)
print("\näº‹æ•…æŠ¥å‘Š:")
print(report)

# å»ºè®®é¢„é˜²æªæ–½
preventive_measures = assistant.suggest_preventive_measures(assistant.diagnosis_history)
print("\nå»ºè®®çš„é¢„é˜²æªæ–½:")
for measure in preventive_measures:
    print(f"- {measure}")
```

## 6. å®é™…åº”ç”¨æ¡ˆä¾‹

### 6.1 ç”µå•†åº”ç”¨æ¡ˆä¾‹

#### Serverlessç”µå•†åç«¯æ¶æ„
```yaml
# ecommerce_serverless_architecture.yaml
ecommerce_backend:
  user_management:
    functions:
      - user_registration:
          trigger: "API Gateway POST /users/register"
          database_operations:
            - "usersé›†åˆ: åˆ›å»ºç”¨æˆ·æ–‡æ¡£"
            - "user_profilesé›†åˆ: åˆå§‹åŒ–ç”¨æˆ·èµ„æ–™"
            - "verification_codesé›†åˆ: å­˜å‚¨éªŒè¯ç "
          integrations:
            - "SES: å‘é€éªŒè¯é‚®ä»¶"
            - "SNS: å‘å¸ƒç”¨æˆ·æ³¨å†Œäº‹ä»¶"
      
      - user_authentication:
          trigger: "API Gateway POST /auth/login"
          database_operations:
            - "usersé›†åˆ: éªŒè¯ç”¨æˆ·å‡­æ®"
            - "user_sessionsé›†åˆ: åˆ›å»ºä¼šè¯ä»¤ç‰Œ"
            - "login_attemptsé›†åˆ: è®°å½•ç™»å½•å°è¯•"
          security:
            - "Cognito: èº«ä»½éªŒè¯"
            - "Rate limiting: é˜²æ­¢æš´åŠ›ç ´è§£"
  
  product_catalog:
    functions:
      - list_products:
          trigger: "API Gateway GET /products"
          database_operations:
            - "productsé›†åˆ: æŸ¥è¯¢å•†å“åˆ—è¡¨"
            - "categoriesé›†åˆ: è·å–åˆ†ç±»ä¿¡æ¯"
            - "inventoryé›†åˆ: æ£€æŸ¥åº“å­˜çŠ¶æ€"
          optimizations:
            - "Redisç¼“å­˜: çƒ­é—¨å•†å“æ•°æ®"
            - "åˆ†é¡µæŸ¥è¯¢: å‡å°‘å•æ¬¡æŸ¥è¯¢æ•°æ®é‡"
      
      - get_product_details:
          trigger: "API Gateway GET /products/{productId}"
          database_operations:
            - "productsé›†åˆ: è·å–å•†å“è¯¦æƒ…"
            - "reviewsé›†åˆ: è·å–å•†å“è¯„ä»·"
            - "related_productsé›†åˆ: è·å–å…³è”å•†å“"
          caching:
            - "å•†å“è¯¦æƒ…ç¼“å­˜1å°æ—¶"
            - "è¯„ä»·æ•°æ®ç¼“å­˜30åˆ†é’Ÿ"
  
  shopping_cart:
    functions:
      - add_to_cart:
          trigger: "API Gateway POST /cart/items"
          database_operations:
            - "shopping_cartsé›†åˆ: æ›´æ–°è´­ç‰©è½¦"
            - "inventoryé›†åˆ: é¢„æ‰£åº“å­˜"
            - "cart_eventsé›†åˆ: è®°å½•è´­ç‰©è½¦äº‹ä»¶"
          real_time:
            - "WebSocket: å®æ—¶æ›´æ–°è´­ç‰©è½¦çŠ¶æ€"
      
      - checkout_process:
          trigger: "API Gateway POST /checkout"
          database_operations:
            - "ordersé›†åˆ: åˆ›å»ºè®¢å•"
            - "paymentsé›†åˆ: å¤„ç†æ”¯ä»˜"
            - "inventoryé›†åˆ: æ‰£å‡åº“å­˜"
            - "user_addressesé›†åˆ: è·å–é…é€åœ°å€"
          orchestration:
            - "Step Functions: ç¼–æ’ç»“è´¦æµç¨‹"
            - "Sagaæ¨¡å¼: ç¡®ä¿äº‹åŠ¡ä¸€è‡´æ€§"
  
  order_management:
    functions:
      - process_payment:
          trigger: "EventBridge: payment_completed"
          database_operations:
            - "ordersé›†åˆ: æ›´æ–°è®¢å•çŠ¶æ€"
            - "notificationsé›†åˆ: åˆ›å»ºé€šçŸ¥"
            - "inventoryé›†åˆ: ç¡®è®¤åº“å­˜æ‰£å‡"
          integrations:
            - "Payment Gateway: ç¬¬ä¸‰æ–¹æ”¯ä»˜å¤„ç†"
            - "Email Service: å‘é€è®¢å•ç¡®è®¤é‚®ä»¶"
      
      - fulfillment_tracking:
          trigger: "Scheduled Event: æ¯15åˆ†é’Ÿ"
          database_operations:
            - "ordersé›†åˆ: æŸ¥è¯¢å¾…å‘è´§è®¢å•"
            - "shipping_providersé›†åˆ: è·å–ç‰©æµä¿¡æ¯"
            - "tracking_updatesé›†åˆ: è®°å½•ç‰©æµæ›´æ–°"
          notifications:
            - "SMS: å‘è´§é€šçŸ¥"
            - "Push: ç‰©æµçŠ¶æ€æ›´æ–°"
  
  analytics_and_reporting:
    functions:
      - generate_sales_report:
          trigger: "Scheduled Event: æ¯æ—¥å‡Œæ™¨"
          database_operations:
            - "ordersé›†åˆ: èšåˆé”€å”®æ•°æ®"
            - "sales_reportsé›†åˆ: å­˜å‚¨æŠ¥å‘Š"
            - "user_behavioré›†åˆ: åˆ†æç”¨æˆ·è¡Œä¸º"
          processing:
            - "æ‰¹å¤„ç†: å¤§é‡æ•°æ®èšåˆ"
            - "æ•°æ®ä»“åº“: ç»“æœå­˜å‚¨"
      
      - real_time_analytics:
          trigger: "Kinesis Stream: ç”¨æˆ·è¡Œä¸ºæµ"
          database_operations:
            - "user_eventsé›†åˆ: å­˜å‚¨å®æ—¶äº‹ä»¶"
            - "realtime_metricsé›†åˆ: è®¡ç®—å®æ—¶æŒ‡æ ‡"
          dashboards:
            - "Grafana: å®æ—¶ç›‘æ§é¢æ¿"
            - "Business Intelligence: å•†ä¸šæ™ºèƒ½åˆ†æ"

  database_design:
    collections:
      users:
        schema:
          userId: "string (ä¸»é”®)"
          email: "string (å”¯ä¸€ç´¢å¼•)"
          passwordHash: "string"
          profile: "object"
          createdAt: "timestamp"
          lastLogin: "timestamp"
        indexing:
          - "email: å”¯ä¸€ç´¢å¼•"
          - "createdAt: äºŒçº§ç´¢å¼•"
      
      products:
        schema:
          productId: "string (ä¸»é”®)"
          name: "string"
          description: "string"
          price: "number"
          category: "string"
          inventory: "object"
          images: "array"
          attributes: "object"
        indexing:
          - "category: äºŒçº§ç´¢å¼•"
          - "price: èŒƒå›´ç´¢å¼•"
          - "name: å…¨æ–‡æœç´¢ç´¢å¼•"
      
      orders:
        schema:
          orderId: "string (ä¸»é”®)"
          userId: "string"
          items: "array"
          totalAmount: "number"
          status: "string"
          shippingAddress: "object"
          paymentInfo: "object"
          timestamps: "object"
        indexing:
          - "userId: äºŒçº§ç´¢å¼•"
          - "status: äºŒçº§ç´¢å¼•"
          - "createdAt: äºŒçº§ç´¢å¼•"
      
      shopping_carts:
        schema:
          cartId: "string (ä¸»é”®)"
          userId: "string"
          items: "array"
          totals: "object"
          updatedAt: "timestamp"
        indexing:
          - "userId: å”¯ä¸€ç´¢å¼•"
          - "updatedAt: TTLç´¢å¼• (30å¤©)"

  monitoring_and_alerting:
    metrics:
      - "å‡½æ•°æ‰§è¡Œæ—¶é—´åˆ†å¸ƒ"
      - "æ•°æ®åº“è¯·æ±‚å»¶è¿Ÿ"
      - "é”™è¯¯ç‡å’ŒæˆåŠŸç‡"
      - "ç”¨æˆ·æ´»è·ƒåº¦æŒ‡æ ‡"
      - "é”€å”®é¢å’Œè½¬åŒ–ç‡"
    
    alerts:
      - "é«˜é”™è¯¯ç‡ (>5%)"
      - "æ…¢æŸ¥è¯¢ (>1ç§’)"
      - "é«˜å»¶è¿Ÿ (>2ç§’)"
      - "åº“å­˜ä¸è¶³é¢„è­¦"
      - "æ”¯ä»˜å¤±è´¥ç‡å¼‚å¸¸"
```

### 6.2 IoTæ•°æ®å¤„ç†æ¡ˆä¾‹

#### IoTå®æ—¶æ•°æ®å¤„ç†æ¶æ„
```python
# iot_data_processor.py
import json
import time
from typing import Dict, List, Any
from dataclasses import dataclass
from datetime import datetime
import boto3

@dataclass
class IoTDeviceData:
    device_id: str
    timestamp: float
    sensor_data: Dict[str, Any]
    location: Dict[str, float]
    battery_level: float

class IoTDataProcessor:
    def __init__(self, dynamodb_table: str):
        self.dynamodb = boto3.resource('dynamodb')
        self.table = self.dynamodb.Table(dynamodb_table)
        self.processed_count = 0
    
    def process_device_message(self, message: Dict[str, Any]) -> Dict[str, Any]:
        """å¤„ç†è®¾å¤‡æ¶ˆæ¯"""
        try:
            # è§£æè®¾å¤‡æ•°æ®
            device_data = self._parse_device_message(message)
            
            # æ•°æ®éªŒè¯å’Œæ¸…æ´—
            cleaned_data = self._validate_and_clean(device_data)
            
            # å®æ—¶å¤„ç†
            processing_results = self._real_time_processing(cleaned_data)
            
            # å­˜å‚¨åˆ°æ•°æ®åº“
            self._store_device_data(cleaned_data)
            
            # è§¦å‘ç›¸å…³ä¸šåŠ¡é€»è¾‘
            self._trigger_business_logic(cleaned_data, processing_results)
            
            self.processed_count += 1
            
            return {
                'status': 'success',
                'processed_data': cleaned_data,
                'processing_results': processing_results,
                'timestamp': time.time()
            }
            
        except Exception as e:
            error_info = {
                'status': 'error',
                'error': str(e),
                'message': message,
                'timestamp': time.time()
            }
            
            # è®°å½•é”™è¯¯ä½†ä¸ä¸­æ–­å¤„ç†æµç¨‹
            self._log_processing_error(error_info)
            return error_info
    
    def _parse_device_message(self, message: Dict[str, Any]) -> IoTDeviceData:
        """è§£æè®¾å¤‡æ¶ˆæ¯"""
        payload = json.loads(message['payload'])
        
        return IoTDeviceData(
            device_id=payload['deviceId'],
            timestamp=payload.get('timestamp', time.time()),
            sensor_data=payload.get('sensors', {}),
            location=payload.get('location', {'lat': 0, 'lng': 0}),
            battery_level=payload.get('battery', 100)
        )
    
    def _validate_and_clean(self, device_data: IoTDeviceData) -> IoTDeviceData:
        """éªŒè¯å’Œæ¸…æ´—æ•°æ®"""
        # æ•°æ®èŒƒå›´éªŒè¯
        for sensor_name, value in device_data.sensor_data.items():
            if sensor_name == 'temperature':
                # æ¸©åº¦èŒƒå›´éªŒè¯ (-40Â°C åˆ° 85Â°C)
                device_data.sensor_data[sensor_name] = max(-40, min(85, value))
            elif sensor_name == 'humidity':
                # æ¹¿åº¦èŒƒå›´éªŒè¯ (0% åˆ° 100%)
                device_data.sensor_data[sensor_name] = max(0, min(100, value))
            elif sensor_name == 'pressure':
                # æ°”å‹èŒƒå›´éªŒè¯ (300 hPa åˆ° 1100 hPa)
                device_data.sensor_data[sensor_name] = max(300, min(1100, value))
        
        # ç”µæ± ç”µé‡éªŒè¯
        device_data.battery_level = max(0, min(100, device_data.battery_level))
        
        return device_data
    
    def _real_time_processing(self, device_data: IoTDeviceData) -> Dict[str, Any]:
        """å®æ—¶æ•°æ®å¤„ç†"""
        results = {}
        
        # å¼‚å¸¸æ£€æµ‹
        anomalies = self._detect_anomalies(device_data)
        if anomalies:
            results['anomalies'] = anomalies
        
        # é¢„æµ‹æ€§ç»´æŠ¤
        maintenance_needed = self._predictive_maintenance(device_data)
        if maintenance_needed:
            results['maintenance_alert'] = maintenance_needed
        
        # ä¸šåŠ¡è§„åˆ™å¤„ç†
        business_actions = self._apply_business_rules(device_data)
        if business_actions:
            results['business_actions'] = business_actions
        
        return results
    
    def _detect_anomalies(self, device_data: IoTDeviceData) -> List[str]:
        """æ£€æµ‹å¼‚å¸¸æ•°æ®"""
        anomalies = []
        
        # æ¸©åº¦å¼‚å¸¸æ£€æµ‹
        temp = device_data.sensor_data.get('temperature', 0)
        if temp > 70 or temp < -30:
            anomalies.append(f"æ¸©åº¦å¼‚å¸¸: {temp}Â°C")
        
        # æ¹¿åº¦å¼‚å¸¸æ£€æµ‹
        humidity = device_data.sensor_data.get('humidity', 0)
        if humidity > 95 or humidity < 5:
            anomalies.append(f"æ¹¿åº¦è¿‡é«˜: {humidity}%")
        
        # ç”µæ± ä½ç”µé‡è­¦å‘Š
        if device_data.battery_level < 20:
            anomalies.append(f"ç”µæ± ç”µé‡ä½: {device_data.battery_level}%")
        
        return anomalies
    
    def _predictive_maintenance(self, device_data: IoTDeviceData) -> Dict[str, Any]:
        """é¢„æµ‹æ€§ç»´æŠ¤åˆ†æ"""
        maintenance_alert = {}
        
        # åŸºäºä¼ æ„Ÿå™¨æ•°æ®çš„è¶‹åŠ¿åˆ†æ
        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”ç”¨ä¸­éœ€è¦æœºå™¨å­¦ä¹ æ¨¡å‹
        
        vibration = device_data.sensor_data.get('vibration', 0)
        if vibration > 10:  # å‡è®¾æŒ¯åŠ¨é˜ˆå€¼
            maintenance_alert['type'] = 'predictive'
            maintenance_alert['component'] = 'motor'
            maintenance_alert['urgency'] = 'medium'
            maintenance_alert['recommendation'] = 'å»ºè®®æ£€æŸ¥ç”µæœºè½´æ‰¿'
        
        return maintenance_alert
    
    def _apply_business_rules(self, device_data: IoTDeviceData) -> List[Dict[str, Any]]:
        """åº”ç”¨ä¸šåŠ¡è§„åˆ™"""
        actions = []
        
        # åŸºäºä½ç½®çš„ä¸šåŠ¡é€»è¾‘
        lat, lng = device_data.location['lat'], device_data.location['lng']
        
        # ç¤ºä¾‹ï¼šå¦‚æœè®¾å¤‡è¿›å…¥ç‰¹å®šåŒºåŸŸ
        if 39.90 < lat < 39.95 and 116.35 < lng < 116.40:  # åŒ—äº¬æŸåŒºåŸŸ
            actions.append({
                'action': 'geofence_enter',
                'zone': 'restricted_area',
                'timestamp': device_data.timestamp
            })
        
        # åŸºäºæ—¶é—´çš„ä¸šåŠ¡é€»è¾‘
        current_hour = datetime.fromtimestamp(device_data.timestamp).hour
        if 22 <= current_hour or current_hour <= 6:  # å¤œé—´æ—¶æ®µ
            if device_data.sensor_data.get('motion', 0) > 0.5:
                actions.append({
                    'action': 'night_motion_detected',
                    'level': 'high',
                    'timestamp': device_data.timestamp
                })
        
        return actions
    
    def _store_device_data(self, device_data: IoTDeviceData):
        """å­˜å‚¨è®¾å¤‡æ•°æ®åˆ°æ•°æ®åº“"""
        item = {
            'deviceId': device_data.device_id,
            'timestamp': int(device_data.timestamp * 1000),  # DynamoDBéœ€è¦æ¯«ç§’æ—¶é—´æˆ³
            'sensorData': device_data.sensor_data,
            'location': device_data.location,
            'batteryLevel': device_data.battery_level,
            'processedAt': int(time.time() * 1000)
        }
        
        # ä½¿ç”¨æ‰¹å¤„ç†æé«˜å†™å…¥æ•ˆç‡
        self.table.put_item(Item=item)
    
    def _trigger_business_logic(self, device_data: IoTDeviceData, 
                              processing_results: Dict[str, Any]):
        """è§¦å‘ç›¸å…³ä¸šåŠ¡é€»è¾‘"""
        
        # å‘é€å‘Šè­¦é€šçŸ¥
        if 'anomalies' in processing_results:
            self._send_alerts(device_data, processing_results['anomalies'])
        
        # è§¦å‘ç»´æŠ¤å·¥å•
        if 'maintenance_alert' in processing_results:
            self._create_maintenance_ticket(device_data, processing_results['maintenance_alert'])
        
        # æ›´æ–°ä¸šåŠ¡ç³»ç»Ÿ
        if 'business_actions' in processing_results:
            self._update_business_systems(device_data, processing_results['business_actions'])
    
    def _send_alerts(self, device_data: IoTDeviceData, anomalies: List[str]):
        """å‘é€å‘Šè­¦é€šçŸ¥"""
        for anomaly in anomalies:
            alert_message = {
                'deviceId': device_data.device_id,
                'alertType': 'sensor_anomaly',
                'message': anomaly,
                'timestamp': device_data.timestamp,
                'severity': 'high'
            }
            
            # å‘é€åˆ°SNSä¸»é¢˜
            sns_client = boto3.client('sns')
            sns_client.publish(
                TopicArn='arn:aws:sns:us-east-1:123456789012:iot-alerts',
                Message=json.dumps(alert_message),
                Subject=f'è®¾å¤‡{device_data.device_id}å‘Šè­¦'
            )
    
    def _create_maintenance_ticket(self, device_data: IoTDeviceData, 
                                 maintenance_info: Dict[str, Any]):
        """åˆ›å»ºç»´æŠ¤å·¥å•"""
        ticket = {
            'deviceId': device_data.device_id,
            'ticketType': maintenance_info['type'],
            'component': maintenance_info['component'],
            'urgency': maintenance_info['urgency'],
            'description': maintenance_info['recommendation'],
            'createdAt': int(time.time() * 1000)
        }
        
        # å­˜å‚¨åˆ°ç»´æŠ¤å·¥å•è¡¨
        maintenance_table = self.dynamodb.Table('maintenance_tickets')
        maintenance_table.put_item(Item=ticket)
    
    def _update_business_systems(self, device_data: IoTDeviceData, 
                               actions: List[Dict[str, Any]]):
        """æ›´æ–°ä¸šåŠ¡ç³»ç»Ÿ"""
        for action in actions:
            if action['action'] == 'geofence_enter':
                # æ›´æ–°èµ„äº§ä½ç½®ç³»ç»Ÿ
                self._update_asset_location(device_data.device_id, device_data.location)
            
            elif action['action'] == 'night_motion_detected':
                # è§¦å‘å®‰é˜²ç³»ç»Ÿ
                self._trigger_security_system(device_data.device_id, action)
    
    def _update_asset_location(self, device_id: str, location: Dict[str, float]):
        """æ›´æ–°èµ„äº§ä½ç½®"""
        assets_table = self.dynamodb.Table('assets')
        assets_table.update_item(
            Key={'assetId': device_id},
            UpdateExpression='SET currentLocation = :loc, lastUpdated = :time',
            ExpressionAttributeValues={
                ':loc': location,
                ':time': int(time.time() * 1000)
            }
        )
    
    def _trigger_security_system(self, device_id: str, action: Dict[str, Any]):
        """è§¦å‘å®‰é˜²ç³»ç»Ÿ"""
        security_event = {
            'deviceId': device_id,
            'eventType': 'night_motion',
            'level': action['level'],
            'timestamp': action['timestamp']
        }
        
        # å‘é€åˆ°å®‰é˜²ç³»ç»Ÿé˜Ÿåˆ—
        sqs_client = boto3.client('sqs')
        sqs_client.send_message(
            QueueUrl='https://sqs.us-east-1.amazonaws.com/123456789012/security-events',
            MessageBody=json.dumps(security_event)
        )
    
    def get_processing_statistics(self) -> Dict[str, Any]:
        """è·å–å¤„ç†ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'processed_count': self.processed_count,
            'processing_rate': self.processed_count / (time.time() - getattr(self, 'start_time', time.time())),
            'uptime': time.time() - getattr(self, 'start_time', time.time()),
            'error_count': getattr(self, 'error_count', 0)
        }
    
    def _log_processing_error(self, error_info: Dict[str, Any]):
        """è®°å½•å¤„ç†é”™è¯¯"""
        # å‘é€åˆ°é”™è¯¯æ—¥å¿—è¡¨
        errors_table = self.dynamodb.Table('processing_errors')
        errors_table.put_item(Item=error_info)
        
        # æ›´æ–°é”™è¯¯è®¡æ•°
        self.error_count = getattr(self, 'error_count', 0) + 1

# Lambdaå¤„ç†å™¨å‡½æ•°
def lambda_handler(event, context):
    """IoTæ•°æ®å¤„ç†Lambdaå‡½æ•°"""
    
    # åˆå§‹åŒ–å¤„ç†å™¨
    processor = IoTDataProcessor('iot_device_data')
    
    # è®°å½•å¼€å§‹æ—¶é—´
    processor.start_time = time.time()
    
    results = []
    
    # å¤„ç†æ‰¹é‡æ¶ˆæ¯
    for record in event['Records']:
        # ä»SQS/Kinesisè·å–æ¶ˆæ¯
        if 'body' in record:
            message = json.loads(record['body'])
        else:
            message = json.loads(record['kinesis']['data'])
        
        # å¤„ç†å•æ¡æ¶ˆæ¯
        result = processor.process_device_message(message)
        results.append(result)
    
    # è¿”å›å¤„ç†ç»Ÿè®¡
    stats = processor.get_processing_statistics()
    
    return {
        'statusCode': 200,
        'body': json.dumps({
            'results': results,
            'statistics': stats
        })
    }

# æ‰¹é‡å¤„ç†ä¼˜åŒ–ç‰ˆæœ¬
def batch_processor_lambda(event, context):
    """æ‰¹é‡å¤„ç†ä¼˜åŒ–ç‰ˆæœ¬"""
    
    processor = IoTDataProcessor('iot_device_data')
    processor.start_time = time.time()
    
    # æ‰¹é‡å¤„ç†æ¶ˆæ¯
    batch_size = 25  # DynamoDBæ‰¹å¤„ç†å¤§å°é™åˆ¶
    records = event['Records']
    
    for i in range(0, len(records), batch_size):
        batch = records[i:i + batch_size]
        
        # å‡†å¤‡æ‰¹å¤„ç†å†™å…¥
        batch_items = []
        for record in batch:
            message = json.loads(record['body'])
            device_data = processor._parse_device_message(message)
            cleaned_data = processor._validate_and_clean(device_data)
            
            item = {
                'PutRequest': {
                    'Item': {
                        'deviceId': cleaned_data.device_id,
                        'timestamp': int(cleaned_data.timestamp * 1000),
                        'sensorData': cleaned_data.sensor_data,
                        'location': cleaned_data.location,
                        'batteryLevel': cleaned_data.battery_level,
                        'processedAt': int(time.time() * 1000)
                    }
                }
            }
            batch_items.append(item)
        
        # æ‰§è¡Œæ‰¹å¤„ç†å†™å…¥
        if batch_items:
            processor.dynamodb.batch_write_item(
                RequestItems={
                    'iot_device_data': batch_items
                }
            )
        
        processor.processed_count += len(batch)
    
    return {
        'statusCode': 200,
        'body': json.dumps(processor.get_processing_statistics())
    }
```

---

## ğŸ” å…³é”®è¦ç‚¹æ€»ç»“

### âœ… Serverlessæ•°æ®åº“æˆåŠŸè¦ç´ 
- **åˆç†çš„å·¥ä½œè´Ÿè½½åŒ¹é…**ï¼šé€‰æ‹©é€‚åˆçªå‘æ€§ã€ä¸å¯é¢„æµ‹å·¥ä½œè´Ÿè½½çš„åœºæ™¯
- **æˆæœ¬æ•ˆç›Šåˆ†æ**ï¼šè¯¦ç»†æµ‹ç®—Serverlessä¸ä¼ ç»Ÿæ–¹æ¡ˆçš„æˆæœ¬å·®å¼‚
- **æ€§èƒ½ä¼˜åŒ–è®¾è®¡**ï¼šé€šè¿‡ç¼“å­˜ã€æ‰¹å¤„ç†ç­‰æ–¹å¼ä¼˜åŒ–æ€§èƒ½
- **ç›‘æ§å‘Šè­¦ä½“ç³»**ï¼šå»ºç«‹å®Œå–„çš„å¯è§‚æµ‹æ€§ç³»ç»Ÿ

### âš ï¸ å¸¸è§é£é™©æé†’
- **å†·å¯åŠ¨å»¶è¿Ÿ**ï¼šé¦–æ¬¡è°ƒç”¨å¯èƒ½äº§ç”Ÿçš„å»¶è¿Ÿå½±å“ç”¨æˆ·ä½“éªŒ
- **æˆæœ¬å¤±æ§é£é™©**ï¼šé«˜é¢‘ç‡è°ƒç”¨å¯èƒ½å¯¼è‡´æ„å¤–çš„é«˜é¢è´¹ç”¨
- **ä¾›åº”å•†é”å®š**ï¼šæ·±åº¦ä¾èµ–ç‰¹å®šäº‘æœåŠ¡å•†çš„ServerlessæœåŠ¡
- **è°ƒè¯•å¤æ‚æ€§**ï¼šåˆ†å¸ƒå¼æ— æœåŠ¡å™¨ç¯å¢ƒä¸‹çš„æ•…éšœæ’æŸ¥éš¾åº¦è¾ƒå¤§

### ğŸ¯ æœ€ä½³å®è·µå»ºè®®
1. **æ¸è¿›å¼é‡‡ç”¨**ï¼šä»éæ ¸å¿ƒä¸šåŠ¡å¼€å§‹è¯•ç‚¹Serverlessæ•°æ®åº“
2. **æˆæœ¬ç›‘æ§**ï¼šå»ºç«‹å®æ—¶æˆæœ¬ç›‘æ§å’Œé¢„ç®—å‘Šè­¦æœºåˆ¶
3. **æ€§èƒ½åŸºå‡†æµ‹è¯•**ï¼šåœ¨ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²å‰è¿›è¡Œå……åˆ†çš„æ€§èƒ½æµ‹è¯•
4. **æ··åˆæ¶æ„**ï¼šç»“åˆServerlesså’Œä¼ ç»Ÿæ¶æ„å‘æŒ¥å„è‡ªä¼˜åŠ¿
5. **å›¢é˜ŸæŠ€èƒ½åŸ¹è®­**ï¼šæå‡å›¢é˜Ÿå¯¹Serverlessæ¶æ„çš„ç†è§£å’Œå®è·µèƒ½åŠ›

é€šè¿‡ç§‘å­¦çš„Serverlessæ•°æ®åº“é€‰å‹å’Œå®æ–½ï¼Œä¼ä¸šå¯ä»¥åœ¨åˆé€‚çš„åœºæ™¯ä¸‹è·å¾—æ˜¾è‘—çš„æˆæœ¬ä¼˜åŠ¿å’Œè¿ç»´æ•ˆç‡æå‡ï¼ŒåŒæ—¶äº«å—äº‘åŸç”Ÿæ¶æ„å¸¦æ¥çš„å¼¹æ€§æ‰©å±•èƒ½åŠ›ã€‚