# æ•°æ®åº“è´Ÿè½½å‡è¡¡é…ç½®å®Œæ•´æŒ‡å—

## ğŸ¯ æ¦‚è¿°

æ•°æ®åº“è´Ÿè½½å‡è¡¡æ˜¯æå‡ç³»ç»Ÿæ€§èƒ½å’Œå¯ç”¨æ€§çš„å…³é”®æŠ€æœ¯ï¼Œé€šè¿‡å°†æ•°æ®åº“è¯·æ±‚åˆ†å‘åˆ°å¤šä¸ªæœåŠ¡å™¨å®ä¾‹ï¼Œå®ç°èµ„æºçš„åˆç†åˆ©ç”¨å’Œæ•…éšœå®¹é”™ã€‚æœ¬æŒ‡å—æä¾›ä»åŸºç¡€ç†è®ºåˆ°ä¼ä¸šçº§å®è·µçš„å®Œæ•´è´Ÿè½½å‡è¡¡è§£å†³æ–¹æ¡ˆã€‚

## ğŸ“‹ ç›®å½•

1. [è´Ÿè½½å‡è¡¡åŸºç¡€ç†è®º](#1-è´Ÿè½½å‡è¡¡åŸºç¡€ç†è®º)
2. [MySQL Proxyé…ç½®](#2-mysql-proxyé…ç½®)
3. [PostgreSQL PgBounceré…ç½®](#3-postgresql-pgbounceré…ç½®)
4. [MongoDBè´Ÿè½½å‡è¡¡](#4-mongodbè´Ÿè½½å‡è¡¡)
5. [Redisé›†ç¾¤è´Ÿè½½å‡è¡¡](#5-redisé›†ç¾¤è´Ÿè½½å‡è¡¡)
6. [è´Ÿè½½å‡è¡¡ç­–ç•¥ä¼˜åŒ–](#6-è´Ÿè½½å‡è¡¡ç­–ç•¥ä¼˜åŒ–)
7. [ç›‘æ§ä¸æ•…éšœå¤„ç†](#7-ç›‘æ§ä¸æ•…éšœå¤„ç†)

---

## 1. è´Ÿè½½å‡è¡¡åŸºç¡€ç†è®º

### 1.1 æ ¸å¿ƒæ¦‚å¿µä¸æ¶æ„

#### è´Ÿè½½å‡è¡¡æ¶æ„æ¨¡å¼
```mermaid
graph TD
    A[å®¢æˆ·ç«¯åº”ç”¨] --> B[è´Ÿè½½å‡è¡¡å™¨]
    B --> C[æ•°æ®åº“èŠ‚ç‚¹1]
    B --> D[æ•°æ®åº“èŠ‚ç‚¹2]
    B --> E[æ•°æ®åº“èŠ‚ç‚¹3]
    
    subgraph "è´Ÿè½½å‡è¡¡å±‚"
        B --> F[è¿æ¥æ± ç®¡ç†]
        B --> G[å¥åº·æ£€æŸ¥]
        B --> H[æ•…éšœè½¬ç§»]
        B --> I[ä¼šè¯ä¿æŒ]
    end
    
    subgraph "æ•°æ®åº“é›†ç¾¤"
        C --> J[ä¸»èŠ‚ç‚¹]
        D --> K[ä»èŠ‚ç‚¹]
        E --> L[ä»èŠ‚ç‚¹]
    end
    
    M[ç›‘æ§ç³»ç»Ÿ] --> B
    M --> C
    M --> D
    M --> E
```

#### è´Ÿè½½å‡è¡¡ä¼˜åŠ¿åˆ†æ
```yaml
load_balancing_advantages:
  performance_improvement:
    description: "æ€§èƒ½æå‡"
    benefits: 
      - "è¯·æ±‚åˆ†å‘åˆ°å¤šä¸ªå®ä¾‹"
      - "å¹¶å‘å¤„ç†èƒ½åŠ›å¢å¼º"
      - "å“åº”æ—¶é—´ä¼˜åŒ–"
    metrics: "é€šå¸¸å¯æå‡2-5å€å¤„ç†èƒ½åŠ›"
  
  high_availability:
    description: "é«˜å¯ç”¨æ€§"
    benefits:
      - "å•ç‚¹æ•…éšœå®¹é”™"
      - "è‡ªåŠ¨æ•…éšœè½¬ç§»"
      - "æœåŠ¡è¿ç»­æ€§ä¿éšœ"
    metrics: "å¯ç”¨æ€§å¯è¾¾99.99%ä»¥ä¸Š"
  
  scalability:
    description: "å¯æ‰©å±•æ€§"
    benefits:
      - "æ°´å¹³æ‰©å±•æ”¯æŒ"
      - "åŠ¨æ€èŠ‚ç‚¹å¢å‡"
      - "èµ„æºå¼¹æ€§ä¼¸ç¼©"
    metrics: "æ”¯æŒæ•°ç™¾èŠ‚ç‚¹é›†ç¾¤"
  
  resource_optimization:
    description: "èµ„æºä¼˜åŒ–"
    benefits:
      - "è´Ÿè½½å‡åŒ€åˆ†å¸ƒ"
      - "èµ„æºåˆ©ç”¨ç‡æå‡"
      - "æˆæœ¬æ•ˆç›Šæœ€å¤§åŒ–"
    metrics: "èµ„æºåˆ©ç”¨ç‡æå‡30-50%"
```

### 1.2 è´Ÿè½½å‡è¡¡ç®—æ³•

#### ä¸»è¦è´Ÿè½½å‡è¡¡ç®—æ³•
```python
# è´Ÿè½½å‡è¡¡ç®—æ³•å®ç°
class LoadBalancingAlgorithms:
    def __init__(self, servers):
        self.servers = servers
        self.current_index = 0
        self.request_counts = {server: 0 for server in servers}
        self.response_times = {server: [] for server in servers}
    
    def round_robin(self):
        """è½®è¯¢ç®—æ³•"""
        server = self.servers[self.current_index]
        self.current_index = (self.current_index + 1) % len(self.servers)
        self.request_counts[server] += 1
        return server
    
    def weighted_round_robin(self, weights):
        """åŠ æƒè½®è¯¢ç®—æ³•"""
        total_weight = sum(weights.values())
        rand_num = random.randint(1, total_weight)
        
        current_weight = 0
        for server, weight in weights.items():
            current_weight += weight
            if rand_num <= current_weight:
                self.request_counts[server] += 1
                return server
    
    def least_connections(self, current_connections):
        """æœ€å°‘è¿æ¥ç®—æ³•"""
        server = min(current_connections.keys(), 
                    key=lambda s: current_connections[s])
        self.request_counts[server] += 1
        return server
    
    def weighted_least_connections(self, current_connections, weights):
        """åŠ æƒæœ€å°‘è¿æ¥ç®—æ³•"""
        weighted_connections = {
            server: current_connections[server] / weights[server] 
            for server in self.servers
        }
        server = min(weighted_connections.keys(), 
                    key=lambda s: weighted_connections[s])
        self.request_counts[server] += 1
        return server
    
    def response_time_based(self):
        """å“åº”æ—¶é—´ç®—æ³•"""
        avg_response_times = {
            server: sum(times) / len(times) if times else 0 
            for server, times in self.response_times.items()
        }
        server = min(avg_response_times.keys(), 
                    key=lambda s: avg_response_times[s])
        self.request_counts[server] += 1
        return server
    
    def consistent_hashing(self, key):
        """ä¸€è‡´æ€§å“ˆå¸Œç®—æ³•"""
        import hashlib
        hash_value = int(hashlib.md5(key.encode()).hexdigest(), 16)
        server_index = hash_value % len(self.servers)
        server = self.servers[server_index]
        self.request_counts[server] += 1
        return server

# ä½¿ç”¨ç¤ºä¾‹
servers = ['db1.example.com', 'db2.example.com', 'db3.example.com']
lb = LoadBalancingAlgorithms(servers)

# ä¸åŒç®—æ³•çš„åº”ç”¨åœºæ™¯
algorithms_use_cases = {
    'round_robin': 'è¯·æ±‚å‡åŒ€åˆ†å¸ƒçš„åœºæ™¯',
    'weighted_round_robin': 'æœåŠ¡å™¨é…ç½®ä¸åŒçš„åœºæ™¯',
    'least_connections': 'è¿æ¥æ•°å·®å¼‚è¾ƒå¤§çš„åœºæ™¯',
    'response_time_based': 'å“åº”æ—¶é—´æ•æ„Ÿçš„åœºæ™¯',
    'consistent_hashing': 'éœ€è¦ä¼šè¯ä¿æŒçš„åœºæ™¯'
}
```

## 2. MySQL Proxyé…ç½®

### 2.1 MySQL Routeré…ç½®

#### åŸºç¡€é…ç½®æ–‡ä»¶
```ini
# mysqlrouter.conf - MySQL Routeré…ç½®
[DEFAULT]
logging_folder = /var/log/mysqlrouter
plugin_folder = /usr/lib/mysqlrouter
runtime_folder = /var/run/mysqlrouter
config_folder = /etc/mysqlrouter

[logger]
level = INFO

[metadata_cache:bootstrap]
cluster_type = gr
router_id = 1
metadata_cluster = mycluster
user = mysql_router
password = router_password
ttl = 5

[routing:bootstrap_rw]
bind_address = 0.0.0.0
bind_port = 6446
destinations = metadata-cache://mycluster/default?role=PRIMARY
routing_strategy = first-available
protocol = classic

[routing:bootstrap_ro]
bind_address = 0.0.0.0
bind_port = 6447
destinations = metadata-cache://mycluster/default?role=SECONDARY
routing_strategy = round-robin-with-fallback
protocol = classic

[routing:bootstrap_x_rw]
bind_address = 0.0.0.0
bind_port = 6448
destinations = metadata-cache://mycluster/default?role=PRIMARY
routing_strategy = first-available
protocol = x

[routing:bootstrap_x_ro]
bind_address = 0.0.0.0
bind_port = 6449
destinations = metadata-cache://mycluster/default?role=SECONDARY
routing_strategy = round-robin-with-fallback
protocol = x
```

#### é«˜çº§é…ç½®ä¼˜åŒ–
```ini
# mysqlrouter_advanced.conf - é«˜çº§é…ç½®
[DEFAULT]
connect_timeout = 5
read_timeout = 30
dynamic_state = /var/lib/mysqlrouter/state.json
client_ssl_cert = /etc/ssl/certs/client-cert.pem
client_ssl_key = /etc/ssl/private/client-key.pem
client_ssl_mode = REQUIRED

[connection_pool]
max_size = 100
min_size = 10
max_idle_time = 300

[health_monitor]
monitor_interval = 2
failure_detector_threshold = 3
failure_detector_window = 10

[routing:advanced_rw]
bind_address = 0.0.0.0
bind_port = 7446
destinations = metadata-cache://mycluster/default?role=PRIMARY
routing_strategy = first-available
protocol = classic
connection_delay = 100
max_connections = 2000
client_connect_timeout = 10
server_connect_timeout = 5
```

### 2.2 ProxySQLé…ç½®

#### ProxySQLæ ¸å¿ƒé…ç½®
```sql
-- proxysql_admin.sql - ProxySQLç®¡ç†é…ç½®
-- æ·»åŠ MySQLæœåŠ¡å™¨
INSERT INTO mysql_servers(hostgroup_id, hostname, port, weight, status) 
VALUES 
(1, 'mysql-master.example.com', 3306, 1000, 'ONLINE'),
(2, 'mysql-slave1.example.com', 3306, 100, 'ONLINE'),
(2, 'mysql-slave2.example.com', 3306, 100, 'ONLINE');

-- é…ç½®è¯»å†™åˆ†ç¦»è§„åˆ™
INSERT INTO mysql_query_rules(rule_id, active, match_digest, destination_hostgroup, apply) 
VALUES 
(1, 1, '^SELECT.*FOR UPDATE$', 1, 1),
(2, 1, '^SELECT', 2, 1),
(3, 1, '^(INSERT|UPDATE|DELETE)', 1, 1);

-- åˆ›å»ºç”¨æˆ·
INSERT INTO mysql_users(username, password, default_hostgroup, transaction_persistent) 
VALUES ('app_user', 'encrypted_password', 1, 1);

-- é…ç½®è¿æ¥æ± 
UPDATE global_variables SET variable_value='1000' WHERE variable_name='mysql-max_connections';
UPDATE global_variables SET variable_value='50' WHERE variable_name='mysql-default_max_latency_ms';

-- åŠ è½½é…ç½®åˆ°è¿è¡Œæ—¶
LOAD MYSQL SERVERS TO RUNTIME;
LOAD MYSQL QUERY RULES TO RUNTIME;
LOAD MYSQL USERS TO RUNTIME;
SAVE MYSQL SERVERS TO DISK;
SAVE MYSQL QUERY RULES TO DISK;
SAVE MYSQL USERS TO DISK;
```

#### ç›‘æ§å’Œç»Ÿè®¡é…ç½®
```sql
-- proxysql_monitoring.sql - ç›‘æ§é…ç½®
-- å¯ç”¨ç›‘æ§
UPDATE global_variables SET variable_value='true' WHERE variable_name='mysql-monitor_enabled';
UPDATE global_variables SET variable_value='1000' WHERE variable_name='mysql-monitor_ping_interval';
UPDATE global_variables SET variable_value='2000' WHERE variable_name='mysql-monitor_read_only_interval';

-- é…ç½®ç›‘æ§ç”¨æˆ·
INSERT INTO mysql_users(username, password, use_ssl, default_hostgroup, transaction_persistent) 
VALUES ('monitor', 'monitor_password', 0, 1, 0);

-- æŸ¥çœ‹ç»Ÿè®¡ä¿¡æ¯
SELECT * FROM stats_mysql_global;
SELECT * FROM stats_mysql_connection_pool;
SELECT * FROM stats_mysql_commands_counters;
SELECT * FROM stats_mysql_query_digest ORDER BY sum_time DESC LIMIT 10;
```

## 3. PostgreSQL PgBounceré…ç½®

### 3.1 PgBounceråŸºç¡€é…ç½®

#### pgbouncer.inié…ç½®æ–‡ä»¶
```ini
# pgbouncer.ini - PgBounceré…ç½®
[databases]
mydb = host=pg-master.example.com port=5432 dbname=mydb pool_size=50
mydb_ro = host=pg-slave1.example.com port=5432 dbname=mydb pool_size=30

[pgbouncer]
# ç½‘ç»œè®¾ç½®
listen_addr = 0.0.0.0
listen_port = 6432
unix_socket_dir = /var/run/postgresql

# è®¤è¯è®¾ç½®
auth_type = md5
auth_file = /etc/pgbouncer/userlist.txt

# è¿æ¥æ± è®¾ç½®
pool_mode = transaction
default_pool_size = 20
min_pool_size = 5
reserve_pool_size = 5
reserve_pool_timeout = 3
max_client_conn = 500
max_db_connections = 100

# è¶…æ—¶è®¾ç½®
server_reset_query = DISCARD ALL
server_check_delay = 30
server_lifetime = 3600
server_idle_timeout = 600
client_idle_timeout = 0
client_login_timeout = 60
autodb_idle_timeout = 3600

# æ—¥å¿—è®¾ç½®
logfile = /var/log/pgbouncer/pgbouncer.log
pidfile = /var/run/pgbouncer/pgbouncer.pid
log_connections = 1
log_disconnections = 1
log_pooler_errors = 1

# ç»Ÿè®¡è®¾ç½®
stats_period = 60
```

#### ç”¨æˆ·è®¤è¯æ–‡ä»¶
```txt
# userlist.txt - ç”¨æˆ·è®¤è¯æ–‡ä»¶
"postgres" "md53175bce1d3201d16594cebf9d7eb3f9d"
"app_user" "md5a3cde912f8c4d56123456789abcdef01"
"readonly_user" "md5b4def023e9d6e7f890123456789abcdef"
```

### 3.2 é«˜çº§PgBounceré…ç½®

#### è¯»å†™åˆ†ç¦»é…ç½®
```ini
# pgbouncer_advanced.ini - é«˜çº§é…ç½®
[databases]
# å†™æ“ä½œè·¯ç”±åˆ°ä¸»åº“
write_db = host=pg-master.example.com port=5432 dbname=mydb pool_mode=transaction pool_size=30
# è¯»æ“ä½œè·¯ç”±åˆ°ä»åº“
read_db = host=pg-slave1.example.com port=5432 dbname=mydb pool_mode=session pool_size=50

# å¤‡ç”¨ä»åº“
read_db_backup = host=pg-slave2.example.com port=5432 dbname=mydb pool_mode=session pool_size=50

[pgbouncer]
# æ™ºèƒ½æ± æ¨¡å¼
pool_mode = statement
default_pool_size = 25
min_pool_size = 10
reserve_pool_size = 10

# è¿æ¥å¤ç”¨ä¼˜åŒ–
server_reset_query_always = 0
ignore_startup_parameters = extra_float_digits

# æ•…éšœè½¬ç§»æ”¯æŒ
server_fast_close = 1
server_check_query = select 1
server_check_delay = 10

# æ€§èƒ½ç›‘æ§
query_timeout = 300
query_wait_timeout = 120
client_idle_timeout = 300
```

#### åŠ¨æ€é…ç½®ç®¡ç†
```python
# pgbouncer_manager.py - PgBouncerç®¡ç†è„šæœ¬
import subprocess
import json
import time

class PgBouncerManager:
    def __init__(self, admin_console_port=6432):
        self.admin_port = admin_console_port
        self.stats_cache = {}
    
    def get_stats(self):
        """è·å–PgBouncerç»Ÿè®¡ä¿¡æ¯"""
        cmd = f"echo 'SHOW STATS;' | psql -p {self.admin_port} -d pgbouncer"
        result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
        return result.stdout
    
    def get_pools(self):
        """è·å–è¿æ¥æ± çŠ¶æ€"""
        cmd = f"echo 'SHOW POOLS;' | psql -p {self.admin_port} -d pgbouncer"
        result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
        return result.stdout
    
    def pause_pool(self, database):
        """æš‚åœæŒ‡å®šæ•°æ®åº“çš„è¿æ¥æ± """
        cmd = f"echo 'PAUSE {database};' | psql -p {self.admin_port} -d pgbouncer"
        subprocess.run(cmd, shell=True)
    
    def resume_pool(self, database):
        """æ¢å¤æŒ‡å®šæ•°æ®åº“çš„è¿æ¥æ± """
        cmd = f"echo 'RESUME {database};' | psql -p {self.admin_port} -d pgbouncer"
        subprocess.run(cmd, shell=True)
    
    def reload_config(self):
        """é‡æ–°åŠ è½½é…ç½®"""
        cmd = f"echo 'RELOAD;' | psql -p {self.admin_port} -d pgbouncer"
        subprocess.run(cmd, shell=True)
    
    def enable_failover(self, failed_host, backup_host):
        """å¯ç”¨æ•…éšœè½¬ç§»"""
        # è¿™é‡Œåº”è¯¥å®ç°å…·ä½“çš„æ•…éšœè½¬ç§»é€»è¾‘
        print(f"åˆ‡æ¢ä» {failed_host} åˆ° {backup_host}")
        # æ›´æ–°é…ç½®æ–‡ä»¶
        # é‡æ–°åŠ è½½é…ç½®
        self.reload_config()

# ä½¿ç”¨ç¤ºä¾‹
manager = PgBouncerManager()
stats = manager.get_stats()
print("å½“å‰ç»Ÿè®¡:", stats)

# ç›‘æ§è¿æ¥æ± å¥åº·çŠ¶æ€
def monitor_pools():
    while True:
        pools = manager.get_pools()
        # è§£æå¹¶æ£€æŸ¥æ± çŠ¶æ€
        # å¦‚æœå‘ç°å¼‚å¸¸ï¼Œè§¦å‘å‘Šè­¦æˆ–è‡ªåŠ¨ä¿®å¤
        time.sleep(60)
```

## 4. MongoDBè´Ÿè½½å‡è¡¡

### 4.1 MongoDBåˆ†ç‰‡é›†ç¾¤é…ç½®

#### åˆ†ç‰‡é›†ç¾¤æ¶æ„é…ç½®
```javascript
// mongosé…ç½®æ–‡ä»¶
sharding:
  configDB: configRepl/192.168.1.10:27019,192.168.1.11:27019,192.168.1.12:27019
  
processManagement:
  fork: true
  pidFilePath: /var/run/mongodb/mongos.pid

net:
  bindIp: 0.0.0.0
  port: 27017

security:
  keyFile: /opt/mongo/keyfile
  
setParameter:
  connPoolMaxShardedConnsPerHost: 200
  connPoolMaxConnsPerHost: 200
```

#### åˆ†ç‰‡é”®è®¾è®¡å’Œé…ç½®
```javascript
// åˆ†ç‰‡é…ç½®è„šæœ¬
// å¯ç”¨åˆ†ç‰‡
sh.enableSharding("myDatabase")

// é€‰æ‹©åˆé€‚çš„åˆ†ç‰‡é”®
// åŸºäºä¸šåŠ¡æŸ¥è¯¢æ¨¡å¼é€‰æ‹©åˆ†ç‰‡é”®
sh.shardCollection("myDatabase.users", { "userId": 1 })
sh.shardCollection("myDatabase.orders", { "customerId": 1, "orderDate": 1 })

// é¢„åˆ†ç‰‡ä»¥é¿å…çƒ­ç‚¹
for (var i = 0; i < 100; i++) {
    sh.splitAt("myDatabase.users", { "userId": i * 10000 })
}

// åˆ†å¸ƒåˆ†ç‰‡åˆ°ä¸åŒåˆ†ç‰‡
sh.moveChunk("myDatabase.users", { "userId": 0 }, "shard0000")
sh.moveChunk("myDatabase.users", { "userId": 500000 }, "shard0001")

// ç›‘æ§åˆ†ç‰‡çŠ¶æ€
sh.status()
db.printShardingStatus()
```

### 4.2 MongoDBé©±åŠ¨ç¨‹åºè´Ÿè½½å‡è¡¡

#### è¿æ¥å­—ç¬¦ä¸²é…ç½®
```python
# mongodb_load_balancing.py - MongoDBè´Ÿè½½å‡è¡¡é…ç½®
from pymongo import MongoClient
from pymongo.read_preferences import ReadPreference

class MongoDBLoadBalancer:
    def __init__(self):
        # è¿æ¥å­—ç¬¦ä¸²é…ç½®
        self.connection_string = (
            "mongodb://user:password@"
            "mongos1.example.com:27017,"
            "mongos2.example.com:27017,"
            "mongos3.example.com:27017/"
            "myDatabase?"
            "readPreference=secondaryPreferred&"
            "maxPoolSize=100&"
            "minPoolSize=10&"
            "maxIdleTimeMS=30000&"
            "waitQueueMultiple=2&"
            "retryWrites=true&"
            "retryReads=true"
        )
        
        self.client = MongoClient(self.connection_string)
        self.db = self.client.myDatabase
    
    def get_read_connection(self):
        """è·å–è¯»æ“ä½œè¿æ¥"""
        # ä½¿ç”¨è¯»åå¥½è®¾ç½®
        return self.client.myDatabase.with_options(
            read_preference=ReadPreference.SECONDARY_PREFERRED
        )
    
    def get_write_connection(self):
        """è·å–å†™æ“ä½œè¿æ¥"""
        # å†™æ“ä½œæ€»æ˜¯è·¯ç”±åˆ°ä¸»èŠ‚ç‚¹
        return self.client.myDatabase.with_options(
            read_preference=ReadPreference.PRIMARY
        )
    
    def execute_balanced_query(self, collection_name, query, read_only=False):
        """æ‰§è¡Œè´Ÿè½½å‡è¡¡æŸ¥è¯¢"""
        if read_only:
            collection = self.get_read_connection()[collection_name]
        else:
            collection = self.get_write_connection()[collection_name]
        
        return collection.find(query)

# ä½¿ç”¨ç¤ºä¾‹
balancer = MongoDBLoadBalancer()

# è¯»æ“ä½œ - è‡ªåŠ¨è·¯ç”±åˆ°ä»èŠ‚ç‚¹
users = balancer.execute_balanced_query("users", {"status": "active"}, read_only=True)

# å†™æ“ä½œ - è·¯ç”±åˆ°ä¸»èŠ‚ç‚¹
balancer.execute_balanced_query("users", {"$set": {"lastLogin": "2024-01-01"}}, read_only=False)
```

## 5. Redisé›†ç¾¤è´Ÿè½½å‡è¡¡

### 5.1 Redis Clusteré…ç½®

#### Redis ClusterèŠ‚ç‚¹é…ç½®
```conf
# redis.conf - Redisé›†ç¾¤èŠ‚ç‚¹é…ç½®
port 7000
bind 0.0.0.0
daemonize yes
pidfile /var/run/redis/redis-7000.pid
logfile /var/log/redis/redis-7000.log
dir /var/lib/redis/7000

# é›†ç¾¤é…ç½®
cluster-enabled yes
cluster-config-file nodes-7000.conf
cluster-node-timeout 15000
cluster-announce-ip 192.168.1.10
cluster-announce-port 7000
cluster-announce-bus-port 17000

# æ€§èƒ½ä¼˜åŒ–
tcp-keepalive 300
timeout 0
tcp-backlog 511
maxclients 10000

# å†…å­˜é…ç½®
maxmemory 2gb
maxmemory-policy allkeys-lru
```

#### é›†ç¾¤åˆ›å»ºè„šæœ¬
```bash
#!/bin/bash
# create_redis_cluster.sh - Redisé›†ç¾¤åˆ›å»ºè„šæœ¬

# èŠ‚ç‚¹é…ç½®
NODES=(
    "192.168.1.10:7000"
    "192.168.1.11:7001" 
    "192.168.1.12:7002"
    "192.168.1.13:7003"
    "192.168.1.14:7004"
    "192.168.1.15:7005"
)

# å¯åŠ¨æ‰€æœ‰èŠ‚ç‚¹
start_nodes() {
    for node in "${NODES[@]}"; do
        host=$(echo $node | cut -d: -f1)
        port=$(echo $node | cut -d: -f2)
        
        ssh $host "redis-server /etc/redis/redis-$port.conf"
        echo "Started Redis node on $host:$port"
    done
}

# åˆ›å»ºé›†ç¾¤
create_cluster() {
    echo "Creating Redis cluster..."
    echo "yes" | redis-cli --cluster create ${NODES[*]} --cluster-replicas 1
}

# éªŒè¯é›†ç¾¤çŠ¶æ€
verify_cluster() {
    redis-cli -c -p 7000 cluster nodes
    redis-cli -c -p 7000 cluster info
}

# æ‰§è¡Œéƒ¨ç½²
start_nodes
sleep 10
create_cluster
verify_cluster
```

### 5.2 Rediså®¢æˆ·ç«¯è´Ÿè½½å‡è¡¡

#### æ™ºèƒ½å®¢æˆ·ç«¯é…ç½®
```python
# redis_cluster_client.py - Redisé›†ç¾¤å®¢æˆ·ç«¯
import redis
from redis.cluster import RedisCluster
import time
import random

class RedisClusterLoadBalancer:
    def __init__(self, startup_nodes):
        self.startup_nodes = startup_nodes
        self.cluster = RedisCluster(
            startup_nodes=startup_nodes,
            decode_responses=True,
            skip_full_coverage_check=True,
            health_check_interval=30,
            max_connections=50
        )
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_requests': 0,
            'node_distribution': {},
            'latency_stats': []
        }
    
    def get_balanced_connection(self, key):
        """è·å–è´Ÿè½½å‡è¡¡çš„è¿æ¥"""
        # Redis Clusterä¼šè‡ªåŠ¨å¤„ç†é”®çš„åˆ†ç‰‡å’Œè·¯ç”±
        return self.cluster
    
    def execute_with_retry(self, operation, *args, **kwargs):
        """å¸¦é‡è¯•æœºåˆ¶çš„æ“ä½œæ‰§è¡Œ"""
        max_retries = 3
        for attempt in range(max_retries):
            try:
                result = operation(*args, **kwargs)
                self.stats['total_requests'] += 1
                return result
            except redis.RedisError as e:
                if attempt == max_retries - 1:
                    raise e
                time.sleep(0.1 * (2 ** attempt))  # æŒ‡æ•°é€€é¿
    
    def get_cluster_stats(self):
        """è·å–é›†ç¾¤ç»Ÿè®¡ä¿¡æ¯"""
        info = {}
        
        # è·å–å„ä¸ªèŠ‚ç‚¹ä¿¡æ¯
        for node in self.startup_nodes:
            try:
                node_client = redis.Redis(host=node['host'], port=node['port'])
                node_info = node_client.info()
                info[f"{node['host']}:{node['port']}"] = {
                    'used_memory': node_info.get('used_memory_human', 'N/A'),
                    'connected_clients': node_info.get('connected_clients', 0),
                    'ops_per_sec': node_info.get('instantaneous_ops_per_sec', 0)
                }
            except Exception as e:
                info[f"{node['host']}:{node['port']}"] = {'error': str(e)}
        
        return info

# ä½¿ç”¨ç¤ºä¾‹
startup_nodes = [
    {'host': '192.168.1.10', 'port': '7000'},
    {'host': '192.168.1.11', 'port': '7001'},
    {'host': '192.168.1.12', 'port': '7002'}
]

balancer = RedisClusterLoadBalancer(startup_nodes)

# æ‰§è¡Œè´Ÿè½½å‡è¡¡æ“ä½œ
balancer.execute_with_retry(balancer.cluster.set, 'key1', 'value1')
result = balancer.execute_with_retry(balancer.cluster.get, 'key1')

# è·å–é›†ç¾¤çŠ¶æ€
stats = balancer.get_cluster_stats()
print("é›†ç¾¤ç»Ÿè®¡:", stats)
```

## 6. è´Ÿè½½å‡è¡¡ç­–ç•¥ä¼˜åŒ–

### 6.1 è‡ªé€‚åº”è´Ÿè½½å‡è¡¡

#### æ™ºèƒ½è´Ÿè½½å‡è¡¡ç®—æ³•
```python
# adaptive_load_balancer.py - è‡ªé€‚åº”è´Ÿè½½å‡è¡¡å™¨
import time
import threading
from collections import defaultdict, deque
import statistics

class AdaptiveLoadBalancer:
    def __init__(self, servers):
        self.servers = servers
        self.server_stats = defaultdict(lambda: {
            'request_count': 0,
            'response_times': deque(maxlen=100),
            'error_count': 0,
            'health_score': 1.0
        })
        self.algorithm = 'adaptive'
        self.lock = threading.Lock()
        
        # å¯åŠ¨ç›‘æ§çº¿ç¨‹
        self.monitor_thread = threading.Thread(target=self._monitor_servers)
        self.monitor_thread.daemon = True
        self.monitor_thread.start()
    
    def _monitor_servers(self):
        """ç›‘æ§æœåŠ¡å™¨å¥åº·çŠ¶æ€"""
        while True:
            for server in self.servers:
                self._update_health_score(server)
            time.sleep(30)  # æ¯30ç§’æ›´æ–°ä¸€æ¬¡
    
    def _update_health_score(self, server):
        """æ›´æ–°æœåŠ¡å™¨å¥åº·è¯„åˆ†"""
        stats = self.server_stats[server]
        
        # åŸºäºå¤šä¸ªå› ç´ è®¡ç®—å¥åº·è¯„åˆ†
        response_time_score = self._calculate_response_time_score(stats['response_times'])
        error_rate_score = self._calculate_error_rate_score(stats['error_count'], stats['request_count'])
        load_balance_score = self._calculate_load_balance_score(server)
        
        # ç»¼åˆè¯„åˆ† (æƒé‡å¯ä»¥æ ¹æ®ä¸šåŠ¡è°ƒæ•´)
        health_score = (
            response_time_score * 0.5 +
            error_rate_score * 0.3 +
            load_balance_score * 0.2
        )
        
        with self.lock:
            stats['health_score'] = max(0.1, min(1.0, health_score))
    
    def _calculate_response_time_score(self, response_times):
        """åŸºäºå“åº”æ—¶é—´è®¡ç®—è¯„åˆ†"""
        if not response_times:
            return 1.0
        
        avg_response_time = statistics.mean(response_times)
        if avg_response_time < 50:  # 50msä»¥ä¸‹
            return 1.0
        elif avg_response_time < 200:  # 50-200ms
            return 0.8
        elif avg_response_time < 500:  # 200-500ms
            return 0.5
        else:  # 500msä»¥ä¸Š
            return 0.2
    
    def _calculate_error_rate_score(self, error_count, request_count):
        """åŸºäºé”™è¯¯ç‡è®¡ç®—è¯„åˆ†"""
        if request_count == 0:
            return 1.0
        
        error_rate = error_count / request_count
        if error_rate < 0.01:  # é”™è¯¯ç‡å°äº1%
            return 1.0
        elif error_rate < 0.05:  # é”™è¯¯ç‡1-5%
            return 0.7
        elif error_rate < 0.1:  # é”™è¯¯ç‡5-10%
            return 0.4
        else:  # é”™è¯¯ç‡è¶…è¿‡10%
            return 0.1
    
    def _calculate_load_balance_score(self, server):
        """åŸºäºè´Ÿè½½å‡è¡¡è®¡ç®—è¯„åˆ†"""
        current_load = self.server_stats[server]['request_count']
        avg_load = sum(stats['request_count'] for stats in self.server_stats.values()) / len(self.servers)
        
        if avg_load == 0:
            return 1.0
        
        load_ratio = current_load / avg_load
        if load_ratio < 0.8:  # è´Ÿè½½è¾ƒä½
            return 1.0
        elif load_ratio < 1.2:  # è´Ÿè½½é€‚ä¸­
            return 0.8
        else:  # è´Ÿè½½è¾ƒé«˜
            return 0.5
    
    def select_server(self, request_key=None):
        """é€‰æ‹©æœ€ä¼˜æœåŠ¡å™¨"""
        with self.lock:
            # æ ¹æ®å¥åº·è¯„åˆ†å’Œæƒé‡é€‰æ‹©æœåŠ¡å™¨
            total_score = sum(stats['health_score'] for stats in self.server_stats.values())
            
            if total_score == 0:
                # å¦‚æœæ‰€æœ‰æœåŠ¡å™¨éƒ½ä¸å¥åº·ï¼Œéšæœºé€‰æ‹©
                return random.choice(self.servers)
            
            # è½®ç›˜èµŒé€‰æ‹©ç®—æ³•
            rand_score = random.uniform(0, total_score)
            current_score = 0
            
            for server in self.servers:
                current_score += self.server_stats[server]['health_score']
                if current_score >= rand_score:
                    self.server_stats[server]['request_count'] += 1
                    return server
            
            # å¤‡é€‰æ–¹æ¡ˆ
            return self.servers[0]
    
    def record_request_result(self, server, response_time, success=True):
        """è®°å½•è¯·æ±‚ç»“æœ"""
        with self.lock:
            stats = self.server_stats[server]
            stats['response_times'].append(response_time)
            
            if not success:
                stats['error_count'] += 1

# ä½¿ç”¨ç¤ºä¾‹
servers = ['db1.example.com', 'db2.example.com', 'db3.example.com']
balancer = AdaptiveLoadBalancer(servers)

# æ¨¡æ‹Ÿè¯·æ±‚å¤„ç†
def handle_request(request_data):
    server = balancer.select_server()
    start_time = time.time()
    
    try:
        # æ‰§è¡Œæ•°æ®åº“æ“ä½œ
        result = execute_db_operation(server, request_data)
        response_time = (time.time() - start_time) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
        balancer.record_request_result(server, response_time, success=True)
        return result
    except Exception as e:
        response_time = (time.time() - start_time) * 1000
        balancer.record_request_result(server, response_time, success=False)
        raise e
```

### 6.2 ä¼šè¯ä¿æŒå’Œç²˜æ€§è¿æ¥

#### ä¼šè¯ä¿æŒç­–ç•¥
```python
# session_sticky_balancer.py - ä¼šè¯ä¿æŒè´Ÿè½½å‡è¡¡å™¨
import hashlib
import time
from collections import defaultdict

class SessionStickyLoadBalancer:
    def __init__(self, servers, session_timeout=300):
        self.servers = servers
        self.session_timeout = session_timeout
        self.session_map = {}  # session_id -> (server, expire_time)
        self.server_sessions = defaultdict(set)  # server -> set of session_ids
        
    def get_server_for_session(self, session_id, user_id=None):
        """ä¸ºä¼šè¯è·å–å›ºå®šæœåŠ¡å™¨"""
        current_time = time.time()
        
        # æ£€æŸ¥ç°æœ‰ä¼šè¯æ˜ å°„
        if session_id in self.session_map:
            server, expire_time = self.session_map[session_id]
            if current_time < expire_time:
                # æ›´æ–°è¿‡æœŸæ—¶é—´
                self.session_map[session_id] = (server, current_time + self.session_timeout)
                return server
            else:
                # ä¼šè¯å·²è¿‡æœŸï¼Œæ¸…ç†æ˜ å°„
                self._cleanup_expired_session(session_id)
        
        # åˆ›å»ºæ–°ä¼šè¯æ˜ å°„
        if user_id:
            # åŸºäºç”¨æˆ·IDçš„ä¸€è‡´æ€§å“ˆå¸Œ
            server = self._consistent_hash(user_id)
        else:
            # åŸºäºä¼šè¯IDçš„å“ˆå¸Œ
            server = self._hash_to_server(session_id)
        
        # å»ºç«‹ä¼šè¯æ˜ å°„
        self.session_map[session_id] = (server, current_time + self.session_timeout)
        self.server_sessions[server].add(session_id)
        
        return server
    
    def _consistent_hash(self, key):
        """ä¸€è‡´æ€§å“ˆå¸Œç®—æ³•"""
        hash_value = int(hashlib.md5(str(key).encode()).hexdigest(), 16)
        server_index = hash_value % len(self.servers)
        return self.servers[server_index]
    
    def _hash_to_server(self, session_id):
        """ç®€å•çš„å“ˆå¸Œåˆ°æœåŠ¡å™¨æ˜ å°„"""
        hash_value = hash(session_id) % len(self.servers)
        return self.servers[hash_value]
    
    def _cleanup_expired_session(self, session_id):
        """æ¸…ç†è¿‡æœŸä¼šè¯"""
        if session_id in self.session_map:
            old_server, _ = self.session_map[session_id]
            del self.session_map[session_id]
            self.server_sessions[old_server].discard(session_id)
    
    def get_session_stats(self):
        """è·å–ä¼šè¯ç»Ÿè®¡ä¿¡æ¯"""
        current_time = time.time()
        active_sessions = sum(1 for _, expire_time in self.session_map.values() 
                            if current_time < expire_time)
        
        server_load = {}
        for server in self.servers:
            active_server_sessions = sum(1 for session_id in self.server_sessions[server]
                                       if session_id in self.session_map and 
                                       current_time < self.session_map[session_id][1])
            server_load[server] = active_server_sessions
        
        return {
            'total_sessions': len(self.session_map),
            'active_sessions': active_sessions,
            'server_load_distribution': server_load
        }
    
    def rebalance_sessions(self, threshold_ratio=1.5):
        """é‡æ–°å¹³è¡¡ä¼šè¯åˆ†å¸ƒ"""
        stats = self.get_session_stats()
        server_load = stats['server_load_distribution']
        
        if not server_load:
            return
        
        avg_load = sum(server_load.values()) / len(server_load)
        overloaded_servers = [s for s, load in server_load.items() if load > avg_load * threshold_ratio]
        underloaded_servers = [s for s, load in server_load.items() if load < avg_load / threshold_ratio]
        
        if not overloaded_servers or not underloaded_servers:
            return
        
        # è¿ç§»ä¼šè¯
        for overloaded_server in overloaded_servers:
            sessions_to_move = self.server_sessions[overloaded_server].copy()
            target_server = underloaded_servers[0]  # ç®€å•é€‰æ‹©ç¬¬ä¸€ä¸ªæ¬ è½½æœåŠ¡å™¨
            
            moved_count = 0
            for session_id in sessions_to_move:
                if moved_count >= 10:  # æ¯æ¬¡æœ€å¤šè¿ç§»10ä¸ªä¼šè¯
                    break
                    
                if (session_id in self.session_map and 
                    self.session_map[session_id][0] == overloaded_server):
                    
                    # æ›´æ–°ä¼šè¯æ˜ å°„
                    expire_time = self.session_map[session_id][1]
                    self.session_map[session_id] = (target_server, expire_time)
                    
                    # æ›´æ–°æœåŠ¡å™¨ä¼šè¯é›†åˆ
                    self.server_sessions[overloaded_server].discard(session_id)
                    self.server_sessions[target_server].add(session_id)
                    
                    moved_count += 1

# ä½¿ç”¨ç¤ºä¾‹
balancer = SessionStickyLoadBalancer(['db1', 'db2', 'db3'])

# ç”¨æˆ·ç™»å½•æ—¶å»ºç«‹ä¼šè¯
user_session_id = create_user_session(user_id=12345)
server = balancer.get_server_for_session(user_session_id, user_id=12345)

# åç»­è¯·æ±‚ä½¿ç”¨ç›¸åŒæœåŠ¡å™¨
same_server = balancer.get_server_for_session(user_session_id)
assert server == same_server  # ç¡®ä¿ä¼šè¯ç²˜æ€§
```

## 7. ç›‘æ§ä¸æ•…éšœå¤„ç†

### 7.1 è´Ÿè½½å‡è¡¡ç›‘æ§ä½“ç³»

#### ç»¼åˆç›‘æ§ä»ªè¡¨æ¿
```python
# load_balancer_monitor.py - è´Ÿè½½å‡è¡¡ç›‘æ§ç³»ç»Ÿ
import time
import json
from datetime import datetime
import threading

class LoadBalancerMonitor:
    def __init__(self, balancer):
        self.balancer = balancer
        self.metrics_history = []
        self.alerts = []
        self.monitoring = True
        
        # å¯åŠ¨ç›‘æ§çº¿ç¨‹
        self.monitor_thread = threading.Thread(target=self._monitor_loop)
        self.monitor_thread.daemon = True
        self.monitor_thread.start()
    
    def _monitor_loop(self):
        """ç›‘æ§å¾ªç¯"""
        while self.monitoring:
            try:
                metrics = self._collect_metrics()
                self._analyze_metrics(metrics)
                self._store_metrics(metrics)
                time.sleep(60)  # æ¯åˆ†é’Ÿæ”¶é›†ä¸€æ¬¡
            except Exception as e:
                self._log_error(f"ç›‘æ§å¾ªç¯å¼‚å¸¸: {str(e)}")
    
    def _collect_metrics(self):
        """æ”¶é›†ç›‘æ§æŒ‡æ ‡"""
        timestamp = datetime.now().isoformat()
        
        # åŸºç¡€æŒ‡æ ‡
        base_metrics = {
            'timestamp': timestamp,
            'total_requests': getattr(self.balancer, 'total_requests', 0),
            'active_connections': getattr(self.balancer, 'active_connections', 0),
            'server_count': len(getattr(self.balancer, 'servers', []))
        }
        
        # æœåŠ¡å™¨æŒ‡æ ‡
        server_metrics = {}
        for server in getattr(self.balancer, 'servers', []):
            server_stats = getattr(self.balancer, 'server_stats', {}).get(server, {})
            server_metrics[server] = {
                'request_count': server_stats.get('request_count', 0),
                'health_score': server_stats.get('health_score', 1.0),
                'avg_response_time': self._calculate_avg_response_time(server_stats.get('response_times', [])),
                'error_count': server_stats.get('error_count', 0)
            }
        
        # æ€§èƒ½æŒ‡æ ‡
        performance_metrics = {
            'overall_avg_response_time': self._calculate_overall_avg_response_time(),
            'request_distribution_std': self._calculate_request_distribution_std(),
            'load_balance_score': self._calculate_load_balance_score(server_metrics)
        }
        
        return {
            'base_metrics': base_metrics,
            'server_metrics': server_metrics,
            'performance_metrics': performance_metrics
        }
    
    def _analyze_metrics(self, metrics):
        """åˆ†ææŒ‡æ ‡å¹¶ç”Ÿæˆå‘Šè­¦"""
        server_metrics = metrics['server_metrics']
        performance_metrics = metrics['performance_metrics']
        
        # æ£€æŸ¥æœåŠ¡å™¨å¥åº·çŠ¶æ€
        for server, stats in server_metrics.items():
            if stats['health_score'] < 0.3:
                self._generate_alert('CRITICAL', f'æœåŠ¡å™¨ {server} å¥åº·è¯„åˆ†è¿‡ä½: {stats["health_score"]}')
            
            if stats['avg_response_time'] > 1000:  # 1ç§’
                self._generate_alert('WARNING', f'æœåŠ¡å™¨ {server} å“åº”æ—¶é—´è¿‡é•¿: {stats["avg_response_time"]}ms')
        
        # æ£€æŸ¥è´Ÿè½½å‡è¡¡çŠ¶æ€
        if performance_metrics['load_balance_score'] < 0.7:
            self._generate_alert('WARNING', f'è´Ÿè½½å‡è¡¡ä¸ä½³ï¼Œè¯„åˆ†ä¸º: {performance_metrics["load_balance_score"]}')
        
        # æ£€æŸ¥æ•´ä½“æ€§èƒ½
        if performance_metrics['overall_avg_response_time'] > 500:  # 500ms
            self._generate_alert('WARNING', f'æ•´ä½“å“åº”æ—¶é—´è¿‡é•¿: {performance_metrics["overall_avg_response_time"]}ms')
    
    def _generate_alert(self, level, message):
        """ç”Ÿæˆå‘Šè­¦"""
        alert = {
            'timestamp': datetime.now().isoformat(),
            'level': level,
            'message': message
        }
        self.alerts.append(alert)
        print(f"[{level}] {message}")  # å®é™…åº”ç”¨ä¸­åº”è¯¥å‘é€åˆ°å‘Šè­¦ç³»ç»Ÿ
    
    def _calculate_avg_response_time(self, response_times):
        """è®¡ç®—å¹³å‡å“åº”æ—¶é—´"""
        if not response_times:
            return 0
        return sum(response_times) / len(response_times)
    
    def _calculate_overall_avg_response_time(self):
        """è®¡ç®—æ•´ä½“å¹³å‡å“åº”æ—¶é—´"""
        all_response_times = []
        for server_stats in getattr(self.balancer, 'server_stats', {}).values():
            all_response_times.extend(server_stats.get('response_times', []))
        return self._calculate_avg_response_time(all_response_times)
    
    def _calculate_request_distribution_std(self):
        """è®¡ç®—è¯·æ±‚åˆ†å¸ƒæ ‡å‡†å·®"""
        request_counts = [stats.get('request_count', 0) 
                         for stats in getattr(self.balancer, 'server_stats', {}).values()]
        if len(request_counts) < 2:
            return 0
        
        mean = sum(request_counts) / len(request_counts)
        variance = sum((x - mean) ** 2 for x in request_counts) / len(request_counts)
        return variance ** 0.5
    
    def _calculate_load_balance_score(self, server_metrics):
        """è®¡ç®—è´Ÿè½½å‡è¡¡è¯„åˆ†"""
        if not server_metrics:
            return 1.0
        
        request_counts = [stats['request_count'] for stats in server_metrics.values()]
        if sum(request_counts) == 0:
            return 1.0
        
        # ä½¿ç”¨å˜å¼‚ç³»æ•°ä½œä¸ºå¹³è¡¡åº¦é‡
        mean = sum(request_counts) / len(request_counts)
        std_dev = (sum((x - mean) ** 2 for x in request_counts) / len(request_counts)) ** 0.5
        
        if mean == 0:
            return 0
        
        cv = std_dev / mean  # å˜å¼‚ç³»æ•°
        # å˜å¼‚ç³»æ•°è¶Šå°ï¼Œè´Ÿè½½è¶Šå‡è¡¡
        return max(0, 1 - min(1, cv))
    
    def _store_metrics(self, metrics):
        """å­˜å‚¨æŒ‡æ ‡æ•°æ®"""
        self.metrics_history.append(metrics)
        # ä¿ç•™æœ€è¿‘1000æ¡è®°å½•
        if len(self.metrics_history) > 1000:
            self.metrics_history.pop(0)
    
    def get_monitoring_report(self):
        """è·å–ç›‘æ§æŠ¥å‘Š"""
        if not self.metrics_history:
            return {"status": "no_data"}
        
        latest_metrics = self.metrics_history[-1]
        historical_metrics = self.metrics_history[-60:]  # æœ€è¿‘ä¸€å°æ—¶
        
        report = {
            'current_status': latest_metrics,
            'trends': self._analyze_trends(historical_metrics),
            'alerts': self.alerts[-10:],  # æœ€è¿‘10æ¡å‘Šè­¦
            'recommendations': self._generate_recommendations(latest_metrics)
        }
        
        return report
    
    def _analyze_trends(self, historical_metrics):
        """åˆ†æè¶‹åŠ¿"""
        if len(historical_metrics) < 2:
            return {}
        
        # ç®€å•çš„è¶‹åŠ¿åˆ†æ
        first_metrics = historical_metrics[0]
        last_metrics = historical_metrics[-1]
        
        trends = {}
        if 'performance_metrics' in first_metrics and 'performance_metrics' in last_metrics:
            first_perf = first_metrics['performance_metrics']
            last_perf = last_metrics['performance_metrics']
            
            trends['response_time_trend'] = (
                last_perf['overall_avg_response_time'] - first_perf['overall_avg_response_time']
            )
            trends['load_balance_trend'] = (
                last_perf['load_balance_score'] - first_perf['load_balance_score']
            )
        
        return trends
    
    def _generate_recommendations(self, metrics):
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []
        
        server_metrics = metrics.get('server_metrics', {})
        performance_metrics = metrics.get('performance_metrics', {})
        
        # è´Ÿè½½å‡è¡¡å»ºè®®
        if performance_metrics.get('load_balance_score', 1.0) < 0.8:
            recommendations.append("å»ºè®®è°ƒæ•´è´Ÿè½½å‡è¡¡ç®—æ³•æˆ–æƒé‡é…ç½®")
        
        # æœåŠ¡å™¨æ€§èƒ½å»ºè®®
        for server, stats in server_metrics.items():
            if stats.get('health_score', 1.0) < 0.5:
                recommendations.append(f"å»ºè®®æ£€æŸ¥æœåŠ¡å™¨ {server} çš„å¥åº·çŠ¶æ€")
            if stats.get('avg_response_time', 0) > 500:
                recommendations.append(f"å»ºè®®ä¼˜åŒ–æœåŠ¡å™¨ {server} çš„æ€§èƒ½")
        
        return recommendations

# ä½¿ç”¨ç¤ºä¾‹
# monitor = LoadBalancerMonitor(your_load_balancer_instance)
# report = monitor.get_monitoring_report()
# print(json.dumps(report, indent=2, ensure_ascii=False))
```

### 7.2 è‡ªåŠ¨æ•…éšœå¤„ç†

#### æ™ºèƒ½æ•…éšœæ¢å¤ç³»ç»Ÿ
```python
# auto_failover_system.py - è‡ªåŠ¨æ•…éšœå¤„ç†ç³»ç»Ÿ
import time
import threading
from enum import Enum

class ServerStatus(Enum):
    HEALTHY = "healthy"
    DEGRADED = "degraded"
    FAILED = "failed"
    MAINTENANCE = "maintenance"

class AutoFailoverSystem:
    def __init__(self, servers, health_checker):
        self.servers = {server: ServerStatus.HEALTHY for server in servers}
        self.health_checker = health_checker
        self.failover_history = []
        self.recovery_attempts = {}
        self.max_recovery_attempts = 3
        
        # å¯åŠ¨æ•…éšœæ£€æµ‹çº¿ç¨‹
        self.monitor_thread = threading.Thread(target=self._failover_monitor)
        self.monitor_thread.daemon = True
        self.monitor_thread.start()
    
    def _failover_monitor(self):
        """æ•…éšœç›‘æ§å¾ªç¯"""
        while True:
            try:
                self._check_server_health()
                self._handle_failures()
                self._attempt_recoveries()
                time.sleep(30)  # æ¯30ç§’æ£€æŸ¥ä¸€æ¬¡
            except Exception as e:
                print(f"æ•…éšœç›‘æ§å¼‚å¸¸: {str(e)}")
                time.sleep(60)
    
    def _check_server_health(self):
        """æ£€æŸ¥æœåŠ¡å™¨å¥åº·çŠ¶æ€"""
        for server in self.servers:
            if self.servers[server] == ServerStatus.MAINTENANCE:
                continue
                
            health_status = self.health_checker.check_health(server)
            
            if health_status.healthy:
                if self.servers[server] != ServerStatus.HEALTHY:
                    self._handle_server_recovery(server)
            else:
                self._handle_server_failure(server, health_status)
    
    def _handle_server_failure(self, server, health_status):
        """å¤„ç†æœåŠ¡å™¨æ•…éšœ"""
        current_status = self.servers[server]
        
        if current_status == ServerStatus.HEALTHY:
            print(f"æ£€æµ‹åˆ°æœåŠ¡å™¨ {server} å‡ºç°é—®é¢˜: {health_status.message}")
            self.servers[server] = ServerStatus.DEGRADED
            self.recovery_attempts[server] = 0
            
        elif current_status == ServerStatus.DEGRADED:
            self.recovery_attempts[server] += 1
            
            if self.recovery_attempts[server] >= self.max_recovery_attempts:
                print(f"æœåŠ¡å™¨ {server} æ•…éšœç¡®è®¤ï¼Œæ ‡è®°ä¸ºå¤±è´¥")
                self.servers[server] = ServerStatus.FAILED
                self._trigger_failover(server)
    
    def _handle_server_recovery(self, server):
        """å¤„ç†æœåŠ¡å™¨æ¢å¤"""
        if self.servers[server] in [ServerStatus.DEGRADED, ServerStatus.FAILED]:
            print(f"æœåŠ¡å™¨ {server} æ¢å¤å¥åº·")
            self.servers[server] = ServerStatus.HEALTHY
            self.recovery_attempts[server] = 0
            
            # å¦‚æœæ˜¯ä»æ•…éšœæ¢å¤ï¼Œå¯èƒ½éœ€è¦é‡æ–°åŠ å…¥è´Ÿè½½å‡è¡¡æ± 
            if self.servers[server] == ServerStatus.FAILED:
                self._reintegrate_server(server)
    
    def _trigger_failover(self, failed_server):
        """è§¦å‘æ•…éšœè½¬ç§»"""
        failover_event = {
            'timestamp': time.time(),
            'failed_server': failed_server,
            'replacement_server': self._select_replacement_server(failed_server),
            'reason': 'automatic_failover'
        }
        
        self.failover_history.append(failover_event)
        print(f"æ‰§è¡Œæ•…éšœè½¬ç§»: {failed_server} -> {failover_event['replacement_server']}")
        
        # å®é™…çš„æ•…éšœè½¬ç§»é€»è¾‘åº”è¯¥åœ¨è¿™é‡Œå®ç°
        # ä¾‹å¦‚æ›´æ–°DNSè®°å½•ã€ä¿®æ”¹è´Ÿè½½å‡è¡¡é…ç½®ç­‰
    
    def _select_replacement_server(self, failed_server):
        """é€‰æ‹©æ›¿æ¢æœåŠ¡å™¨"""
        healthy_servers = [server for server, status in self.servers.items() 
                          if status == ServerStatus.HEALTHY and server != failed_server]
        
        if not healthy_servers:
            # å¦‚æœæ²¡æœ‰å¥åº·æœåŠ¡å™¨ï¼Œé€‰æ‹©é™çº§æœåŠ¡å™¨
            degraded_servers = [server for server, status in self.servers.items() 
                              if status == ServerStatus.DEGRADED and server != failed_server]
            return degraded_servers[0] if degraded_servers else None
        
        # ç®€å•çš„é€‰æ‹©ç­–ç•¥ï¼šé€‰æ‹©è¯·æ±‚æœ€å°‘çš„å¥åº·æœåŠ¡å™¨
        return min(healthy_servers, key=lambda s: self._get_server_load(s))
    
    def _get_server_load(self, server):
        """è·å–æœåŠ¡å™¨è´Ÿè½½"""
        # è¿™é‡Œåº”è¯¥ä»è´Ÿè½½å‡è¡¡å™¨è·å–å®é™…è´Ÿè½½ä¿¡æ¯
        # ç®€åŒ–å®ç°è¿”å›ä¸€ä¸ªé»˜è®¤å€¼
        return getattr(self, 'server_loads', {}).get(server, 0)
    
    def _attempt_recoveries(self):
        """å°è¯•æ¢å¤æ•…éšœæœåŠ¡å™¨"""
        for server, status in self.servers.items():
            if status == ServerStatus.FAILED and self.recovery_attempts[server] < self.max_recovery_attempts:
                if self._attempt_server_recovery(server):
                    print(f"æœåŠ¡å™¨ {server} æ¢å¤æˆåŠŸ")
                    self.servers[server] = ServerStatus.HEALTHY
                    self.recovery_attempts[server] = 0
    
    def _attempt_server_recovery(self, server):
        """å°è¯•æ¢å¤å•ä¸ªæœåŠ¡å™¨"""
        try:
            # æ‰§è¡Œæ¢å¤æ“ä½œ
            # ä¾‹å¦‚ï¼šé‡å¯æœåŠ¡ã€ä¿®å¤é…ç½®ç­‰
            recovery_result = self.health_checker.attempt_recovery(server)
            return recovery_result.success
        except Exception as e:
            print(f"æœåŠ¡å™¨ {server} æ¢å¤å¤±è´¥: {str(e)}")
            return False
    
    def _reintegrate_server(self, server):
        """é‡æ–°é›†æˆæœåŠ¡å™¨åˆ°æœåŠ¡æ± """
        print(f"é‡æ–°é›†æˆæœåŠ¡å™¨ {server}")
        # è¿™é‡Œåº”è¯¥å®ç°å°†æœåŠ¡å™¨é‡æ–°åŠ å…¥è´Ÿè½½å‡è¡¡æ± çš„é€»è¾‘
        # ä¾‹å¦‚ï¼šæ›´æ–°é…ç½®ã€é‡æ–°æ³¨å†ŒæœåŠ¡ç­‰
    
    def get_system_status(self):
        """è·å–ç³»ç»ŸçŠ¶æ€"""
        return {
            'server_status': dict(self.servers),
            'recent_failovers': self.failover_history[-10:],  # æœ€è¿‘10æ¬¡æ•…éšœè½¬ç§»
            'recovery_statistics': self._get_recovery_statistics()
        }
    
    def _get_recovery_statistics(self):
        """è·å–æ¢å¤ç»Ÿè®¡ä¿¡æ¯"""
        total_failures = len([s for s in self.servers.values() if s == ServerStatus.FAILED])
        successful_recoveries = len([event for event in self.failover_history 
                                   if event.get('recovered', False)])
        
        return {
            'total_failures': total_failures,
            'successful_recoveries': successful_recoveries,
            'recovery_rate': successful_recoveries / max(1, len(self.failover_history))
        }
    
    def manual_failover(self, source_server, target_server):
        """æ‰‹åŠ¨æ‰§è¡Œæ•…éšœè½¬ç§»"""
        if self.servers[source_server] == ServerStatus.HEALTHY:
            print(f"è­¦å‘Š: æºæœåŠ¡å™¨ {source_server} å½“å‰çŠ¶æ€å¥åº·")
        
        if self.servers[target_server] != ServerStatus.HEALTHY:
            print(f"è­¦å‘Š: ç›®æ ‡æœåŠ¡å™¨ {target_server} å½“å‰çŠ¶æ€ä¸å¥åº·")
        
        self._trigger_failover(source_server)
        print(f"æ‰‹åŠ¨æ•…éšœè½¬ç§»å®Œæˆ: {source_server} -> {target_server}")

# ä½¿ç”¨ç¤ºä¾‹
# failover_system = AutoFailoverSystem(servers_list, health_checker_instance)
# status = failover_system.get_system_status()
```

---

## ğŸ” å…³é”®è¦ç‚¹æ€»ç»“

### âœ… è´Ÿè½½å‡è¡¡æˆåŠŸè¦ç´ 
- **åˆç†çš„ç®—æ³•é€‰æ‹©**ï¼šæ ¹æ®ä¸šåŠ¡ç‰¹ç‚¹é€‰æ‹©æœ€é€‚åˆçš„è´Ÿè½½å‡è¡¡ç®—æ³•
- **å®Œå–„çš„ç›‘æ§ä½“ç³»**ï¼šå®æ—¶ç›‘æ§å„èŠ‚ç‚¹çŠ¶æ€å’Œæ€§èƒ½æŒ‡æ ‡
- **æ™ºèƒ½çš„æ•…éšœå¤„ç†**ï¼šè‡ªåŠ¨æ£€æµ‹æ•…éšœå¹¶æ‰§è¡Œç›¸åº”çš„æ¢å¤æªæ–½
- **æŒç»­çš„æ€§èƒ½ä¼˜åŒ–**ï¼šåŸºäºç›‘æ§æ•°æ®åˆ†æä¸æ–­ä¼˜åŒ–è´Ÿè½½å‡è¡¡ç­–ç•¥

### âš ï¸ å¸¸è§é£é™©æé†’
- **å•ç‚¹æ•…éšœé£é™©**ï¼šè´Ÿè½½å‡è¡¡å™¨æœ¬èº«å¯èƒ½æˆä¸ºæ–°çš„å•ç‚¹æ•…éšœ
- **é…ç½®å¤æ‚æ€§**ï¼šå¤æ‚çš„è´Ÿè½½å‡è¡¡é…ç½®å¯èƒ½å¼•å…¥æ–°çš„é—®é¢˜
- **æ€§èƒ½ç“¶é¢ˆ**ï¼šä¸å½“çš„è´Ÿè½½å‡è¡¡å¯èƒ½å¯¼è‡´æŸäº›èŠ‚ç‚¹è¿‡è½½
- **ä¼šè¯ä¸€è‡´æ€§**ï¼šéœ€è¦ç‰¹åˆ«æ³¨æ„ä¼šè¯ä¿æŒå’Œæ•°æ®ä¸€è‡´æ€§é—®é¢˜

### ğŸ¯ æœ€ä½³å®è·µå»ºè®®
1. **æ¸è¿›å¼éƒ¨ç½²**ï¼šä»ç®€å•é…ç½®å¼€å§‹ï¼Œé€æ­¥å¢åŠ å¤æ‚åŠŸèƒ½
2. **å……åˆ†æµ‹è¯•**ï¼šåœ¨ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²å‰è¿›è¡Œå……åˆ†çš„å‹åŠ›æµ‹è¯•
3. **ç›‘æ§å‘Šè­¦**ï¼šå»ºç«‹å®Œå–„çš„ç›‘æ§å’Œå‘Šè­¦æœºåˆ¶
4. **æ–‡æ¡£åŒ–é…ç½®**ï¼šè¯¦ç»†è®°å½•æ‰€æœ‰é…ç½®å‚æ•°å’Œå˜æ›´å†å²
5. **å®šæœŸè¯„å®¡**ï¼šå®šæœŸè¯„å®¡è´Ÿè½½å‡è¡¡ç­–ç•¥çš„æœ‰æ•ˆæ€§

é€šè¿‡ç§‘å­¦çš„è´Ÿè½½å‡è¡¡è®¾è®¡å’Œå®æ–½ï¼Œå¯ä»¥æ˜¾è‘—æå‡æ•°æ®åº“ç³»ç»Ÿçš„æ€§èƒ½ã€å¯ç”¨æ€§å’Œå¯æ‰©å±•æ€§ï¼Œä¸ºä¼ä¸šä¸šåŠ¡å‘å±•æä¾›å¼ºæœ‰åŠ›çš„æŠ€æœ¯æ”¯æ’‘ã€‚