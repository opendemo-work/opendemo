# æ•°æ®åº“æ•…éšœè‡ªåŠ¨åˆ‡æ¢å®Œæ•´æŒ‡å—

## ğŸ¯ æ¦‚è¿°

æ•°æ®åº“æ•…éšœè‡ªåŠ¨åˆ‡æ¢æ˜¯ä¿éšœä¸šåŠ¡è¿ç»­æ€§çš„æ ¸å¿ƒæŠ€æœ¯ï¼Œèƒ½å¤Ÿåœ¨ä¸»èŠ‚ç‚¹å‘ç”Ÿæ•…éšœæ—¶è‡ªåŠ¨å°†æœåŠ¡åˆ‡æ¢åˆ°å¤‡ç”¨èŠ‚ç‚¹ï¼Œæœ€å¤§ç¨‹åº¦å‡å°‘ä¸šåŠ¡ä¸­æ–­æ—¶é—´ã€‚æœ¬æŒ‡å—æä¾›ä»ç†è®ºåŸºç¡€åˆ°å®è·µéƒ¨ç½²çš„å®Œæ•´æ•…éšœè‡ªåŠ¨åˆ‡æ¢è§£å†³æ–¹æ¡ˆã€‚

## ğŸ“‹ ç›®å½•

1. [æ•…éšœåˆ‡æ¢åŸºç¡€ç†è®º](#1-æ•…éšœåˆ‡æ¢åŸºç¡€ç†è®º)
2. [MySQLè‡ªåŠ¨æ•…éšœè½¬ç§»](#2-mysqlè‡ªåŠ¨æ•…éšœè½¬ç§»)
3. [PostgreSQLæ•…éšœåˆ‡æ¢](#3-postgresqlæ•…éšœåˆ‡æ¢)
4. [MongoDBè‡ªåŠ¨åˆ‡æ¢](#4-mongodbè‡ªåŠ¨åˆ‡æ¢)
5. [åˆ‡æ¢ç­–ç•¥ä¼˜åŒ–](#5-åˆ‡æ¢ç­–ç•¥ä¼˜åŒ–)
6. [ç›‘æ§ä¸å‘Šè­¦ä½“ç³»](#6-ç›‘æ§ä¸å‘Šè­¦ä½“ç³»)

---

## 1. æ•…éšœåˆ‡æ¢åŸºç¡€ç†è®º

### 1.1 æ•…éšœåˆ‡æ¢æ ¸å¿ƒæ¦‚å¿µ

#### æ•…éšœåˆ‡æ¢æ¶æ„æ¨¡å¼
```mermaid
graph TD
    A[ä¸»èŠ‚ç‚¹] --> B[ç›‘æ§ç³»ç»Ÿ]
    C[ä»èŠ‚ç‚¹1] --> B
    D[ä»èŠ‚ç‚¹2] --> B
    E[ä»èŠ‚ç‚¹3] --> B
    
    B --> F[æ•…éšœæ£€æµ‹]
    F --> G{æ•…éšœç¡®è®¤}
    G -->|ç¡®è®¤æ•…éšœ| H[é€‰ä¸¾æ–°ä¸»]
    G -->|è¯¯æŠ¥| I[ç»´æŒç°çŠ¶]
    
    H --> J[æ•°æ®åŒæ­¥æ£€æŸ¥]
    J --> K[è§’è‰²åˆ‡æ¢]
    K --> L[é…ç½®æ›´æ–°]
    L --> M[æœåŠ¡æ¢å¤]
    
    subgraph "æ•…éšœæ£€æµ‹å±‚"
        B --> N[å¿ƒè·³æ£€æµ‹]
        B --> O[è¿æ¥æ£€æµ‹]
        B --> P[æŸ¥è¯¢æ£€æµ‹]
    end
    
    subgraph "åˆ‡æ¢æ‰§è¡Œå±‚"
        K --> Q[VIPåˆ‡æ¢]
        K --> R[DNSæ›´æ–°]
        K --> S[åº”ç”¨é‡è¿]
    end
    
    T[å‘Šè­¦é€šçŸ¥] --> B
    T --> H
    T --> M
```

#### æ•…éšœåˆ‡æ¢å…³é”®æŒ‡æ ‡
```yaml
failover_metrics:
  mttd:
    description: "å¹³å‡æ£€æµ‹æ—¶é—´ (Mean Time To Detect)"
    target: "< 30ç§’"
    measurement: "ä»æ•…éšœå‘ç”Ÿåˆ°è¢«æ£€æµ‹åˆ°çš„æ—¶é—´"
  
  mttr:
    description: "å¹³å‡æ¢å¤æ—¶é—´ (Mean Time To Recover)"
    target: "< 2åˆ†é’Ÿ"
    measurement: "ä»æ£€æµ‹åˆ°æ•…éšœåˆ°æœåŠ¡å®Œå…¨æ¢å¤çš„æ—¶é—´"
  
  availability:
    description: "ç³»ç»Ÿå¯ç”¨æ€§"
    target: "> 99.9%"
    measurement: "å¹´åº¦åœæœºæ—¶é—´ä¸è¶…è¿‡8.76å°æ—¶"
  
  data_loss:
    description: "æ•°æ®ä¸¢å¤±é‡"
    target: "0å­—èŠ‚"
    measurement: "RPO (Recovery Point Objective) = 0"
```

### 1.2 æ•…éšœæ£€æµ‹æœºåˆ¶

#### å¤šå±‚æ¬¡æ•…éšœæ£€æµ‹
```python
# å¤šå±‚æ¬¡æ•…éšœæ£€æµ‹ç³»ç»Ÿ
class MultiLayerFaultDetector:
    def __init__(self, nodes_config):
        self.nodes = nodes_config
        self.detection_layers = {
            'network_layer': NetworkDetector(),
            'application_layer': ApplicationDetector(),
            'data_layer': DataDetector(),
            'system_layer': SystemDetector()
        }
        self.fault_history = []
    
    def detect_faults(self):
        """å¤šå±‚æ¬¡æ•…éšœæ£€æµ‹"""
        fault_reports = {}
        
        # 1. ç½‘ç»œå±‚æ£€æµ‹
        network_faults = self.detection_layers['network_layer'].check_connectivity(self.nodes)
        fault_reports['network'] = network_faults
        
        # 2. åº”ç”¨å±‚æ£€æµ‹
        app_faults = self.detection_layers['application_layer'].check_application_health(self.nodes)
        fault_reports['application'] = app_faults
        
        # 3. æ•°æ®å±‚æ£€æµ‹
        data_faults = self.detection_layers['data_layer'].check_data_consistency(self.nodes)
        fault_reports['data'] = data_faults
        
        # 4. ç³»ç»Ÿå±‚æ£€æµ‹
        system_faults = self.detection_layers['system_layer'].check_system_resources(self.nodes)
        fault_reports['system'] = system_faults
        
        # ç»¼åˆæ•…éšœåˆ¤æ–­
        confirmed_faults = self._confirm_faults(fault_reports)
        return confirmed_faults
    
    def _confirm_faults(self, fault_reports):
        """æ•…éšœç¡®è®¤æœºåˆ¶"""
        confirmed = []
        
        for node in self.nodes:
            fault_evidence = []
            
            # æ”¶é›†å„å±‚æ•…éšœè¯æ®
            for layer, faults in fault_reports.items():
                if node['host'] in faults:
                    fault_evidence.append({
                        'layer': layer,
                        'severity': faults[node['host']]['severity'],
                        'timestamp': faults[node['host']]['timestamp']
                    })
            
            # å¤šè¯æ®ç¡®è®¤æœºåˆ¶
            if len(fault_evidence) >= 2:  # è‡³å°‘ä¸¤å±‚æ£€æµ‹åˆ°æ•…éšœ
                confirmation_score = self._calculate_confirmation_score(fault_evidence)
                if confirmation_score > 0.7:  # 70%ç½®ä¿¡åº¦é˜ˆå€¼
                    confirmed.append({
                        'node': node,
                        'fault_type': self._classify_fault(fault_evidence),
                        'confidence': confirmation_score,
                        'evidence': fault_evidence
                    })
        
        return confirmed

# å„å±‚æ£€æµ‹å™¨å®ç°
class NetworkDetector:
    def check_connectivity(self, nodes):
        """ç½‘ç»œè¿é€šæ€§æ£€æµ‹"""
        faults = {}
        
        for node in nodes:
            # ICMP pingæ£€æµ‹
            ping_result = self._ping_test(node['host'])
            if not ping_result['success']:
                faults[node['host']] = {
                    'type': 'network_unreachable',
                    'severity': 'critical',
                    'timestamp': time.time(),
                    'details': ping_result
                }
            
            # ç«¯å£è¿é€šæ€§æ£€æµ‹
            port_result = self._port_test(node['host'], node['port'])
            if not port_result['success']:
                faults[node['host']] = {
                    'type': 'port_unreachable',
                    'severity': 'warning',
                    'timestamp': time.time(),
                    'details': port_result
                }
        
        return faults

class ApplicationDetector:
    def check_application_health(self, nodes):
        """åº”ç”¨å±‚å¥åº·æ£€æµ‹"""
        faults = {}
        
        for node in nodes:
            # æ•°æ®åº“è¿æ¥æµ‹è¯•
            db_result = self._database_connectivity_test(node)
            if not db_result['success']:
                faults[node['host']] = {
                    'type': 'database_unreachable',
                    'severity': 'critical',
                    'timestamp': time.time(),
                    'details': db_result
                }
            
            # æŸ¥è¯¢å“åº”æµ‹è¯•
            query_result = self._query_response_test(node)
            if query_result['response_time'] > 5000:  # 5ç§’è¶…æ—¶
                faults[node['host']] = {
                    'type': 'slow_response',
                    'severity': 'warning',
                    'timestamp': time.time(),
                    'details': query_result
                }
        
        return faults
```

## 2. MySQLè‡ªåŠ¨æ•…éšœè½¬ç§»

### 2.1 MHA (Master High Availability) é…ç½®

#### MHA Manageré…ç½®
```ini
# /etc/masterha/app1.cnf - MHAé…ç½®æ–‡ä»¶
[server default]
# SSHé…ç½®
ssh_user=root
ssh_port=22
repl_user=repl
repl_password=repl_password

# MySQLé…ç½®
user=mha_user
password=mha_password
port=3306

# å·¥ä½œç›®å½•
manager_workdir=/var/log/masterha/app1
manager_log=/var/log/masterha/app1/manager.log

# æ•…éšœè½¬ç§»é…ç½®
remote_workdir=/var/lib/mysql
master_binlog_dir=/var/lib/mysql
master_ip_failover_script=/usr/local/bin/master_ip_failover
master_ip_online_change_script=/usr/local/bin/master_ip_online_change
report_script=/usr/local/bin/send_report

# æ£€æµ‹é…ç½®
ping_interval=3
ping_type=SELECT

[server1]
hostname=master_host
candidate_master=1

[server2]
hostname=slave1_host
candidate_master=1

[server3]
hostname=slave2_host
no_master=1
```

#### æ•…éšœè½¬ç§»è„šæœ¬
```bash
#!/bin/bash
# master_ip_failover - VIPåˆ‡æ¢è„šæœ¬

# è„šæœ¬å‚æ•°
SSH_USER="root"
VIP="192.168.1.100/24"
INTERFACE="eth0"

# æ•…éšœè½¬ç§»æ“ä½œ
failover_vip() {
    local new_master_host=$1
    local orig_master_host=$2
    
    echo "æ‰§è¡ŒVIPæ•…éšœè½¬ç§»åˆ°: $new_master_host"
    
    # åœ¨æ–°ä¸»èŠ‚ç‚¹ä¸Šæ·»åŠ VIP
    ssh $SSH_USER@$new_master_host "/sbin/ip addr add $VIP dev $INTERFACE"
    
    # åœ¨åŸä¸»èŠ‚ç‚¹ä¸Šç§»é™¤VIP
    if [ -n "$orig_master_host" ]; then
        ssh $SSH_USER@$orig_master_host "/sbin/ip addr del $VIP dev $INTERFACE" 2>/dev/null || true
    fi
    
    # éªŒè¯VIPçŠ¶æ€
    ssh $SSH_USER@$new_master_host "/sbin/ip addr show $INTERFACE | grep '$VIP'"
    
    echo "VIPæ•…éšœè½¬ç§»å®Œæˆ"
}

# åœ¨çº¿åˆ‡æ¢æ“ä½œ
online_change_vip() {
    local new_master_host=$1
    local orig_master_host=$2
    
    echo "æ‰§è¡Œåœ¨çº¿VIPåˆ‡æ¢: $orig_master_host -> $new_master_host"
    
    # åŒæ­¥æ‰§è¡ŒVIPåˆ‡æ¢
    ssh $SSH_USER@$new_master_host "/sbin/ip addr add $VIP dev $INTERFACE" &
    NEW_PID=$!
    
    ssh $SSH_USER@$orig_master_host "/sbin/ip addr del $VIP dev $INTERFACE" &
    OLD_PID=$!
    
    # ç­‰å¾…åˆ‡æ¢å®Œæˆ
    wait $NEW_PID $OLD_PID
    
    echo "åœ¨çº¿VIPåˆ‡æ¢å®Œæˆ"
}

# æ ¹æ®æ“ä½œç±»å‹æ‰§è¡Œç›¸åº”åŠ¨ä½œ
case "$1" in
    stop)
        failover_vip $2 $3
        ;;
    start)
        online_change_vip $2 $3
        ;;
    status)
        # æ£€æŸ¥VIPçŠ¶æ€
        ssh $SSH_USER@$2 "/sbin/ip addr show $INTERFACE | grep '$VIP'" >/dev/null 2>&1
        if [ $? -eq 0 ]; then
            echo "VIPåœ¨èŠ‚ç‚¹ $2 ä¸Šæ­£å¸¸"
        else
            echo "VIPåœ¨èŠ‚ç‚¹ $2 ä¸Šç¼ºå¤±"
        fi
        ;;
    *)
        echo "Usage: $0 {start|stop|status} new_master_host [orig_master_host]"
        exit 1
        ;;
esac
```

### 2.2 Orchestratoré…ç½®

#### Orchestratoré…ç½®æ–‡ä»¶
```json
{
  "Debug": true,
  "ListenAddress": ":3000",
  "MySQLTopologyUser": "orchestrator",
  "MySQLTopologypassword: "${DB_PASSWORD}",
  "MySQLTopologyCredentialsConfigFile": "",
  "MySQLTopologySSLPrivateKeyFile": "",
  "MySQLTopologySSLCertFile": "",
  "MySQLTopologySSLCAFile": "",
  "MySQLTopologySSLSkipVerify": true,
  "MySQLTopologyUseMutualTLS": false,
  "MySQLTopologyMaxPoolConnections": 3,
  "MySQLDiscoveryUser": "orchestrator",
  "MySQLDiscoverypassword: "${DB_PASSWORD}",
  "MySQLDiscoveryCredentialsConfigFile": "",
  "MySQLDiscoverySSLPrivateKeyFile": "",
  "MySQLDiscoverySSLCertFile": "",
  "MySQLDiscoverySSLCAFile": "",
  "MySQLDiscoverySSLSkipVerify": true,
  "MySQLDiscoveryUseMutualTLS": false,
  "MySQLDiscoveryMaxPoolConnections": 3,
  "DatabaselessMode__experimental": false,
  "RaftEnabled": false,
  "RaftDataDir": "/var/lib/orchestrator",
  "RaftBind": "127.0.0.1:10008",
  "DefaultRaftPort": 10008,
  "RaftNodes": [],
  "ExpectFailureAnalysisConcensus": true,
  "MySQLOrchestratorHost": "127.0.0.1",
  "MySQLOrchestratorPort": 3306,
  "MySQLOrchestratorDatabase": "orchestrator",
  "MySQLOrchestratorUser": "orchestrator",
  "MySQLOrchestratorpassword: "${DB_PASSWORD}",
  "MySQLOrchestratorCredentialsConfigFile": "",
  "MySQLOrchestratorSSLPrivateKeyFile": "",
  "MySQLOrchestratorSSLCertFile": "",
  "MySQLOrchestratorSSLCAFile": "",
  "MySQLOrchestratorSSLSkipVerify": true,
  "MySQLOrchestratorUseMutualTLS": false,
  "MySQLConnectTimeoutSeconds": 1,
  "DefaultInstancePort": 3306,
  "DiscoverByShowSlaveHosts": true,
  "InstancePollSeconds": 5,
  "InstanceWriteBufferSize": 100,
  "BufferInstanceWrites": true,
  "InstanceFlushIntervalMilliseconds": 100,
  "ReadLongRunningQueries": true,
  "LongRunningQueryThresholdSeconds": 60,
  "AutoPseudoGTID": true,
  "DetectDataCenterQuery": "select @@datadir",
  "DetectPhysicalEnvironmentQuery": "select @@basedir",
  "DetectSemiSyncEnforcedQuery": "",
  "SupportFuzzyPoolHostnames": true,
  "InstanceBulkOperationsWaitTimeoutSeconds": 10,
  "HostnameResolveMethod": "default",
  "MySQLHostnameResolveMethod": "@@hostname",
  "SkipBinlogServerUnresolveCheck": true,
  "ExpiryHostnameResolvesMinutes": 60,
  "RejectHostnameResolvePattern": "",
  "ReasonableReplicationLagSeconds": 10,
  "ProblemIgnoreHostnameFilters": [],
  "VerifyReplicationFilters": false,
  "ReasonableMaintenanceReplicationLagSeconds": 20,
  "CandidateInstanceExpireMinutes": 60,
  "AuditLogFile": "",
  "AuditToSyslog": false,
  "RemoveTextFromHostnameDisplay": ".mydomain.com:3306",
  "ReadOnly": false,
  "AuthenticationMethod": "",
  "HTTPAuthUser": "",
  "HTTPAuthpassword: "${DB_PASSWORD}",
  "AuthUserHeader": "",
  "PowerAuthUsers": [
    "*"
  ],
  "ClusterNameToAlias": {
    "127.0.0.1:3306": "testcluster"
  },
  "SlaveLagQuery": "",
  "DetectClusterAliasQuery": "SELECT SUBSTRING_INDEX(@@hostname, '.', 1)",
  "DetectClusterDomainQuery": "",
  "DataCenterPattern": "[.]([^.]+)[.][^.]+[.]mydomain[.]com",
  "PhysicalEnvironmentPattern": "[.]([^.]+)[.][^.]+[.]mydomain[.]com",
  "PromotionIgnoreHostnameFilters": [],
  "ServeAgentsHttp": false,
  "AgentsServerPort": ":3001",
  "AgentsUseSSL": false,
  "AgentsUseMutualTLS": false,
  "AgentSSLSkipVerify": false,
  "AgentSSLPrivateKeyFile": "",
  "AgentSSLCertFile": "",
  "AgentSSLCAFile": "",
  "AgentSSLValidOUs": [],
  "UseSSL": false,
  "UseMutualTLS": false,
  "SSLSkipVerify": false,
  "SSLPrivateKeyFile": "",
  "SSLCertFile": "",
  "SSLCAFile": "",
  "SSLValidOUs": [],
  "URLPrefix": "",
  "MaxOutdatedKeysToShow": 64,
  "DiscoveryIgnoreReplicaHostnameFilters": [],
  "DiscoveryIgnoreMasterHostnameFilters": [],
  "DiscoveryIgnoreHostnameFilters": [],
  "DiscoveryIgnoreInstancePortFilters": [],
  "ActiveNodeExpireSeconds": 5,
  "BinlogFileHistoryDays": 7,
  "UnseenInstanceForgetHours": 240,
  "SnapshotTopologiesIntervalHours": 0,
  "DiscoveryMaxConcurrency": 300,
  "DiscoveryQueueCapacity": 100000,
  "DiscoveryQueueMaxStatisticsSize": 120,
  "DiscoveryCollectionRetentionSeconds": 120,
  "InstanceBulkOperationsWaitTimeoutSeconds": 10,
  "HostnameResolveMethod": "default",
  "MySQLHostnameResolveMethod": "@@hostname",
  "SkipBinlogServerUnresolveCheck": true,
  "ExpiryHostnameResolvesMinutes": 60,
  "RejectHostnameResolvePattern": "",
  "ReasonableReplicationLagSeconds": 10,
  "ProblemIgnoreHostnameFilters": [],
  "VerifyReplicationFilters": false,
  "ReasonableMaintenanceReplicationLagSeconds": 20,
  "CandidateInstanceExpireMinutes": 60,
  "AuditLogFile": "",
  "AuditToSyslog": false,
  "RemoveTextFromHostnameDisplay": ".mydomain.com:3306",
  "ReadOnly": false,
  "AuthenticationMethod": "",
  "HTTPAuthUser": "",
  "HTTPAuthpassword: "${DB_PASSWORD}",
  "AuthUserHeader": "",
  "PowerAuthUsers": [
    "*"
  ],
  "ClusterNameToAlias": {
    "127.0.0.1:3306": "testcluster"
  },
  "SlaveLagQuery": "",
  "DetectClusterAliasQuery": "SELECT SUBSTRING_INDEX(@@hostname, '.', 1)",
  "DetectClusterDomainQuery": "",
  "DataCenterPattern": "[.]([^.]+)[.][^.]+[.]mydomain[.]com",
  "PhysicalEnvironmentPattern": "[.]([^.]+)[.][^.]+[.]mydomain[.]com",
  "PromotionIgnoreHostnameFilters": [],
  "ServeAgentsHttp": false,
  "AgentsServerPort": ":3001",
  "AgentsUseSSL": false,
  "AgentsUseMutualTLS": false,
  "AgentSSLSkipVerify": false,
  "AgentSSLPrivateKeyFile": "",
  "AgentSSLCertFile": "",
  "AgentSSLCAFile": "",
  "AgentSSLValidOUs": [],
  "UseSSL": false,
  "UseMutualTLS": false,
  "SSLSkipVerify": false,
  "SSLPrivateKeyFile": "",
  "SSLCertFile": "",
  "SSLCAFile": "",
  "SSLValidOUs": [],
  "URLPrefix": "",
  "MaxOutdatedKeysToShow": 64,
  "DiscoveryIgnoreReplicaHostnameFilters": [],
  "DiscoveryIgnoreMasterHostnameFilters": [],
  "DiscoveryIgnoreHostnameFilters": [],
  "DiscoveryIgnoreInstancePortFilters": []
}
```

## 3. PostgreSQLæ•…éšœåˆ‡æ¢

### 3.1 Patroniè‡ªåŠ¨æ•…éšœè½¬ç§»

#### Patronié…ç½®è¯¦è§£
```yaml
# patroni.yml - Patronié«˜å¯ç”¨é…ç½®
scope: postgres-cluster
namespace: /db/
name: postgresql0

restapi:
  listen: 0.0.0.0:8008
  connect_address: 192.168.1.10:8008
  certfile: /etc/ssl/certs/ssl-cert-snakeoil.pem
  keyfile: /etc/ssl/private/ssl-cert-snakeoil.key
  authentication:
    username: patroni
    password: patroni_password

etcd:
  hosts: 192.168.1.100:2379,192.168.1.101:2379,192.168.1.102:2379
  protocol: https
  cacert: /etc/ssl/etcd/ca.crt
  certfile: /etc/ssl/etcd/client.crt
  keyfile: /etc/ssl/etcd/client.key

bootstrap:
  dcs:
    ttl: 30
    loop_wait: 10
    retry_timeout: 10
    maximum_lag_on_failover: 1048576
    synchronous_mode: true
    synchronous_mode_strict: false
    postgresql:
      use_pg_rewind: true
      use_slots: true
      parameters:
        wal_level: replica
        hot_standby: "on"
        max_connections: 200
        max_wal_senders: 8
        max_replication_slots: 8
        wal_keep_segments: 64
        max_prepared_transactions: 0
        max_locks_per_transaction: 64
        max_worker_processes: 8
        wal_log_hints: "on"
        track_commit_timestamp: "off"
        archive_mode: "on"
        archive_timeout: 1800s
        archive_command: "mkdir -p ../wal_archive && cp %p ../wal_archive/%f"

  initdb:
  - encoding: UTF8
  - data-checksums

  pg_hba:
  - host replication replicator 127.0.0.1/32 md5
  - host replication replicator 192.168.1.0/24 md5
  - host all all 0.0.0.0/0 md5

  users:
    replicator:
      password: replicator_password
      options:
        - replication

postgresql:
  listen: 0.0.0.0:5432
  connect_address: 192.168.1.10:5432
  data_dir: /var/lib/postgresql/14/main
  bin_dir: /usr/lib/postgresql/14/bin
  pgpass: /tmp/pgpass
  authentication:
    replication:
      username: replicator
      password: replicator_password
    superuser:
      username: postgres
      password: postgres_password
  parameters:
    unix_socket_directories: '.'

watchdog:
  mode: automatic
  device: /dev/watchdog
  safety_margin: 5

tags:
  nofailover: false
  noloadbalance: false
  clonefrom: false
  nosync: false
```

#### æ•…éšœåˆ‡æ¢æµ‹è¯•è„šæœ¬
```bash
#!/bin/bash
# Patroniæ•…éšœåˆ‡æ¢æµ‹è¯•

test_patroni_failover() {
    echo "=== Patroniæ•…éšœåˆ‡æ¢æµ‹è¯• ==="
    
    # 1. æ£€æŸ¥å½“å‰é›†ç¾¤çŠ¶æ€
    check_cluster_status() {
        echo "å½“å‰é›†ç¾¤çŠ¶æ€:"
        patronictl -c /etc/patroni.yml list
        
        current_master=$(patronictl -c /etc/patroni.yml list | grep -E "Leader|Master" | awk '{print $2}')
        echo "å½“å‰ä¸»èŠ‚ç‚¹: $current_master"
    }
    
    # 2. æ¨¡æ‹Ÿä¸»èŠ‚ç‚¹æ•…éšœ
    simulate_master_failure() {
        local master_node=$1
        echo "æ¨¡æ‹Ÿä¸»èŠ‚ç‚¹ $master_node æ•…éšœ"
        
        # åœæ­¢ä¸»èŠ‚ç‚¹æœåŠ¡
        systemctl stop patroni
        
        # ç­‰å¾…æ•…éšœæ£€æµ‹å’Œåˆ‡æ¢
        sleep 30
        
        # æ£€æŸ¥æ–°çš„ä¸»èŠ‚ç‚¹
        new_master=$(patronictl -c /etc/patroni.yml list | grep -E "Leader|Master" | awk '{print $2}')
        echo "æ–°çš„ä¸»èŠ‚ç‚¹: $new_master"
        
        if [ "$new_master" != "$master_node" ] && [ -n "$new_master" ]; then
            echo "âœ… æ•…éšœåˆ‡æ¢æˆåŠŸ"
            return 0
        else
            echo "âŒ æ•…éšœåˆ‡æ¢å¤±è´¥"
            return 1
        fi
    }
    
    # 3. éªŒè¯æ•°æ®ä¸€è‡´æ€§
    verify_data_consistency() {
        local new_master=$1
        
        echo "éªŒè¯æ•°æ®ä¸€è‡´æ€§..."
        
        # è¿æ¥åˆ°æ–°ä¸»èŠ‚ç‚¹æ‰§è¡Œæµ‹è¯•æŸ¥è¯¢
        psql -h $new_master -U postgres -d postgres -c "SELECT COUNT(*) FROM test_table;" > /tmp/new_count
        
        # æ¯”è¾ƒæ•°æ®ä¸€è‡´æ€§
        # è¿™é‡Œåº”è¯¥ä¸æ•…éšœå‰çš„æ•°æ®è¿›è¡Œæ¯”è¾ƒ
        echo "æ•°æ®ä¸€è‡´æ€§éªŒè¯å®Œæˆ"
    }
    
    # 4. æ¢å¤æ•…éšœèŠ‚ç‚¹
    recover_failed_node() {
        local failed_node=$1
        echo "æ¢å¤æ•…éšœèŠ‚ç‚¹ $failed_node"
        
        # é‡æ–°å¯åŠ¨PatroniæœåŠ¡
        systemctl start patroni
        
        # ç­‰å¾…èŠ‚ç‚¹é‡æ–°åŠ å…¥é›†ç¾¤
        sleep 60
        
        # éªŒè¯èŠ‚ç‚¹çŠ¶æ€
        patronictl -c /etc/patroni.yml list | grep $failed_node
    }
    
    # æ‰§è¡Œæµ‹è¯•æµç¨‹
    check_cluster_status
    simulate_master_failure $current_master
    verify_data_consistency $new_master
    recover_failed_node $current_master
}

# è‡ªåŠ¨åŒ–æ•…éšœåˆ‡æ¢ç›‘æ§
monitor_failover_events() {
    # ç›‘æ§Patroniäº‹ä»¶
    tail -f /var/log/patroni/patroni.log | while read line; do
        if echo "$line" | grep -q "promoted\|demoted\|failover"; then
            echo "[$(date)] æ•…éšœåˆ‡æ¢äº‹ä»¶: $line"
            # å‘é€å‘Šè­¦é€šçŸ¥
            send_alert "Patroniæ•…éšœåˆ‡æ¢äº‹ä»¶" "$line"
        fi
    done
}
```

## 4. MongoDBè‡ªåŠ¨åˆ‡æ¢

### 4.1 å‰¯æœ¬é›†è‡ªåŠ¨æ•…éšœè½¬ç§»

#### MongoDBå‰¯æœ¬é›†é…ç½®
```javascript
// MongoDBå‰¯æœ¬é›†é…ç½®
rs_config = {
    _id: "myReplicaSet",
    members: [
        { _id: 0, host: "mongo-primary:27017", priority: 2 },
        { _id: 1, host: "mongo-secondary-1:27017", priority: 1 },
        { _id: 2, host: "mongo-secondary-2:27017", priority: 1 },
        { _id: 3, host: "mongo-arbiter:27017", arbiterOnly: true }
    ],
    settings: {
        heartbeatTimeoutSecs: 10,
        electionTimeoutMillis: 10000,
        catchUpTimeoutMillis: 30000,
        getLastErrorModes: {
            waitForTwo: { 
                "mongo-secondary-1:27017": 1, 
                "mongo-secondary-2:27017": 1 
            }
        },
        getLastErrorDefaults: { w: "majority", wtimeout: 5000 }
    }
};

// åˆå§‹åŒ–å‰¯æœ¬é›†
rs.initiate(rs_config);

// éªŒè¯é…ç½®
rs.conf();
rs.status();
```

#### æ•…éšœæ£€æµ‹å’Œåˆ‡æ¢ç›‘æ§
```python
# MongoDBæ•…éšœåˆ‡æ¢ç›‘æ§
import pymongo
import time
import smtplib
from email.mime.text import MIMEText

class MongoDBFailoverMonitor:
    def __init__(self, replica_set_hosts):
        self.hosts = replica_set_hosts
        self.primary_host = None
        self.failover_history = []
        self.alert_recipients = ["admin@company.com"]
    
    def monitor_replica_set(self):
        """ç›‘æ§å‰¯æœ¬é›†çŠ¶æ€"""
        while True:
            try:
                # è·å–å½“å‰ä¸»èŠ‚ç‚¹
                current_primary = self.get_current_primary()
                
                # æ£€æŸ¥æ˜¯å¦å‘ç”Ÿæ•…éšœåˆ‡æ¢
                if self.primary_host and current_primary != self.primary_host:
                    self.handle_failover(self.primary_host, current_primary)
                
                self.primary_host = current_primary
                time.sleep(30)  # 30ç§’æ£€æŸ¥ä¸€æ¬¡
                
            except Exception as e:
                print(f"ç›‘æ§å¼‚å¸¸: {str(e)}")
                time.sleep(60)
    
    def get_current_primary(self):
        """è·å–å½“å‰ä¸»èŠ‚ç‚¹"""
        for host in self.hosts:
            try:
                client = pymongo.MongoClient(
                    host, 
                    serverSelectionTimeoutMS=5000,
                    connectTimeoutMS=5000
                )
                is_master = client.admin.command('ismaster')
                client.close()
                
                if is_master.get('ismaster', False):
                    return host
            except:
                continue
        return None
    
    def handle_failover(self, old_primary, new_primary):
        """å¤„ç†æ•…éšœåˆ‡æ¢äº‹ä»¶"""
        failover_event = {
            'timestamp': time.time(),
            'old_primary': old_primary,
            'new_primary': new_primary,
            'downtime_seconds': self.calculate_downtime(),
            'data_consistency': self.verify_data_consistency()
        }
        
        self.failover_history.append(failover_event)
        self.send_failover_alert(failover_event)
        self.log_failover_event(failover_event)
    
    def calculate_downtime(self):
        """è®¡ç®—åœæœºæ—¶é—´"""
        # è¿™é‡Œåº”è¯¥å®ç°å®é™…çš„åœæœºæ—¶é—´è®¡ç®—é€»è¾‘
        return 0
    
    def verify_data_consistency(self):
        """éªŒè¯æ•°æ®ä¸€è‡´æ€§"""
        # å®ç°æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥é€»è¾‘
        return True
    
    def send_failover_alert(self, event):
        """å‘é€æ•…éšœåˆ‡æ¢å‘Šè­¦"""
        subject = f"MongoDBæ•…éšœåˆ‡æ¢å‘Šè­¦ - {event['new_primary']}"
        body = f"""
        MongoDBå‰¯æœ¬é›†å‘ç”Ÿæ•…éšœåˆ‡æ¢:
        - åŸä¸»èŠ‚ç‚¹: {event['old_primary']}
        - æ–°ä¸»èŠ‚ç‚¹: {event['new_primary']}
        - åœæœºæ—¶é—´: {event['downtime_seconds']}ç§’
        - æ•°æ®ä¸€è‡´æ€§: {'æ­£å¸¸' if event['data_consistency'] else 'å¼‚å¸¸'}
        - æ—¶é—´: {time.ctime(event['timestamp'])}
        """
        
        msg = MIMEText(body)
        msg['Subject'] = subject
        msg['From'] = "mongodb-monitor@company.com"
        msg['To'] = ", ".join(self.alert_recipients)
        
        try:
            smtp = smtplib.SMTP('localhost')
            smtp.send_message(msg)
            smtp.quit()
        except Exception as e:
            print(f"å‘é€å‘Šè­¦é‚®ä»¶å¤±è´¥: {str(e)}")

# ä½¿ç”¨ç¤ºä¾‹
monitor = MongoDBFailoverMonitor([
    "mongo-primary:27017",
    "mongo-secondary-1:27017", 
    "mongo-secondary-2:27017"
])
monitor.monitor_replica_set()
```

## 5. åˆ‡æ¢ç­–ç•¥ä¼˜åŒ–

### 5.1 æ™ºèƒ½åˆ‡æ¢ç®—æ³•

#### æ•…éšœåˆ‡æ¢å†³ç­–å¼•æ“
```python
# æ™ºèƒ½æ•…éšœåˆ‡æ¢å†³ç­–å¼•æ“
class IntelligentFailoverEngine:
    def __init__(self, cluster_config):
        self.config = cluster_config
        self.decision_history = []
        self.performance_metrics = {}
    
    def evaluate_failover_candidate(self, candidates, failure_context):
        """è¯„ä¼°æ•…éšœåˆ‡æ¢å€™é€‰èŠ‚ç‚¹"""
        scores = {}
        
        for candidate in candidates:
            score = self._calculate_candidate_score(candidate, failure_context)
            scores[candidate['host']] = score
        
        # é€‰æ‹©å¾—åˆ†æœ€é«˜çš„å€™é€‰èŠ‚ç‚¹
        best_candidate = max(scores, key=scores.get)
        return {
            'selected_candidate': best_candidate,
            'scores': scores,
            'decision_reason': self._generate_decision_reason(scores, failure_context)
        }
    
    def _calculate_candidate_score(self, candidate, context):
        """è®¡ç®—å€™é€‰èŠ‚ç‚¹å¾—åˆ†"""
        score = 0
        
        # æ•°æ®æ–°é²œåº¦å¾—åˆ† (40%æƒé‡)
        data_lag = context['replication_lag'].get(candidate['host'], 0)
        if data_lag < 10:  # 10ç§’ä»¥å†…
            score += 40
        elif data_lag < 30:  # 30ç§’ä»¥å†…
            score += 20
        else:
            score += 5
        
        # ç¡¬ä»¶æ€§èƒ½å¾—åˆ† (30%æƒé‡)
        hardware_score = self._assess_hardware_performance(candidate)
        score += hardware_score * 0.3
        
        # ç½‘ç»œè´¨é‡å¾—åˆ† (20%æƒé‡)
        network_score = self._assess_network_quality(candidate, context['clients'])
        score += network_score * 0.2
        
        # å†å²è¡¨ç°å¾—åˆ† (10%æƒé‡)
        history_score = self._get_historical_performance(candidate['host'])
        score += history_score * 0.1
        
        return round(score, 2)
    
    def _assess_hardware_performance(self, candidate):
        """è¯„ä¼°ç¡¬ä»¶æ€§èƒ½"""
        # CPUã€å†…å­˜ã€ç£ç›˜I/Oç­‰æŒ‡æ ‡è¯„ä¼°
        metrics = self.performance_metrics.get(candidate['host'], {})
        
        cpu_usage = metrics.get('cpu_usage', 50)
        memory_usage = metrics.get('memory_usage', 60)
        disk_io = metrics.get('disk_io_utilization', 40)
        
        # æ€§èƒ½è¶Šä½å¾—åˆ†è¶Šé«˜ï¼ˆè¡¨ç¤ºèµ„æºå……è¶³ï¼‰
        cpu_score = max(0, 100 - cpu_usage)
        memory_score = max(0, 100 - memory_usage)
        io_score = max(0, 100 - disk_io)
        
        return (cpu_score + memory_score + io_score) / 3
    
    def _assess_network_quality(self, candidate, client_locations):
        """è¯„ä¼°ç½‘ç»œè´¨é‡"""
        total_latency = 0
        valid_measurements = 0
        
        for client in client_locations:
            latency = self._measure_network_latency(candidate['host'], client)
            if latency is not None:
                total_latency += latency
                valid_measurements += 1
        
        if valid_measurements > 0:
            avg_latency = total_latency / valid_measurements
            # å»¶è¿Ÿè¶Šä½å¾—åˆ†è¶Šé«˜
            return max(0, 100 - (avg_latency / 10))  # æ¯10msæ‰£1åˆ†
        else:
            return 50  # é»˜è®¤ä¸­ç­‰åˆ†æ•°
    
    def optimize_failover_timing(self, failure_severity):
        """ä¼˜åŒ–æ•…éšœåˆ‡æ¢æ—¶æœº"""
        timing_strategy = {
            'critical': {
                'detection_threshold': 2,  # 2æ¬¡æ£€æµ‹ç¡®è®¤
                'switch_delay': 0,         # ç«‹å³åˆ‡æ¢
                'verification_time': 30    # 30ç§’éªŒè¯
            },
            'major': {
                'detection_threshold': 3,
                'switch_delay': 30,        # 30ç§’å»¶è¿Ÿ
                'verification_time': 60
            },
            'minor': {
                'detection_threshold': 5,
                'switch_delay': 120,       # 2åˆ†é’Ÿå»¶è¿Ÿ
                'verification_time': 120
            }
        }
        
        return timing_strategy.get(failure_severity, timing_strategy['minor'])

# åˆ‡æ¢ç­–ç•¥é…ç½®
failover_policies = {
    'conservative': {
        'require_majority': True,
        'min_uptime_minutes': 30,
        'max_switches_per_hour': 3,
        'manual_confirmation': True
    },
    'aggressive': {
        'require_majority': False,
        'min_uptime_minutes': 5,
        'max_switches_per_hour': 10,
        'manual_confirmation': False
    },
    'balanced': {
        'require_majority': True,
        'min_uptime_minutes': 15,
        'max_switches_per_hour': 5,
        'manual_confirmation': False
    }
}
```

### 5.2 åˆ‡æ¢è¿‡ç¨‹ä¼˜åŒ–

#### é›¶åœæœºåˆ‡æ¢å®ç°
```bash
#!/bin/bash
# é›¶åœæœºæ•…éšœåˆ‡æ¢è„šæœ¬

zero_downtime_failover() {
    echo "=== é›¶åœæœºæ•…éšœåˆ‡æ¢ ==="
    
    # 1. é¢„åˆ‡æ¢å‡†å¤‡
    pre_failover_preparation() {
        local new_master=$1
        
        echo "æ‰§è¡Œé¢„åˆ‡æ¢å‡†å¤‡..."
        
        # é¢„çƒ­æ–°ä¸»èŠ‚ç‚¹ç¼“å­˜
        warmup_cache $new_master
        
        # é¢„å»ºç«‹è¿æ¥æ± 
        pre_build_connections $new_master
        
        # é¢„åŠ è½½å¿…è¦æ•°æ®
        preload_critical_data $new_master
        
        echo "é¢„åˆ‡æ¢å‡†å¤‡å®Œæˆ"
    }
    
    # 2. å¹¶è¡Œåˆ‡æ¢æ“ä½œ
    parallel_failover_execution() {
        local old_master=$1
        local new_master=$2
        
        echo "æ‰§è¡Œå¹¶è¡Œåˆ‡æ¢..."
        
        # åå°æ‰§è¡ŒVIPåˆ‡æ¢
        switch_vip $new_master $old_master &
        VIP_PID=$!
        
        # åå°æ›´æ–°DNSè®°å½•
        update_dns_records $new_master &
        DNS_PID=$!
        
        # åå°é€šçŸ¥åº”ç”¨å±‚
        notify_applications $new_master &
        APP_PID=$!
        
        # ç­‰å¾…æ‰€æœ‰æ“ä½œå®Œæˆ
        wait $VIP_PID $DNS_PID $APP_PID
        
        echo "å¹¶è¡Œåˆ‡æ¢å®Œæˆ"
    }
    
    # 3. åˆ‡æ¢éªŒè¯
    verify_failover_success() {
        local new_master=$1
        
        echo "éªŒè¯åˆ‡æ¢ç»“æœ..."
        
        # éªŒè¯æœåŠ¡å¯ç”¨æ€§
        if ! verify_service_availability $new_master; then
            echo "æœåŠ¡éªŒè¯å¤±è´¥ï¼Œæ‰§è¡Œå›æ»š"
            rollback_failover $new_master
            return 1
        fi
        
        # éªŒè¯æ•°æ®ä¸€è‡´æ€§
        if ! verify_data_consistency $new_master; then
            echo "æ•°æ®ä¸€è‡´æ€§éªŒè¯å¤±è´¥"
            return 1
        fi
        
        # éªŒè¯æ€§èƒ½æŒ‡æ ‡
        if ! verify_performance_metrics $new_master; then
            echo "æ€§èƒ½æŒ‡æ ‡å¼‚å¸¸"
            return 1
        fi
        
        echo "åˆ‡æ¢éªŒè¯é€šè¿‡"
        return 0
    }
    
    # æ‰§è¡Œå®Œæ•´åˆ‡æ¢æµç¨‹
    local old_master=$(get_current_master)
    local new_master=$(select_best_candidate)
    
    pre_failover_preparation $new_master
    parallel_failover_execution $old_master $new_master
    verify_failover_success $new_master
}

# ç¼“å­˜é¢„çƒ­å‡½æ•°
warmup_cache() {
    local target_host=$1
    
    # é¢„æ‰§è¡Œçƒ­ç‚¹æŸ¥è¯¢
    for query in "${HOT_QUERIES[@]}"; do
        mysql -h $target_host -e "$query" >/dev/null 2>&1 &
    done
    
    # ç­‰å¾…é¢„çƒ­å®Œæˆ
    wait
}

# è¿æ¥æ± é¢„å»ºå‡½æ•°
pre_build_connections() {
    local target_host=$1
    
    # é¢„å»ºç«‹è¿æ¥
    for i in {1..20}; do
        mysql -h $target_host -e "SELECT 1" >/dev/null 2>&1 &
    done
    
    wait
}
```

## 6. ç›‘æ§ä¸å‘Šè­¦ä½“ç³»

### 6.1 æ•…éšœåˆ‡æ¢ç›‘æ§

#### å…¨é¢ç›‘æ§æŒ‡æ ‡ä½“ç³»
```python
# æ•…éšœåˆ‡æ¢ç›‘æ§ç³»ç»Ÿ
class FailoverMonitoringSystem:
    def __init__(self):
        self.metrics_collector = MetricsCollector()
        self.alert_manager = AlertManager()
        self.dashboard_manager = DashboardManager()
    
    def setup_comprehensive_monitoring(self):
        """è®¾ç½®å…¨é¢ç›‘æ§"""
        monitors = {
            'detection_monitor': self.setup_detection_monitoring(),
            'switching_monitor': self.setup_switching_monitoring(),
            'post_failover_monitor': self.setup_post_failover_monitoring(),
            'performance_monitor': self.setup_performance_monitoring()
        }
        return monitors
    
    def setup_detection_monitoring(self):
        """è®¾ç½®æ•…éšœæ£€æµ‹ç›‘æ§"""
        detection_metrics = {
            'detection_time': {
                'type': 'histogram',
                'description': 'æ•…éšœæ£€æµ‹æ—¶é—´åˆ†å¸ƒ',
                'buckets': [1, 5, 10, 30, 60, 120]
            },
            'false_positive_rate': {
                'type': 'gauge',
                'description': 'è¯¯æŠ¥ç‡'
            },
            'detection_accuracy': {
                'type': 'gauge',
                'description': 'æ£€æµ‹å‡†ç¡®ç‡'
            },
            'concurrent_failures': {
                'type': 'counter',
                'description': 'å¹¶å‘æ•…éšœæ•°'
            }
        }
        
        return detection_metrics
    
    def setup_switching_monitoring(self):
        """è®¾ç½®åˆ‡æ¢è¿‡ç¨‹ç›‘æ§"""
        switching_metrics = {
            'switch_duration': {
                'type': 'histogram',
                'description': 'åˆ‡æ¢è€—æ—¶åˆ†å¸ƒ',
                'buckets': [1, 5, 10, 30, 60, 120, 300]
            },
            'switch_success_rate': {
                'type': 'gauge',
                'description': 'åˆ‡æ¢æˆåŠŸç‡'
            },
            'rollback_count': {
                'type': 'counter',
                'description': 'å›æ»šæ¬¡æ•°'
            },
            'partial_failures': {
                'type': 'counter',
                'description': 'éƒ¨åˆ†ç»„ä»¶åˆ‡æ¢å¤±è´¥æ¬¡æ•°'
            }
        }
        
        return switching_metrics
    
    def setup_post_failover_monitoring(self):
        """è®¾ç½®åˆ‡æ¢åç›‘æ§"""
        post_metrics = {
            'service_availability': {
                'type': 'gauge',
                'description': 'æœåŠ¡å¯ç”¨æ€§'
            },
            'data_consistency': {
                'type': 'gauge',
                'description': 'æ•°æ®ä¸€è‡´æ€§çŠ¶æ€'
            },
            'performance_degradation': {
                'type': 'gauge',
                'description': 'æ€§èƒ½ä¸‹é™ç¨‹åº¦'
            },
            'client_impact': {
                'type': 'histogram',
                'description': 'å®¢æˆ·ç«¯å½±å“ç¨‹åº¦',
                'buckets': [0, 1, 5, 10, 30, 60]
            }
        }
        
        return post_metrics

# ç›‘æ§å‘Šè­¦è§„åˆ™é…ç½®
alert_rules = {
    'critical': [
        {
            'name': 'ä¸»èŠ‚ç‚¹å®Œå…¨æ•…éšœ',
            'condition': 'master_node_status == "down" and slave_nodes_available < 1',
            'severity': 'critical',
            'notification_channels': ['sms', 'email', 'slack']
        },
        {
            'name': 'åˆ‡æ¢å¤±è´¥',
            'condition': 'failover_status == "failed"',
            'severity': 'critical',
            'notification_channels': ['sms', 'email']
        }
    ],
    'warning': [
        {
            'name': 'ä¸»èŠ‚ç‚¹æ€§èƒ½å¼‚å¸¸',
            'condition': 'master_response_time > 5000 or master_cpu_usage > 90',
            'severity': 'warning',
            'notification_channels': ['email']
        },
        {
            'name': 'ä»èŠ‚ç‚¹åŒæ­¥å»¶è¿Ÿ',
            'condition': 'replication_lag > 30',
            'severity': 'warning',
            'notification_channels': ['email']
        }
    ],
    'info': [
        {
            'name': 'æ•…éšœåˆ‡æ¢å¼€å§‹',
            'condition': 'failover_initiated == true',
            'severity': 'info',
            'notification_channels': ['slack']
        },
        {
            'name': 'åˆ‡æ¢å®Œæˆ',
            'condition': 'failover_completed == true',
            'severity': 'info',
            'notification_channels': ['slack']
        }
    ]
}
```

### 6.2 æ€§èƒ½åˆ†æä¸ä¼˜åŒ–

#### åˆ‡æ¢æ€§èƒ½åˆ†æå·¥å…·
```python
# æ•…éšœåˆ‡æ¢æ€§èƒ½åˆ†æå™¨
class FailoverPerformanceAnalyzer:
    def __init__(self):
        self.performance_data = []
        self.baseline_metrics = {}
    
    def collect_performance_data(self, failover_event):
        """æ”¶é›†åˆ‡æ¢æ€§èƒ½æ•°æ®"""
        performance_record = {
            'timestamp': failover_event['timestamp'],
            'detection_time': failover_event['detection_time'],
            'switch_time': failover_event['switch_time'],
            'total_downtime': failover_event['total_downtime'],
            'data_loss': failover_event['data_loss'],
            'client_impact': failover_event['client_impact'],
            'resource_usage': failover_event['resource_usage']
        }
        
        self.performance_data.append(performance_record)
        return performance_record
    
    def analyze_performance_trends(self):
        """åˆ†ææ€§èƒ½è¶‹åŠ¿"""
        if len(self.performance_data) < 10:
            return {"status": "insufficient_data"}
        
        recent_data = self.performance_data[-50:]  # æœ€è¿‘50æ¬¡åˆ‡æ¢
        
        analysis = {
            'mttd_trend': self._calculate_trend([d['detection_time'] for d in recent_data]),
            'mttr_trend': self._calculate_trend([d['switch_time'] for d in recent_data]),
            'downtime_trend': self._calculate_trend([d['total_downtime'] for d in recent_data]),
            'success_rate': self._calculate_success_rate(recent_data),
            'performance_regression': self._detect_performance_regression(recent_data)
        }
        
        return analysis
    
    def generate_optimization_recommendations(self, analysis):
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []
        
        # æ£€æµ‹æ—¶é—´ä¼˜åŒ–
        if analysis['mttd_trend'] > 5:  # æ£€æµ‹æ—¶é—´æ¶åŒ–è¶…è¿‡5ç§’
            recommendations.append({
                'area': 'detection_optimization',
                'priority': 'high',
                'actions': [
                    'ä¼˜åŒ–æ•…éšœæ£€æµ‹ç®—æ³•',
                    'å¢åŠ æ£€æµ‹é¢‘ç‡',
                    'æ”¹å–„ç½‘ç»œè¿é€šæ€§ç›‘æ§'
                ]
            })
        
        # åˆ‡æ¢æ—¶é—´ä¼˜åŒ–
        if analysis['mttr_trend'] > 30:  # åˆ‡æ¢æ—¶é—´æ¶åŒ–è¶…è¿‡30ç§’
            recommendations.append({
                'area': 'switching_optimization',
                'priority': 'medium',
                'actions': [
                    'ä¼˜åŒ–VIPåˆ‡æ¢è„šæœ¬',
                    'é¢„å»ºç«‹è¿æ¥æ± ',
                    'å¹¶è¡Œæ‰§è¡Œåˆ‡æ¢æ“ä½œ'
                ]
            })
        
        # æˆåŠŸç‡ä¼˜åŒ–
        if analysis['success_rate'] < 0.95:  # æˆåŠŸç‡ä½äº95%
            recommendations.append({
                'area': 'reliability_improvement',
                'priority': 'high',
                'actions': [
                    'åŠ å¼ºé¢„åˆ‡æ¢æ£€æŸ¥',
                    'å®Œå–„å›æ»šæœºåˆ¶',
                    'å¢åŠ åˆ‡æ¢éªŒè¯æ­¥éª¤'
                ]
            })
        
        return recommendations
    
    def create_performance_report(self):
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        analysis = self.analyze_performance_trends()
        recommendations = self.generate_optimization_recommendations(analysis)
        
        report = {
            'report_date': time.time(),
            'period': 'last_30_days',
            'key_metrics': self._calculate_key_metrics(),
            'trend_analysis': analysis,
            'recommendations': recommendations,
            'benchmark_comparison': self._compare_with_benchmark()
        }
        
        return report

# æ€§èƒ½åŸºå‡†æµ‹è¯•
def run_failover_benchmark():
    """è¿è¡Œæ•…éšœåˆ‡æ¢åŸºå‡†æµ‹è¯•"""
    test_scenarios = [
        {
            'name': 'å•ç‚¹æ•…éšœåˆ‡æ¢',
            'failure_type': 'primary_node_crash',
            'expected_mttd': 15,
            'expected_mttr': 45
        },
        {
            'name': 'ç½‘ç»œåˆ†åŒºåˆ‡æ¢',
            'failure_type': 'network_partition',
            'expected_mttd': 30,
            'expected_mttr': 90
        },
        {
            'name': 'æ€§èƒ½é™çº§åˆ‡æ¢',
            'failure_type': 'performance_degradation',
            'expected_mttd': 60,
            'expected_mttr': 120
        }
    ]
    
    results = {}
    for scenario in test_scenarios:
        print(f"æ‰§è¡Œæµ‹è¯•: {scenario['name']}")
        result = execute_failover_test(scenario)
        results[scenario['name']] = result
        
        # ä¸é¢„æœŸå¯¹æ¯”
        actual_mttd = result['metrics']['mttd']
        actual_mttr = result['metrics']['mttr']
        
        print(f"MTTD: {actual_mttd}s (é¢„æœŸ: {scenario['expected_mttd']}s)")
        print(f"MTTR: {actual_mttr}s (é¢„æœŸ: {scenario['expected_mttr']}s)")
    
    return results
```

---

## ğŸ” å…³é”®è¦ç‚¹æ€»ç»“

### âœ… æ•…éšœåˆ‡æ¢æˆåŠŸè¦ç´ 
- **å¿«é€Ÿå‡†ç¡®çš„æ•…éšœæ£€æµ‹**ï¼šå¤šå±‚æ£€æµ‹æœºåˆ¶ç¡®ä¿åŠæ—¶å‘ç°æ•…éšœ
- **æ™ºèƒ½çš„åˆ‡æ¢å†³ç­–**ï¼šåŸºäºå¤šç»´åº¦è¯„ä¼°é€‰æ‹©æœ€ä¼˜å€™é€‰èŠ‚ç‚¹
- **è‡ªåŠ¨åŒ–çš„åˆ‡æ¢æ‰§è¡Œ**ï¼šå‡å°‘äººå·¥å¹²é¢„ï¼Œæé«˜åˆ‡æ¢æ•ˆç‡
- **å®Œå–„çš„éªŒè¯æœºåˆ¶**ï¼šç¡®ä¿åˆ‡æ¢åçš„æœåŠ¡è´¨é‡å’Œæ•°æ®ä¸€è‡´æ€§

### âš ï¸ å¸¸è§é£é™©æé†’
- **è¯¯åˆ‡æ¢é£é™©**ï¼šç½‘ç»œæŠ–åŠ¨ç­‰å¯èƒ½å¯¼è‡´ä¸å¿…è¦çš„æ•…éšœåˆ‡æ¢
- **æ•°æ®ä¸¢å¤±é£é™©**ï¼šå¼‚æ­¥å¤åˆ¶å¯èƒ½å­˜åœ¨æ•°æ®ä¸¢å¤±çª—å£
- **åˆ‡æ¢å¤±è´¥é£é™©**ï¼šå¤æ‚ç¯å¢ƒä¸‹åˆ‡æ¢å¯èƒ½éƒ¨åˆ†æˆåŠŸæˆ–å®Œå…¨å¤±è´¥
- **æ€§èƒ½å½±å“é£é™©**ï¼šåˆ‡æ¢è¿‡ç¨‹å¯èƒ½å¯¹ç³»ç»Ÿæ€§èƒ½äº§ç”ŸçŸ­æœŸå½±å“

### ğŸ¯ æœ€ä½³å®è·µå»ºè®®
1. **æ¸è¿›å¼éƒ¨ç½²**ï¼šä»éæ ¸å¿ƒä¸šåŠ¡å¼€å§‹ï¼Œé€æ­¥æ‰©å±•åˆ°æ ¸å¿ƒç³»ç»Ÿ
2. **å……åˆ†æµ‹è¯•**ï¼šå®šæœŸè¿›è¡Œæ•…éšœåˆ‡æ¢æ¼”ç»ƒï¼ŒéªŒè¯æ–¹æ¡ˆæœ‰æ•ˆæ€§
3. **ç›‘æ§å‘Šè­¦**ï¼šå»ºç«‹å®Œå–„çš„ç›‘æ§ä½“ç³»ï¼ŒåŠæ—¶å‘ç°é—®é¢˜
4. **æ–‡æ¡£è®°å½•**ï¼šè¯¦ç»†è®°å½•æ‰€æœ‰é…ç½®å’Œåˆ‡æ¢å†å²
5. **æŒç»­ä¼˜åŒ–**ï¼šåŸºäºå®é™…è¿è¡Œæ•°æ®ä¸æ–­ä¼˜åŒ–åˆ‡æ¢ç­–ç•¥

é€šè¿‡ç§‘å­¦çš„æ•…éšœè‡ªåŠ¨åˆ‡æ¢è®¾è®¡å’Œå®æ–½ï¼Œå¯ä»¥æ˜¾è‘—æå‡æ•°æ®åº“ç³»ç»Ÿçš„å¯ç”¨æ€§å’Œå¯é æ€§ï¼Œä¸ºä¼ä¸šä¸šåŠ¡è¿ç»­æ€§æä¾›åšå®ä¿éšœã€‚